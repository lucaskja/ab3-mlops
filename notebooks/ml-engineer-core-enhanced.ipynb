{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engineer Core Workflow with MLFlow and Model Registry\n",
    "\n",
    "This notebook provides enhanced functionality for ML Engineers to execute and manage YOLOv11 training pipelines with MLFlow experiment tracking and SageMaker Model Registry integration.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Pipeline Configuration**: Set up YOLOv11 training pipeline parameters\n",
    "2. **Pipeline Execution**: Execute the training pipeline with MLFlow tracking\n",
    "3. **Pipeline Monitoring**: Monitor training progress and results\n",
    "4. **Model Registration**: Register trained models in SageMaker Model Registry\n",
    "5. **Model Management**: Manage model versions and approval workflows\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with appropriate permissions\n",
    "- AWS CLI configured with \"ab\" profile\n",
    "- SageMaker Studio access with ML Engineer role\n",
    "- Access to the drone imagery dataset in S3 bucket: `lucaskle-ab3-project-pv`\n",
    "- Labeled data in YOLOv11 format\n",
    "- SageMaker managed MLFlow tracking server\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet mlflow>=3.0.0 requests-auth-aws-sigv4>=0.7 boto3>=1.28.0 sagemaker>=2.190.0 pandas>=2.0.0 matplotlib>=3.7.0 numpy>=1.24.0 PyYAML>=6.0\n",
    "\n",
    "print(\"‚úÖ Required packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "import mlflow\n",
    "import mlflow.sagemaker\n",
    "\n",
    "# Correct imports for SageMaker Model Registry\n",
    "from sagemaker import ModelPackage\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "# Set up AWS session with \"ab\" profile\n",
    "session = boto3.Session(profile_name='ab')\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "sagemaker_client = session.client('sagemaker')\n",
    "region = session.region_name\n",
    "account_id = session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Set up MLFlow tracking with SageMaker managed server\n",
    "try:\n",
    "    # Use the correct tracking server ARN format for SageMaker managed MLflow\n",
    "    tracking_server_arn = \"arn:aws:sagemaker:us-east-1:192771711075:mlflow-tracking-server/sagemaker-core-setup-mlflow-server\"\n",
    "    mlflow.set_tracking_uri(tracking_server_arn)\n",
    "    mlflow_tracking_uri = tracking_server_arn\n",
    "    \n",
    "    print(f\"‚úÖ Connected to SageMaker managed MLflow server\")\n",
    "    print(f\"Tracking Server ARN: {tracking_server_arn}\")\n",
    "    \n",
    "    # Create experiment\n",
    "    experiment_name = \"yolov11-drone-detection\"\n",
    "    try:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        print(f\"Created new experiment: {experiment_name}\")\n",
    "    except Exception:\n",
    "        # Experiment already exists\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        print(f\"Using existing experiment: {experiment_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not connect to SageMaker managed MLflow: {e}\")\n",
    "    print(\"Using basic MLflow setup as fallback\")\n",
    "    experiment_name = \"yolov11-drone-detection\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    mlflow_tracking_uri = \"file:///tmp/mlruns\"\n",
    "\n",
    "# Set up visualization\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Define bucket name and role\n",
    "BUCKET_NAME = 'lucaskle-ab3-project-pv'\n",
    "ROLE_ARN = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "# Model Registry configuration\n",
    "MODEL_PACKAGE_GROUP_NAME = \"yolov11-drone-detection-models\"\n",
    "\n",
    "print(f\"Data Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Role ARN: {ROLE_ARN}\")\n",
    "print(f\"MLFlow Experiment: {experiment_name}\")\n",
    "print(f\"MLFlow Tracking URI: {mlflow_tracking_uri}\")\n",
    "print(f\"Model Package Group: {MODEL_PACKAGE_GROUP_NAME}\")\n",
    "\n",
    "# Helper functions for MLflow logging\n",
    "def log_params(params_dict):\n",
    "    \"\"\"Log parameters using MLflow\"\"\"\n",
    "    mlflow.log_params(params_dict)\n",
    "\n",
    "def log_metrics(metrics_dict, step=None):\n",
    "    \"\"\"Log metrics using MLflow\"\"\"\n",
    "    for key, value in metrics_dict.items():\n",
    "        mlflow.log_metric(key, value, step=step)\n",
    "\n",
    "def log_artifact(local_path, artifact_path=None):\n",
    "    \"\"\"Log artifact using MLflow\"\"\"\n",
    "    mlflow.log_artifact(local_path, artifact_path)\n",
    "\n",
    "def start_run(run_name=None, experiment_name=None, tags=None):\n",
    "    \"\"\"Start MLflow run\"\"\"\n",
    "    return mlflow.start_run(run_name=run_name, tags=tags)\n",
    "\n",
    "# Model Registry helper functions\n",
    "def create_model_package_group(group_name, description=\"YOLOv11 drone detection models\"):\n",
    "    \"\"\"Create a model package group for organizing model versions\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.create_model_package_group(\n",
    "            ModelPackageGroupName=group_name,\n",
    "            ModelPackageGroupDescription=description\n",
    "        )\n",
    "        print(f\"‚úÖ Created model package group: {group_name}\")\n",
    "        return response\n",
    "    except sagemaker_client.exceptions.ValidationException as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"‚ÑπÔ∏è  Model package group '{group_name}' already exists\")\n",
    "            return None\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "def register_model_version(model_package_group_name, model_data_url, image_uri, \n",
    "                          model_approval_status=\"PendingManualApproval\"):\n",
    "    \"\"\"Register a new model version in the model package group\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.create_model_package(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelPackageDescription=f\"YOLOv11 model version created at {datetime.now().isoformat()}\",\n",
    "            ModelApprovalStatus=model_approval_status,\n",
    "            InferenceSpecification={\n",
    "                'Containers': [{\n",
    "                    'Image': image_uri,\n",
    "                    'ModelDataUrl': model_data_url,\n",
    "                    'Framework': 'PYTORCH',\n",
    "                    'FrameworkVersion': '2.0'\n",
    "                }],\n",
    "                'SupportedContentTypes': ['image/jpeg', 'image/png'],\n",
    "                'SupportedResponseMIMETypes': ['application/json']\n",
    "            }\n",
    "        )\n",
    "        print(f\"‚úÖ Registered model version: {response['ModelPackageArn']}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to register model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ MLflow helper functions loaded\")\n",
    "print(\"‚úÖ Model Registry helper functions loaded\")\n",
    "\n",
    "def get_model_artifacts_from_registry(model_package_arn):\n",
    "    \"\"\"Retrieve model artifacts location from Model Registry (proper way to access artifacts)\"\"\"\n",
    "    try:\n",
    "        # Get model package details from registry\n",
    "        response = sagemaker_client.describe_model_package(\n",
    "            ModelPackageName=model_package_arn\n",
    "        )\n",
    "        \n",
    "        # Extract artifact location from inference specification\n",
    "        containers = response['InferenceSpecification']['Containers']\n",
    "        if containers:\n",
    "            model_data_url = containers[0]['ModelDataUrl']\n",
    "            print(f\"üì¶ Model artifacts (via Model Registry): {model_data_url}\")\n",
    "            \n",
    "            # Additional metadata\n",
    "            approval_status = response['ModelApprovalStatus']\n",
    "            creation_time = response['CreationTime']\n",
    "            \n",
    "            artifact_info = {\n",
    "                'model_package_arn': model_package_arn,\n",
    "                'model_data_url': model_data_url,\n",
    "                'approval_status': approval_status,\n",
    "                'creation_time': creation_time,\n",
    "                'registry_managed': True\n",
    "            }\n",
    "            \n",
    "            return artifact_info\n",
    "        else:\n",
    "            print(\"‚ùå No containers found in model package\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error retrieving model artifacts from registry: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def list_model_artifacts_in_registry(model_package_group_name):\n",
    "    \"\"\"List all model artifacts managed by the Model Registry\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending'\n",
    "        )\n",
    "        \n",
    "        models = response.get('ModelPackageSummaryList', [])\n",
    "        \n",
    "        print(f\"üìã Models in Registry: {model_package_group_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            model_arn = model['ModelPackageArn']\n",
    "            artifact_info = get_model_artifacts_from_registry(model_arn)\n",
    "            \n",
    "            if artifact_info:\n",
    "                print(f\"{i+1}. Model Package: {model_arn.split('/')[-1]}\")\n",
    "                print(f\"   Status: {artifact_info['approval_status']}\")\n",
    "                print(f\"   Artifacts: {artifact_info['model_data_url']}\")\n",
    "                print(f\"   Created: {artifact_info['creation_time'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                print(\"-\" * 80)\n",
    "        \n",
    "        return models\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing models in registry: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Model Registry artifact management functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Model Registry\n",
    "\n",
    "First, let's create the Model Package Group in SageMaker Model Registry if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model package group\n",
    "def create_model_package_group(group_name, description=\"YOLOv11 drone detection models\"):\n",
    "    \"\"\"Create a model package group in SageMaker Model Registry\"\"\"\n",
    "    try:\n",
    "        # Check if group already exists\n",
    "        response = sagemaker_client.describe_model_package_group(\n",
    "            ModelPackageGroupName=group_name\n",
    "        )\n",
    "        print(f\"Model package group '{group_name}' already exists.\")\n",
    "        print(f\"Status: {response['ModelPackageGroupStatus']}\")\n",
    "        return response\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ValidationException':\n",
    "            # Group doesn't exist, create it\n",
    "            print(f\"Creating model package group: {group_name}\")\n",
    "            response = sagemaker_client.create_model_package_group(\n",
    "                ModelPackageGroupName=group_name,\n",
    "                ModelPackageGroupDescription=description\n",
    "            )\n",
    "            print(f\"Created model package group: {response['ModelPackageGroupArn']}\")\n",
    "            return response\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# Create model package group\n",
    "model_package_group = create_model_package_group(MODEL_PACKAGE_GROUP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration\n",
    "\n",
    "Let's configure our YOLOv11 training pipeline parameters with MLFlow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced function to list and validate datasets\n",
    "def list_and_validate_datasets(bucket, prefix=\"datasets/\"):\n",
    "    \"\"\"List available datasets with validation and selection interface\"\"\"\n",
    "    s3_client = session.client('s3')\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "        Delimiter='/'\n",
    "    )\n",
    "    \n",
    "    datasets = []\n",
    "    if 'CommonPrefixes' in response:\n",
    "        for obj in response['CommonPrefixes']:\n",
    "            dataset_prefix = obj['Prefix']\n",
    "            dataset_name = dataset_prefix.split('/')[-2]\n",
    "            \n",
    "            # Validate dataset structure\n",
    "            validation_result = validate_yolo_dataset_structure(bucket, dataset_prefix)\n",
    "            \n",
    "            datasets.append({\n",
    "                'name': dataset_name,\n",
    "                'prefix': dataset_prefix,\n",
    "                'full_path': f's3://{bucket}/{dataset_prefix}',\n",
    "                'valid': validation_result['valid'],\n",
    "                'validation_details': validation_result\n",
    "            })\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "def validate_yolo_dataset_structure(bucket, dataset_prefix):\n",
    "    \"\"\"Validate YOLOv11 dataset structure\"\"\"\n",
    "    s3_client = session.client('s3')\n",
    "    \n",
    "    required_structure = {\n",
    "        'train/images/': False,\n",
    "        'train/labels/': False,\n",
    "        'val/images/': False,\n",
    "        'val/labels/': False,\n",
    "        'data.yaml': False,\n",
    "        'dataset_info.json': False\n",
    "    }\n",
    "    \n",
    "    validation_details = {\n",
    "        'valid': False,\n",
    "        'missing_components': [],\n",
    "        'found_components': [],\n",
    "        'train_image_count': 0,\n",
    "        'val_image_count': 0,\n",
    "        'train_label_count': 0,\n",
    "        'val_label_count': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check for required directories and files\n",
    "        for required_path in required_structure.keys():\n",
    "            full_path = dataset_prefix + required_path\n",
    "            \n",
    "            if required_path.endswith('/'):\n",
    "                # Check directory exists and has content\n",
    "                response = s3_client.list_objects_v2(\n",
    "                    Bucket=bucket,\n",
    "                    Prefix=full_path,\n",
    "                    MaxKeys=1\n",
    "                )\n",
    "                if 'Contents' in response:\n",
    "                    required_structure[required_path] = True\n",
    "                    validation_details['found_components'].append(required_path)\n",
    "                    \n",
    "                    # Count files in image/label directories\n",
    "                    if 'images/' in required_path:\n",
    "                        count_response = s3_client.list_objects_v2(\n",
    "                            Bucket=bucket,\n",
    "                            Prefix=full_path\n",
    "                        )\n",
    "                        count = len(count_response.get('Contents', []))\n",
    "                        if 'train/' in required_path:\n",
    "                            validation_details['train_image_count'] = count\n",
    "                        else:\n",
    "                            validation_details['val_image_count'] = count\n",
    "                    \n",
    "                    elif 'labels/' in required_path:\n",
    "                        count_response = s3_client.list_objects_v2(\n",
    "                            Bucket=bucket,\n",
    "                            Prefix=full_path\n",
    "                        )\n",
    "                        count = len(count_response.get('Contents', []))\n",
    "                        if 'train/' in required_path:\n",
    "                            validation_details['train_label_count'] = count\n",
    "                        else:\n",
    "                            validation_details['val_label_count'] = count\n",
    "                else:\n",
    "                    validation_details['missing_components'].append(required_path)\n",
    "            else:\n",
    "                # Check file exists\n",
    "                try:\n",
    "                    s3_client.head_object(Bucket=bucket, Key=full_path)\n",
    "                    required_structure[required_path] = True\n",
    "                    validation_details['found_components'].append(required_path)\n",
    "                except:\n",
    "                    validation_details['missing_components'].append(required_path)\n",
    "        \n",
    "        # Dataset is valid if all required components are found\n",
    "        validation_details['valid'] = all(required_structure.values())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error validating dataset structure: {e}\")\n",
    "    \n",
    "    return validation_details\n",
    "\n",
    "def display_dataset_selection_interface(datasets):\n",
    "    \"\"\"Display interactive dataset selection interface\"\"\"\n",
    "    if not datasets:\n",
    "        print(\"‚ùå No datasets found in s3://lucaskle-ab3-project-pv/datasets/\")\n",
    "        print(\"\\nExpected dataset structure:\")\n",
    "        print(\"s3://lucaskle-ab3-project-pv/datasets/your_dataset_name/\")\n",
    "        print(\"‚îú‚îÄ‚îÄ train/\")\n",
    "        print(\"‚îÇ   ‚îú‚îÄ‚îÄ images/     ‚úÖ Training images\")\n",
    "        print(\"‚îÇ   ‚îî‚îÄ‚îÄ labels/     ‚úÖ Training labels (.txt files)\")\n",
    "        print(\"‚îú‚îÄ‚îÄ val/\")\n",
    "        print(\"‚îÇ   ‚îú‚îÄ‚îÄ images/     ‚úÖ Validation images\")\n",
    "        print(\"‚îÇ   ‚îî‚îÄ‚îÄ labels/     ‚úÖ Validation labels (.txt files)\")\n",
    "        print(\"‚îú‚îÄ‚îÄ data.yaml       ‚úÖ Dataset configuration\")\n",
    "        print(\"‚îî‚îÄ‚îÄ dataset_info.json ‚úÖ Complete metadata\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä Found {len(datasets)} datasets in s3://{BUCKET_NAME}/datasets/\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    valid_datasets = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        status_icon = \"‚úÖ\" if dataset['valid'] else \"‚ùå\"\n",
    "        print(f\"{i+1}. {status_icon} {dataset['name']}\")\n",
    "        print(f\"   Path: {dataset['full_path']}\")\n",
    "        \n",
    "        if dataset['valid']:\n",
    "            details = dataset['validation_details']\n",
    "            print(f\"   üìà Training: {details['train_image_count']} images, {details['train_label_count']} labels\")\n",
    "            print(f\"   üìä Validation: {details['val_image_count']} images, {details['val_label_count']} labels\")\n",
    "            valid_datasets.append((i, dataset))\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Missing: {', '.join(dataset['validation_details']['missing_components'])}\")\n",
    "        \n",
    "        print(\"-\" * 100)\n",
    "    \n",
    "    if not valid_datasets:\n",
    "        print(\"‚ùå No valid datasets found. Please ensure datasets follow the required structure.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n‚úÖ {len(valid_datasets)} valid dataset(s) available for training\")\n",
    "    return datasets\n",
    "\n",
    "# List and validate all datasets\n",
    "print(\"üîç Discovering and validating datasets...\")\n",
    "available_datasets = list_and_validate_datasets(BUCKET_NAME)\n",
    "validated_datasets = display_dataset_selection_interface(available_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive dataset selection\n",
    "def select_dataset_for_training(datasets):\n",
    "    \"\"\"Allow user to select dataset for training\"\"\"\n",
    "    if not datasets:\n",
    "        return None\n",
    "    \n",
    "    valid_datasets = [d for d in datasets if d['valid']]\n",
    "    \n",
    "    if not valid_datasets:\n",
    "        print(\"No valid datasets available for selection\")\n",
    "        return None\n",
    "    \n",
    "    if len(valid_datasets) == 1:\n",
    "        selected = valid_datasets[0]\n",
    "        print(f\"üéØ Auto-selecting the only valid dataset: {selected['name']}\")\n",
    "        return selected\n",
    "    \n",
    "    print(\"\\nüìã Select a dataset for training:\")\n",
    "    for i, dataset in enumerate(valid_datasets):\n",
    "        details = dataset['validation_details']\n",
    "        total_images = details['train_image_count'] + details['val_image_count']\n",
    "        print(f\"  {i+1}. {dataset['name']} ({total_images} total images)\")\n",
    "    \n",
    "    print(f\"\\nüí° Recommendation: Choose the dataset with the most recent timestamp\")\n",
    "    print(f\"   or the one specifically prepared for your training task.\")\n",
    "    \n",
    "    # For notebook execution, default to the first valid dataset\n",
    "    # In interactive mode, user would input their choice\n",
    "    selected_dataset = valid_datasets[0]  # Default to first valid dataset\n",
    "    \n",
    "    print(f\"\\n‚úÖ Selected dataset: {selected_dataset['name']}\")\n",
    "    print(f\"   Path: {selected_dataset['full_path']}\")\n",
    "    \n",
    "    details = selected_dataset['validation_details']\n",
    "    print(f\"   üìä Training data: {details['train_image_count']} images, {details['train_label_count']} labels\")\n",
    "    print(f\"   üìä Validation data: {details['val_image_count']} images, {details['val_label_count']} labels\")\n",
    "    \n",
    "    return selected_dataset\n",
    "\n",
    "# Select dataset for training\n",
    "if validated_datasets:\n",
    "    selected_dataset = select_dataset_for_training(validated_datasets)\n",
    "    \n",
    "    if selected_dataset:\n",
    "        # Update training parameters with selected dataset\n",
    "        SELECTED_DATASET_NAME = selected_dataset['name']\n",
    "        SELECTED_DATASET_PREFIX = selected_dataset['prefix']\n",
    "        SELECTED_DATASET_PATH = selected_dataset['full_path']\n",
    "        \n",
    "        print(f\"\\nüéØ Dataset ready for training:\")\n",
    "        print(f\"   Name: {SELECTED_DATASET_NAME}\")\n",
    "        print(f\"   S3 Path: {SELECTED_DATASET_PATH}\")\n",
    "    else:\n",
    "        print(\"‚ùå No dataset selected. Cannot proceed with training.\")\n",
    "else:\n",
    "    print(\"‚ùå No datasets available. Please prepare a dataset first using the Data Scientist notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters with selected dataset\n",
    "if 'selected_dataset' in locals() and selected_dataset:\n",
    "    training_params = {\n",
    "        # Dataset parameters (using selected dataset)\n",
    "        'dataset_name': SELECTED_DATASET_NAME,\n",
    "        'dataset_prefix': SELECTED_DATASET_PREFIX,\n",
    "        'dataset_path': SELECTED_DATASET_PATH,\n",
    "        \n",
    "        # Model parameters\n",
    "        'model_variant': 'yolov11n',  # Options: yolov11n, yolov11s, yolov11m, yolov11l, yolov11x\n",
    "        'image_size': 640,  # Input image size (px)\n",
    "        \n",
    "        # Training parameters\n",
    "        'batch_size': 16,\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 0.001,\n",
    "        \n",
    "        # Infrastructure parameters\n",
    "        'instance_type': 'ml.g4dn.xlarge',\n",
    "        'instance_count': 1,\n",
    "        'use_spot': True,\n",
    "        'max_wait': 36000,  # Max wait time for spot instances (seconds)\n",
    "        'max_run': 3600,    # Max run time (seconds)\n",
    "        \n",
    "        # Output parameters\n",
    "        'output_path': f\"s3://{BUCKET_NAME}/model-artifacts/\",\n",
    "        'job_name': f\"yolov11-training-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    "        \n",
    "        # MLFlow parameters\n",
    "        'experiment_name': experiment_name,\n",
    "        'run_name': f\"yolov11-run-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    }\n",
    "    \n",
    "    # Display training parameters\n",
    "    print(\"üöÄ YOLOv11 Training Parameters:\")\n",
    "    print(\"=\" * 60)\n",
    "    for key, value in training_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "    # Display dataset validation summary\n",
    "    if selected_dataset['validation_details']:\n",
    "        details = selected_dataset['validation_details']\n",
    "        print(f\"\\nüìä Selected Dataset Summary:\")\n",
    "        print(f\"  Training Images: {details['train_image_count']}\")\n",
    "        print(f\"  Training Labels: {details['train_label_count']}\")\n",
    "        print(f\"  Validation Images: {details['val_image_count']}\")\n",
    "        print(f\"  Validation Labels: {details['val_label_count']}\")\n",
    "        print(f\"  Total Images: {details['train_image_count'] + details['val_image_count']}\")\n",
    "        \n",
    "        # Validation warnings\n",
    "        if details['train_image_count'] != details['train_label_count']:\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: Training images/labels count mismatch\")\n",
    "        if details['val_image_count'] != details['val_label_count']:\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: Validation images/labels count mismatch\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå Cannot configure training parameters - no dataset selected\")\n",
    "    print(\"Please run the dataset selection cells above first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Execution with MLFlow Tracking\n",
    "\n",
    "Now let's execute the YOLOv11 training pipeline with comprehensive MLFlow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and execute training job with MLFlow tracking\n",
    "def execute_training_job_with_mlflow(params):\n",
    "    \"\"\"Create and execute SageMaker training job for YOLOv11 with MLFlow tracking\"\"\"\n",
    "    \n",
    "    # Start MLFlow run\n",
    "    with mlflow.start_run(run_name=params['run_name']) as run:\n",
    "        # Log parameters to MLFlow\n",
    "        mlflow.log_param(\"model_variant\", params['model_variant'])\n",
    "        mlflow.log_param(\"image_size\", params['image_size'])\n",
    "        mlflow.log_param(\"batch_size\", params['batch_size'])\n",
    "        mlflow.log_param(\"epochs\", params['epochs'])\n",
    "        mlflow.log_param(\"learning_rate\", params['learning_rate'])\n",
    "        mlflow.log_param(\"instance_type\", params['instance_type'])\n",
    "        mlflow.log_param(\"instance_count\", params['instance_count'])\n",
    "        mlflow.log_param(\"use_spot\", params['use_spot'])\n",
    "        mlflow.log_param(\"dataset_name\", params['dataset_name'])\n",
    "        mlflow.log_param(\"dataset_prefix\", params['dataset_prefix'])\n",
    "        \n",
    "        # Define hyperparameters for SageMaker\n",
    "        hyperparameters = {\n",
    "            \"model_variant\": params['model_variant'],\n",
    "            \"image_size\": str(params['image_size']),\n",
    "            \"batch_size\": str(params['batch_size']),\n",
    "            \"epochs\": str(params['epochs']),\n",
    "            \"learning_rate\": str(params['learning_rate']),\n",
    "            \"mlflow_run_id\": run.info.run_id,\n",
    "            \"mlflow_experiment_id\": run.info.experiment_id\n",
    "        }\n",
    "        \n",
    "        # Define input data channels\n",
    "        input_data = {\n",
    "            'training': f\"s3://{BUCKET_NAME}/{params['dataset_prefix']}\"\n",
    "        }\n",
    "        \n",
    "        # Create SageMaker estimator\n",
    "        estimator = sagemaker.estimator.Estimator(\n",
    "            image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-training:latest\",\n",
    "            role=ROLE_ARN,\n",
    "            instance_count=params['instance_count'],\n",
    "            instance_type=params['instance_type'],\n",
    "            hyperparameters=hyperparameters,\n",
    "            output_path=params['output_path'],\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            use_spot_instances=params['use_spot'],\n",
    "            max_wait=params['max_wait'] if params['use_spot'] else None,\n",
    "            max_run=params['max_run']\n",
    "        )\n",
    "        \n",
    "        # Start training job\n",
    "        print(f\"Starting training job: {params['job_name']}\")\n",
    "        print(f\"MLFlow Run ID: {run.info.run_id}\")\n",
    "        \n",
    "        # Log training job details to MLFlow\n",
    "        mlflow.log_param(\"sagemaker_job_name\", params['job_name'])\n",
    "        mlflow.log_param(\"output_path\", params['output_path'])\n",
    "        \n",
    "        # Start the training job\n",
    "        estimator.fit(input_data, job_name=params['job_name'], wait=False)\n",
    "        \n",
    "        # Log additional metadata\n",
    "        mlflow.set_tag(\"sagemaker_job_name\", params['job_name'])\n",
    "        mlflow.set_tag(\"model_type\", \"YOLOv11\")\n",
    "        mlflow.set_tag(\"task_type\", \"object_detection\")\n",
    "        mlflow.set_tag(\"dataset\", params['dataset_name'])\n",
    "        \n",
    "        return params['job_name'], run.info.run_id\n",
    "\n",
    "# Execute training job with MLFlow tracking\n",
    "try:\n",
    "    job_name, mlflow_run_id = execute_training_job_with_mlflow(training_params)\n",
    "    print(f\"\\nTraining job started: {job_name}\")\n",
    "    print(f\"MLFlow Run ID: {mlflow_run_id}\")\n",
    "    print(f\"You can monitor the job in the SageMaker console or using the cell below.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error starting training job: {str(e)}\")\n",
    "    print(\"\\nPossible causes:\")\n",
    "    print(\"1. The dataset doesn't exist or has incorrect structure\")\n",
    "    print(\"2. The YOLOv11 training container doesn't exist in ECR\")\n",
    "    print(\"3. Insufficient permissions to start training job\")\n",
    "    print(\"\\nPlease check the error message and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Monitoring with Enhanced Metrics\n",
    "\n",
    "Let's monitor the progress of our training job and update MLFlow with metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor training job and update MLFlow\n",
    "def monitor_training_job_with_mlflow(job_name, mlflow_run_id):\n",
    "    \"\"\"Monitor SageMaker training job status and update MLFlow\"\"\"\n",
    "    # Get job description\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Extract job status\n",
    "    status = response['TrainingJobStatus']\n",
    "    creation_time = response['CreationTime']\n",
    "    last_modified_time = response.get('LastModifiedTime', creation_time)\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = last_modified_time - creation_time\n",
    "    duration_minutes = duration.total_seconds() / 60\n",
    "    \n",
    "    # Display job information\n",
    "    print(f\"Job Name: {job_name}\")\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Creation Time: {creation_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Last Modified: {last_modified_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Duration: {duration_minutes:.2f} minutes\")\n",
    "    print(f\"MLFlow Run ID: {mlflow_run_id}\")\n",
    "    \n",
    "    # Update MLFlow with job status\n",
    "    with mlflow.start_run(run_id=mlflow_run_id):\n",
    "        mlflow.log_metric(\"training_duration_minutes\", duration_minutes)\n",
    "        mlflow.set_tag(\"job_status\", status)\n",
    "        mlflow.set_tag(\"last_updated\", last_modified_time.isoformat())\n",
    "    \n",
    "    # Display additional information based on status\n",
    "    if status == 'InProgress':\n",
    "        print(\"\\nJob is still running. Check back later for results.\")\n",
    "    elif status == 'Completed':\n",
    "        print(\"\\nJob completed successfully!\")\n",
    "        model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "        print(f\"Model artifacts: {model_artifacts}\")\n",
    "        \n",
    "        # Update MLFlow with completion details\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.log_param(\"model_artifacts_path\", model_artifacts)\n",
    "            mlflow.set_tag(\"training_completed\", \"true\")\n",
    "            \n",
    "    elif status == 'Failed':\n",
    "        print(\"\\nJob failed!\")\n",
    "        failure_reason = response.get('FailureReason', 'Unknown')\n",
    "        print(f\"Failure reason: {failure_reason}\")\n",
    "        \n",
    "        # Update MLFlow with failure details\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.set_tag(\"failure_reason\", failure_reason)\n",
    "            mlflow.set_tag(\"training_failed\", \"true\")\n",
    "            \n",
    "    elif status == 'Stopped':\n",
    "        print(\"\\nJob was stopped.\")\n",
    "        \n",
    "        # Update MLFlow with stopped status\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.set_tag(\"training_stopped\", \"true\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Monitor the training job\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        job_response = monitor_training_job_with_mlflow(job_name, mlflow_run_id)\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error monitoring training job: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh job status (run this cell to update status)\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        job_response = monitor_training_job_with_mlflow(job_name, mlflow_run_id)\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error monitoring training job: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Automated Model Validation Before Registration\n",
    "\n",
    "Before registering models, let's implement automated validation to ensure model quality and performance thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to register model in Model Registry\n",
    "def register_model_in_registry(job_name, mlflow_run_id, model_package_group_name):\n",
    "    \"\"\"Register trained model in SageMaker Model Registry with proper artifact management\"\"\"\n",
    "    \n",
    "    # Get training job details\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Check if job is completed\n",
    "    if response['TrainingJobStatus'] != 'Completed':\n",
    "        print(f\"Training job is not completed yet. Status: {response['TrainingJobStatus']}\")\n",
    "        return None\n",
    "    \n",
    "    # Get model artifacts from training job (SageMaker automatically stores these)\n",
    "    model_artifacts_s3_uri = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    print(f\"üì¶ Model artifacts location: {model_artifacts_s3_uri}\")\n",
    "    \n",
    "    # Create model package with proper artifact reference\n",
    "    model_package_name = f\"yolov11-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    \n",
    "    # Define inference specification with proper artifact reference\n",
    "    inference_specification = {\n",
    "        'Containers': [\n",
    "            {\n",
    "                'Image': f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-inference:latest\",\n",
    "                'ModelDataUrl': model_artifacts_s3_uri,  # Reference to S3 artifacts\n",
    "                'Framework': 'PYTORCH',\n",
    "                'FrameworkVersion': '2.0',\n",
    "                'Environment': {\n",
    "                    'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "                    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code'\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'SupportedContentTypes': ['application/json', 'image/jpeg', 'image/png'],\n",
    "        'SupportedResponseMIMETypes': ['application/json'],\n",
    "        'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium', 'ml.m5.large', 'ml.m5.xlarge']\n",
    "    }\n",
    "    \n",
    "    # Create model package in registry\n",
    "    try:\n",
    "        create_response = sagemaker_client.create_model_package(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelPackageDescription=f\"YOLOv11 drone detection model trained from job {job_name}\",\n",
    "            InferenceSpecification=inference_specification,\n",
    "            ModelApprovalStatus='PendingManualApproval',\n",
    "            MetadataProperties={\n",
    "                'GeneratedBy': f'sagemaker-training-job-{job_name}',\n",
    "                'ProjectId': 'yolov11-drone-detection',\n",
    "                'Repository': 'mlops-sagemaker-demo',\n",
    "                'ModelArtifactsS3Uri': model_artifacts_s3_uri,  # Store S3 reference in metadata\n",
    "                'TrainingJobArn': response['TrainingJobArn']\n",
    "            },\n",
    "            Tags=[\n",
    "                {'Key': 'Project', 'Value': 'MLOps-SageMaker-Demo'},\n",
    "                {'Key': 'Model', 'Value': 'YOLOv11'},\n",
    "                {'Key': 'Task', 'Value': 'ObjectDetection'},\n",
    "                {'Key': 'TrainingJob', 'Value': job_name},\n",
    "                {'Key': 'MLFlowRunId', 'Value': mlflow_run_id},\n",
    "                {'Key': 'ModelArtifactsS3Uri', 'Value': model_artifacts_s3_uri}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        model_package_arn = create_response['ModelPackageArn']\n",
    "        print(f\"‚úÖ Model registered successfully in Model Registry!\")\n",
    "        print(f\"üìã Model Package ARN: {model_package_arn}\")\n",
    "        print(f\"üì¶ Artifacts managed through Model Registry (stored in S3: {model_artifacts_s3_uri})\")\n",
    "        \n",
    "        # Update MLFlow with model registration details\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.log_param(\"model_package_arn\", model_package_arn)\n",
    "            mlflow.log_param(\"model_package_group\", model_package_group_name)\n",
    "            mlflow.log_param(\"model_artifacts_s3_uri\", model_artifacts_s3_uri)\n",
    "            mlflow.set_tag(\"model_registered\", \"true\")\n",
    "            mlflow.set_tag(\"model_approval_status\", \"PendingManualApproval\")\n",
    "            mlflow.set_tag(\"model_registry_managed\", \"true\")\n",
    "            \n",
    "            # Log model reference (not the actual artifacts)\n",
    "            mlflow.log_param(\"model_registry_reference\", model_package_arn)\n",
    "        \n",
    "        return model_package_arn\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error registering model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Register the model\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        model_package_arn = register_model_in_registry(job_name, mlflow_run_id, MODEL_PACKAGE_GROUP_NAME)\n",
    "        if model_package_arn:\n",
    "            print(f\"\\nModel registration completed!\")\n",
    "            print(f\"You can view the model in the SageMaker Model Registry console.\")\n",
    "    else:\n",
    "        print(\"No completed training job to register.\")\n",
    "        print(\"Please complete a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model registration: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to validate model performance before registration\n",
    "def validate_model_performance(job_name, mlflow_run_id, min_map50=0.3, min_map50_95=0.2):\n",
    "    \"\"\"\n",
    "    Validate model performance against minimum thresholds before registration\n",
    "    \n",
    "    Args:\n",
    "        job_name: SageMaker training job name\n",
    "        mlflow_run_id: MLFlow run ID\n",
    "        min_map50: Minimum mAP@0.5 threshold\n",
    "        min_map50_95: Minimum mAP@0.5:0.95 threshold\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation results with pass/fail status and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get training job details\n",
    "    try:\n",
    "        response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "        \n",
    "        if response['TrainingJobStatus'] != 'Completed':\n",
    "            return {\n",
    "                'validation_passed': False,\n",
    "                'reason': f\"Training job not completed. Status: {response['TrainingJobStatus']}\",\n",
    "                'metrics': {}\n",
    "            }\n",
    "        \n",
    "        # Get final metrics from CloudWatch\n",
    "        cloudwatch = session.client('cloudwatch')\n",
    "        \n",
    "        # Retrieve validation metrics\n",
    "        validation_metrics = {}\n",
    "        metric_names = ['val:mAP50', 'val:mAP50-95', 'val:precision', 'val:recall']\n",
    "        \n",
    "        for metric_name in metric_names:\n",
    "            try:\n",
    "                cw_response = cloudwatch.get_metric_statistics(\n",
    "                    Namespace='SageMaker',\n",
    "                    MetricName=metric_name,\n",
    "                    Dimensions=[{'Name': 'TrainingJobName', 'Value': job_name}],\n",
    "                    StartTime=response['CreationTime'],\n",
    "                    EndTime=response['LastModifiedTime'],\n",
    "                    Period=300,  # 5-minute periods\n",
    "                    Statistics=['Maximum']  # Get best performance\n",
    "                )\n",
    "                \n",
    "                datapoints = cw_response.get('Datapoints', [])\n",
    "                if datapoints:\n",
    "                    # Get the maximum value (best performance)\n",
    "                    max_value = max(dp['Maximum'] for dp in datapoints)\n",
    "                    validation_metrics[metric_name] = max_value\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not retrieve metric {metric_name}: {str(e)}\")\n",
    "        \n",
    "        # Perform validation checks\n",
    "        validation_results = {\n",
    "            'validation_passed': True,\n",
    "            'reason': 'All validation checks passed',\n",
    "            'metrics': validation_metrics,\n",
    "            'thresholds': {\n",
    "                'min_map50': min_map50,\n",
    "                'min_map50_95': min_map50_95\n",
    "            },\n",
    "            'checks': []\n",
    "        }\n",
    "        \n",
    "        # Check mAP@0.5 threshold\n",
    "        if 'val:mAP50' in validation_metrics:\n",
    "            map50_value = validation_metrics['val:mAP50']\n",
    "            if map50_value >= min_map50:\n",
    "                validation_results['checks'].append({\n",
    "                    'metric': 'mAP@0.5',\n",
    "                    'value': map50_value,\n",
    "                    'threshold': min_map50,\n",
    "                    'passed': True\n",
    "                })\n",
    "            else:\n",
    "                validation_results['validation_passed'] = False\n",
    "                validation_results['reason'] = f\"mAP@0.5 ({map50_value:.3f}) below threshold ({min_map50})\"\n",
    "                validation_results['checks'].append({\n",
    "                    'metric': 'mAP@0.5',\n",
    "                    'value': map50_value,\n",
    "                    'threshold': min_map50,\n",
    "                    'passed': False\n",
    "                })\n",
    "        \n",
    "        # Check mAP@0.5:0.95 threshold\n",
    "        if 'val:mAP50-95' in validation_metrics:\n",
    "            map50_95_value = validation_metrics['val:mAP50-95']\n",
    "            if map50_95_value >= min_map50_95:\n",
    "                validation_results['checks'].append({\n",
    "                    'metric': 'mAP@0.5:0.95',\n",
    "                    'value': map50_95_value,\n",
    "                    'threshold': min_map50_95,\n",
    "                    'passed': True\n",
    "                })\n",
    "            else:\n",
    "                validation_results['validation_passed'] = False\n",
    "                validation_results['reason'] = f\"mAP@0.5:0.95 ({map50_95_value:.3f}) below threshold ({min_map50_95})\"\n",
    "                validation_results['checks'].append({\n",
    "                    'metric': 'mAP@0.5:0.95',\n",
    "                    'value': map50_95_value,\n",
    "                    'threshold': min_map50_95,\n",
    "                    'passed': False\n",
    "                })\n",
    "        \n",
    "        # Log validation results to MLFlow\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.log_param(\"validation_passed\", validation_results['validation_passed'])\n",
    "            mlflow.log_param(\"validation_reason\", validation_results['reason'])\n",
    "            \n",
    "            for check in validation_results['checks']:\n",
    "                mlflow.log_metric(f\"validation_{check['metric'].replace('@', '_').replace(':', '_')}\", check['value'])\n",
    "                mlflow.log_param(f\"threshold_{check['metric'].replace('@', '_').replace(':', '_')}\", check['threshold'])\n",
    "            \n",
    "            mlflow.set_tag(\"model_validation_status\", \"passed\" if validation_results['validation_passed'] else \"failed\")\n",
    "        \n",
    "        # Display validation results\n",
    "        print(\"üîç Model Validation Results\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Overall Status: {'‚úÖ PASSED' if validation_results['validation_passed'] else '‚ùå FAILED'}\")\n",
    "        print(f\"Reason: {validation_results['reason']}\")\n",
    "        print(\"\\nDetailed Checks:\")\n",
    "        \n",
    "        for check in validation_results['checks']:\n",
    "            status = \"‚úÖ PASS\" if check['passed'] else \"‚ùå FAIL\"\n",
    "            print(f\"  {check['metric']}: {check['value']:.3f} (threshold: {check['threshold']}) - {status}\")\n",
    "        \n",
    "        if validation_metrics:\n",
    "            print(\"\\nAll Retrieved Metrics:\")\n",
    "            for metric, value in validation_metrics.items():\n",
    "                print(f\"  {metric}: {value:.3f}\")\n",
    "        \n",
    "        return validation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_result = {\n",
    "            'validation_passed': False,\n",
    "            'reason': f\"Validation error: {str(e)}\",\n",
    "            'metrics': {}\n",
    "        }\n",
    "        \n",
    "        # Log error to MLFlow\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.set_tag(\"model_validation_status\", \"error\")\n",
    "            mlflow.set_tag(\"validation_error\", str(e))\n",
    "        \n",
    "        return error_result\n",
    "\n",
    "# Validate the model before registration\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        print(\"Starting automated model validation...\")\n",
    "        validation_results = validate_model_performance(\n",
    "            job_name, \n",
    "            mlflow_run_id,\n",
    "            min_map50=0.3,      # Minimum 30% mAP@0.5\n",
    "            min_map50_95=0.2    # Minimum 20% mAP@0.5:0.95\n",
    "        )\n",
    "        \n",
    "        if validation_results['validation_passed']:\n",
    "            print(\"\\nüéâ Model validation passed! Proceeding with registration...\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Model validation failed: {validation_results['reason']}\")\n",
    "            print(\"Consider retraining with different parameters or adjusting thresholds.\")\n",
    "    else:\n",
    "        print(\"No completed training job to validate.\")\n",
    "        print(\"Please complete a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model validation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Management and Approval Workflow\n",
    "\n",
    "Let's manage the registered models and handle approval workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list models in the registry\n",
    "def list_models_in_registry(model_package_group_name):\n",
    "    \"\"\"List all models in the Model Registry\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending'\n",
    "        )\n",
    "        \n",
    "        models = response.get('ModelPackageSummaryList', [])\n",
    "        \n",
    "        if not models:\n",
    "            print(f\"No models found in group: {model_package_group_name}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(models)} models in group: {model_package_group_name}\")\n",
    "        print(\"\\nModel List:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            print(f\"{i+1}. Model Package ARN: {model['ModelPackageArn']}\")\n",
    "            print(f\"   Status: {model['ModelPackageStatus']}\")\n",
    "            print(f\"   Approval Status: {model['ModelApprovalStatus']}\")\n",
    "            print(f\"   Creation Time: {model['CreationTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            if 'ModelPackageDescription' in model:\n",
    "                print(f\"   Description: {model['ModelPackageDescription']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        return models\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# List models in registry\n",
    "models = list_models_in_registry(MODEL_PACKAGE_GROUP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to approve a model\n",
    "def approve_model(model_package_arn, approval_description=\"Model approved for deployment\"):\n",
    "    \"\"\"Approve a model in the Model Registry\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.update_model_package(\n",
    "            ModelPackageArn=model_package_arn,\n",
    "            ModelApprovalStatus='Approved',\n",
    "            ApprovalDescription=approval_description\n",
    "        )\n",
    "        \n",
    "        print(f\"Model approved successfully!\")\n",
    "        print(f\"Model Package ARN: {model_package_arn}\")\n",
    "        print(f\"Approval Description: {approval_description}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error approving model: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Example: Approve the latest model (uncomment to use)\n",
    "# if models:\n",
    "#     latest_model_arn = models[0]['ModelPackageArn']\n",
    "#     approve_model(latest_model_arn, \"Model approved after validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get model details\n",
    "def get_model_details(model_package_arn):\n",
    "    \"\"\"Get detailed information about a model\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.describe_model_package(\n",
    "            ModelPackageName=model_package_arn\n",
    "        )\n",
    "        \n",
    "        print(f\"Model Package Details:\")\n",
    "        print(f\"ARN: {response['ModelPackageArn']}\")\n",
    "        print(f\"Status: {response['ModelPackageStatus']}\")\n",
    "        print(f\"Approval Status: {response['ModelApprovalStatus']}\")\n",
    "        print(f\"Creation Time: {response['CreationTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if 'ModelPackageDescription' in response:\n",
    "            print(f\"Description: {response['ModelPackageDescription']}\")\n",
    "        \n",
    "        if 'InferenceSpecification' in response:\n",
    "            containers = response['InferenceSpecification']['Containers']\n",
    "            print(f\"\\nInference Specification:\")\n",
    "            for i, container in enumerate(containers):\n",
    "                print(f\"  Container {i+1}:\")\n",
    "                print(f\"    Image: {container['Image']}\")\n",
    "                print(f\"    Model Data: {container['ModelDataUrl']}\")\n",
    "        \n",
    "        if 'Tags' in response:\n",
    "            print(f\"\\nTags:\")\n",
    "            for tag in response['Tags']:\n",
    "                print(f\"  {tag['Key']}: {tag['Value']}\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting model details: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example: Get details of the latest model (uncomment to use)\n",
    "# if models:\n",
    "#     latest_model_arn = models[0]['ModelPackageArn']\n",
    "#     model_details = get_model_details(latest_model_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automated Deployment for Approved Models\n",
    "\n",
    "Implement automated deployment pipeline that triggers when models are approved in the Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list MLFlow experiments\n",
    "def list_mlflow_experiments():\n",
    "    \"\"\"List all MLFlow experiments\"\"\"\n",
    "    try:\n",
    "        experiments = mlflow.search_experiments()\n",
    "        \n",
    "        print(f\"Found {len(experiments)} experiments:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for exp in experiments:\n",
    "            print(f\"Experiment ID: {exp.experiment_id}\")\n",
    "            print(f\"Name: {exp.name}\")\n",
    "            print(f\"Lifecycle Stage: {exp.lifecycle_stage}\")\n",
    "            if exp.tags:\n",
    "                print(f\"Tags: {exp.tags}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        return experiments\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing experiments: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# List experiments\n",
    "experiments = list_mlflow_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search MLFlow runs\n",
    "def search_mlflow_runs(experiment_name, max_results=10):\n",
    "    \"\"\"Search MLFlow runs in an experiment\"\"\"\n",
    "    try:\n",
    "        # Get experiment by name\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if not experiment:\n",
    "            print(f\"Experiment '{experiment_name}' not found\")\n",
    "            return []\n",
    "        \n",
    "        # Search runs\n",
    "        runs = mlflow.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            max_results=max_results,\n",
    "            order_by=[\"start_time DESC\"]\n",
    "        )\n",
    "        \n",
    "        if runs.empty:\n",
    "            print(f\"No runs found in experiment '{experiment_name}'\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(runs)} runs in experiment '{experiment_name}':\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Display run information\n",
    "        for idx, run in runs.iterrows():\n",
    "            print(f\"Run ID: {run['run_id']}\")\n",
    "            print(f\"Status: {run['status']}\")\n",
    "            print(f\"Start Time: {run['start_time']}\")\n",
    "            \n",
    "            # Display parameters\n",
    "            param_cols = [col for col in runs.columns if col.startswith('params.')]\n",
    "            if param_cols:\n",
    "                print(\"Parameters:\")\n",
    "                for param_col in param_cols:\n",
    "                    param_name = param_col.replace('params.', '')\n",
    "                    param_value = run[param_col]\n",
    "                    if pd.notna(param_value):\n",
    "                        print(f\"  {param_name}: {param_value}\")\n",
    "            \n",
    "            # Display metrics\n",
    "            metric_cols = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "            if metric_cols:\n",
    "                print(\"Metrics:\")\n",
    "                for metric_col in metric_cols:\n",
    "                    metric_name = metric_col.replace('metrics.', '')\n",
    "                    metric_value = run[metric_col]\n",
    "                    if pd.notna(metric_value):\n",
    "                        print(f\"  {metric_name}: {metric_value}\")\n",
    "            \n",
    "            # Display tags\n",
    "            tag_cols = [col for col in runs.columns if col.startswith('tags.')]\n",
    "            if tag_cols:\n",
    "                print(\"Tags:\")\n",
    "                for tag_col in tag_cols:\n",
    "                    tag_name = tag_col.replace('tags.', '')\n",
    "                    tag_value = run[tag_col]\n",
    "                    if pd.notna(tag_value):\n",
    "                        print(f\"  {tag_name}: {tag_value}\")\n",
    "            \n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        return runs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching runs: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Search runs in the current experiment\n",
    "runs_df = search_mlflow_runs(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare runs\n",
    "def compare_runs(runs_df, metrics_to_compare=['training_duration_minutes']):\n",
    "    \"\"\"Compare MLFlow runs\"\"\"\n",
    "    if runs_df.empty:\n",
    "        print(\"No runs to compare\")\n",
    "        return\n",
    "    \n",
    "    print(\"Run Comparison:\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_cols = ['run_id', 'status', 'start_time']\n",
    "    \n",
    "    # Add parameter columns\n",
    "    param_cols = [col for col in runs_df.columns if col.startswith('params.')]\n",
    "    comparison_cols.extend(param_cols)\n",
    "    \n",
    "    # Add metric columns\n",
    "    for metric in metrics_to_compare:\n",
    "        metric_col = f'metrics.{metric}'\n",
    "        if metric_col in runs_df.columns:\n",
    "            comparison_cols.append(metric_col)\n",
    "    \n",
    "    # Display comparison table\n",
    "    comparison_df = runs_df[comparison_cols].copy()\n",
    "    \n",
    "    # Rename columns for better display\n",
    "    column_mapping = {}\n",
    "    for col in comparison_df.columns:\n",
    "        if col.startswith('params.'):\n",
    "            column_mapping[col] = col.replace('params.', 'param_')\n",
    "        elif col.startswith('metrics.'):\n",
    "            column_mapping[col] = col.replace('metrics.', 'metric_')\n",
    "    \n",
    "    comparison_df = comparison_df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Display the comparison\n",
    "    display(comparison_df)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare runs if available\n",
    "if not runs_df.empty:\n",
    "    comparison_df = compare_runs(runs_df)\n",
    "else:\n",
    "    print(\"No runs available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to automatically deploy approved models\n",
    "def deploy_approved_model(model_package_arn, endpoint_name=None, instance_type='ml.m5.large', \n",
    "                         initial_instance_count=1, enable_autoscaling=True):\n",
    "    \"\"\"\n",
    "    Automatically deploy an approved model to a SageMaker endpoint\n",
    "    \n",
    "    Args:\n",
    "        model_package_arn: ARN of the approved model package\n",
    "        endpoint_name: Name for the endpoint (auto-generated if None)\n",
    "        instance_type: EC2 instance type for the endpoint\n",
    "        initial_instance_count: Initial number of instances\n",
    "        enable_autoscaling: Whether to enable auto-scaling\n",
    "    \n",
    "    Returns:\n",
    "        dict: Deployment results with endpoint details\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check model approval status\n",
    "        model_details = sagemaker_client.describe_model_package(\n",
    "            ModelPackageName=model_package_arn\n",
    "        )\n",
    "        \n",
    "        if model_details['ModelApprovalStatus'] != 'Approved':\n",
    "            return {\n",
    "                'deployment_success': False,\n",
    "                'reason': f\"Model not approved. Status: {model_details['ModelApprovalStatus']}\",\n",
    "                'endpoint_name': None\n",
    "            }\n",
    "        \n",
    "        # Generate endpoint name if not provided\n",
    "        if endpoint_name is None:\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "            endpoint_name = f\"yolov11-endpoint-{timestamp}\"\n",
    "        \n",
    "        # Create SageMaker model from model package\n",
    "        model_name = f\"yolov11-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "        \n",
    "        create_model_response = sagemaker_client.create_model(\n",
    "            ModelName=model_name,\n",
    "            Containers=model_details['InferenceSpecification']['Containers'],\n",
    "            ExecutionRoleArn=ROLE_ARN,\n",
    "            Tags=[\n",
    "                {'Key': 'Project', 'Value': 'MLOps-SageMaker-Demo'},\n",
    "                {'Key': 'ModelPackageArn', 'Value': model_package_arn},\n",
    "                {'Key': 'AutoDeployed', 'Value': 'true'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Created SageMaker model: {model_name}\")\n",
    "        \n",
    "        # Create endpoint configuration\n",
    "        endpoint_config_name = f\"yolov11-config-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "        \n",
    "        create_endpoint_config_response = sagemaker_client.create_endpoint_configuration(\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            ProductionVariants=[\n",
    "                {\n",
    "                    'VariantName': 'primary',\n",
    "                    'ModelName': model_name,\n",
    "                    'InitialInstanceCount': initial_instance_count,\n",
    "                    'InstanceType': instance_type,\n",
    "                    'InitialVariantWeight': 1.0\n",
    "                }\n",
    "            ],\n",
    "            Tags=[\n",
    "                {'Key': 'Project', 'Value': 'MLOps-SageMaker-Demo'},\n",
    "                {'Key': 'ModelPackageArn', 'Value': model_package_arn},\n",
    "                {'Key': 'AutoDeployed', 'Value': 'true'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Created endpoint configuration: {endpoint_config_name}\")\n",
    "        \n",
    "        # Create endpoint\n",
    "        create_endpoint_response = sagemaker_client.create_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            EndpointConfigName=endpoint_config_name,\n",
    "            Tags=[\n",
    "                {'Key': 'Project', 'Value': 'MLOps-SageMaker-Demo'},\n",
    "                {'Key': 'ModelPackageArn', 'Value': model_package_arn},\n",
    "                {'Key': 'AutoDeployed', 'Value': 'true'}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Creating endpoint: {endpoint_name}\")\n",
    "        print(\"‚è≥ Endpoint creation in progress... This may take 5-10 minutes.\")\n",
    "        \n",
    "        # Set up auto-scaling if enabled\n",
    "        if enable_autoscaling:\n",
    "            try:\n",
    "                autoscaling_client = session.client('application-autoscaling')\n",
    "                \n",
    "                # Register scalable target\n",
    "                autoscaling_client.register_scalable_target(\n",
    "                    ServiceNamespace='sagemaker',\n",
    "                    ResourceId=f'endpoint/{endpoint_name}/variant/primary',\n",
    "                    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "                    MinCapacity=1,\n",
    "                    MaxCapacity=5,\n",
    "                    RoleArn=ROLE_ARN\n",
    "                )\n",
    "                \n",
    "                # Create scaling policy\n",
    "                autoscaling_client.put_scaling_policy(\n",
    "                    PolicyName=f'{endpoint_name}-scaling-policy',\n",
    "                    ServiceNamespace='sagemaker',\n",
    "                    ResourceId=f'endpoint/{endpoint_name}/variant/primary',\n",
    "                    ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "                    PolicyType='TargetTrackingScaling',\n",
    "                    TargetTrackingScalingPolicyConfiguration={\n",
    "                        'TargetValue': 70.0,\n",
    "                        'PredefinedMetricSpecification': {\n",
    "                            'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'\n",
    "                        },\n",
    "                        'ScaleOutCooldown': 300,\n",
    "                        'ScaleInCooldown': 300\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                print(\"‚úÖ Auto-scaling configured for endpoint\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Auto-scaling setup failed: {str(e)}\")\n",
    "        \n",
    "        deployment_result = {\n",
    "            'deployment_success': True,\n",
    "            'reason': 'Model deployed successfully',\n",
    "            'endpoint_name': endpoint_name,\n",
    "            'model_name': model_name,\n",
    "            'endpoint_config_name': endpoint_config_name,\n",
    "            'model_package_arn': model_package_arn,\n",
    "            'instance_type': instance_type,\n",
    "            'initial_instance_count': initial_instance_count,\n",
    "            'autoscaling_enabled': enable_autoscaling\n",
    "        }\n",
    "        \n",
    "        return deployment_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'deployment_success': False,\n",
    "            'reason': f\"Deployment error: {str(e)}\",\n",
    "            'endpoint_name': endpoint_name\n",
    "        }\n",
    "\n",
    "# Function to monitor endpoint deployment\n",
    "def monitor_endpoint_deployment(endpoint_name, max_wait_minutes=15):\n",
    "    \"\"\"Monitor endpoint deployment status\"\"\"\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    max_wait_seconds = max_wait_minutes * 60\n",
    "    \n",
    "    print(f\"üîç Monitoring endpoint deployment: {endpoint_name}\")\n",
    "    print(f\"Maximum wait time: {max_wait_minutes} minutes\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "            status = response['EndpointStatus']\n",
    "            \n",
    "            elapsed_time = datetime.now() - start_time\n",
    "            elapsed_minutes = elapsed_time.total_seconds() / 60\n",
    "            \n",
    "            print(f\"Status: {status} | Elapsed: {elapsed_minutes:.1f} minutes\")\n",
    "            \n",
    "            if status == 'InService':\n",
    "                print(\"‚úÖ Endpoint is now in service!\")\n",
    "                return True\n",
    "            elif status == 'Failed':\n",
    "                failure_reason = response.get('FailureReason', 'Unknown')\n",
    "                print(f\"‚ùå Endpoint deployment failed: {failure_reason}\")\n",
    "                return False\n",
    "            elif elapsed_time.total_seconds() > max_wait_seconds:\n",
    "                print(f\"‚è∞ Timeout reached ({max_wait_minutes} minutes)\")\n",
    "                return False\n",
    "            \n",
    "            time.sleep(30)  # Wait 30 seconds before checking again\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error monitoring endpoint: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "# Example: Deploy the latest approved model (uncomment to use)\n",
    "# if models:\n",
    "#     # Find the first approved model\n",
    "#     approved_models = [m for m in models if m['ModelApprovalStatus'] == 'Approved']\n",
    "#     \n",
    "#     if approved_models:\n",
    "#         latest_approved = approved_models[0]\n",
    "#         print(f\"Deploying approved model: {latest_approved['ModelPackageArn']}\")\n",
    "#         \n",
    "#         deployment_result = deploy_approved_model(\n",
    "#             model_package_arn=latest_approved['ModelPackageArn'],\n",
    "#             instance_type='ml.m5.large',\n",
    "#             initial_instance_count=1,\n",
    "#             enable_autoscaling=True\n",
    "#         )\n",
    "#         \n",
    "#         if deployment_result['deployment_success']:\n",
    "#             print(f\"\\nüöÄ Deployment initiated successfully!\")\n",
    "#             print(f\"Endpoint name: {deployment_result['endpoint_name']}\")\n",
    "#             \n",
    "#             # Monitor deployment\n",
    "#             success = monitor_endpoint_deployment(deployment_result['endpoint_name'])\n",
    "#             \n",
    "#             if success:\n",
    "#                 print(f\"\\nüéâ Model successfully deployed to endpoint: {deployment_result['endpoint_name']}\")\n",
    "#             else:\n",
    "#                 print(f\"\\n‚ö†Ô∏è  Deployment monitoring ended. Check SageMaker console for details.\")\n",
    "#         else:\n",
    "#             print(f\"\\n‚ùå Deployment failed: {deployment_result['reason']}\")\n",
    "#     else:\n",
    "#         print(\"No approved models found for deployment.\")\n",
    "# else:\n",
    "#     print(\"No models available for deployment.\")\n",
    "\n",
    "print(\"‚úÖ Automated deployment functions loaded\")\n",
    "print(\"Uncomment the example code above to deploy an approved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Metrics Visualization\n",
    "\n",
    "Let's visualize training metrics from completed jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get training metrics from CloudWatch\n",
    "def get_training_metrics_with_mlflow(job_name, mlflow_run_id):\n",
    "    \"\"\"Get training metrics from CloudWatch and log to MLFlow\"\"\"\n",
    "    # Get job description\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Check if job is complete\n",
    "    if response['TrainingJobStatus'] != 'Completed':\n",
    "        print(f\"Job is not yet complete. Current status: {response['TrainingJobStatus']}\")\n",
    "        return None\n",
    "    \n",
    "    # Get CloudWatch metrics\n",
    "    cloudwatch = session.client('cloudwatch')\n",
    "    \n",
    "    # Define metrics to retrieve\n",
    "    metrics = [\n",
    "        'train:loss',\n",
    "        'val:loss',\n",
    "        'val:mAP50',\n",
    "        'val:mAP50-95'\n",
    "    ]\n",
    "    \n",
    "    # Get metrics data\n",
    "    metrics_data = {}\n",
    "    final_metrics = {}\n",
    "    \n",
    "    for metric_name in metrics:\n",
    "        try:\n",
    "            cw_response = cloudwatch.get_metric_statistics(\n",
    "                Namespace='SageMaker',\n",
    "                MetricName=metric_name,\n",
    "                Dimensions=[\n",
    "                    {\n",
    "                        'Name': 'TrainingJobName',\n",
    "                        'Value': job_name\n",
    "                    }\n",
    "                ],\n",
    "                StartTime=response['CreationTime'],\n",
    "                EndTime=response['LastModifiedTime'],\n",
    "                Period=60,  # 1-minute periods\n",
    "                Statistics=['Average']\n",
    "            )\n",
    "            \n",
    "            # Extract datapoints\n",
    "            datapoints = cw_response.get('Datapoints', [])\n",
    "            if datapoints:\n",
    "                # Sort by timestamp\n",
    "                datapoints.sort(key=lambda x: x['Timestamp'])\n",
    "                \n",
    "                # Extract values\n",
    "                timestamps = [dp['Timestamp'] for dp in datapoints]\n",
    "                values = [dp['Average'] for dp in datapoints]\n",
    "                \n",
    "                metrics_data[metric_name] = {\n",
    "                    'timestamps': timestamps,\n",
    "                    'values': values\n",
    "                }\n",
    "                \n",
    "                # Store final metric value\n",
    "                if values:\n",
    "                    final_metrics[metric_name] = values[-1]\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving metric {metric_name}: {str(e)}\")\n",
    "    \n",
    "    # Log final metrics to MLFlow\n",
    "    if final_metrics:\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            for metric_name, value in final_metrics.items():\n",
    "                # Clean metric name for MLFlow\n",
    "                clean_name = metric_name.replace(':', '_')\n",
    "                mlflow.log_metric(clean_name, value)\n",
    "    \n",
    "    return metrics_data, final_metrics\n",
    "\n",
    "# Get and visualize training metrics\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        metrics_data, final_metrics = get_training_metrics_with_mlflow(job_name, mlflow_run_id)\n",
    "        \n",
    "        if metrics_data:\n",
    "            # Plot metrics\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "            \n",
    "            # Plot loss\n",
    "            if 'train:loss' in metrics_data:\n",
    "                axes[0].plot(\n",
    "                    metrics_data['train:loss']['timestamps'],\n",
    "                    metrics_data['train:loss']['values'],\n",
    "                    label='Train Loss',\n",
    "                    marker='o'\n",
    "                )\n",
    "            \n",
    "            if 'val:loss' in metrics_data:\n",
    "                axes[0].plot(\n",
    "                    metrics_data['val:loss']['timestamps'],\n",
    "                    metrics_data['val:loss']['values'],\n",
    "                    label='Validation Loss',\n",
    "                    marker='s'\n",
    "                )\n",
    "            \n",
    "            axes[0].set_title('Training and Validation Loss')\n",
    "            axes[0].set_xlabel('Time')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot mAP\n",
    "            if 'val:mAP50' in metrics_data:\n",
    "                axes[1].plot(\n",
    "                    metrics_data['val:mAP50']['timestamps'],\n",
    "                    metrics_data['val:mAP50']['values'],\n",
    "                    label='mAP@0.5',\n",
    "                    marker='o'\n",
    "                )\n",
    "            \n",
    "            if 'val:mAP50-95' in metrics_data:\n",
    "                axes[1].plot(\n",
    "                    metrics_data['val:mAP50-95']['timestamps'],\n",
    "                    metrics_data['val:mAP50-95']['values'],\n",
    "                    label='mAP@0.5:0.95',\n",
    "                    marker='s'\n",
    "                )\n",
    "            \n",
    "            axes[1].set_title('Validation mAP')\n",
    "            axes[1].set_xlabel('Time')\n",
    "            axes[1].set_ylabel('mAP')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display final metrics\n",
    "            if final_metrics:\n",
    "                print(\"\\nFinal Training Metrics:\")\n",
    "                print(\"=\" * 40)\n",
    "                for metric_name, value in final_metrics.items():\n",
    "                    print(f\"{metric_name}: {value:.4f}\")\n",
    "        else:\n",
    "            print(\"No metrics available yet. Job may still be running or has failed.\")\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving training metrics: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Model Performance Comparison\n",
    "\n",
    "Let's implement comprehensive model performance comparison utilities to analyze and compare different model versions across multiple dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1: Performance Comparison Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model performance comparison utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "def get_model_performance_metrics(model_package_arn):\n",
    "    \"\"\"Extract performance metrics from a registered model (using Model Registry as source)\"\"\"\n",
    "    try:\n",
    "        # Get model package details from Model Registry (primary source)\n",
    "        response = sagemaker_client.describe_model_package(\n",
    "            ModelPackageName=model_package_arn\n",
    "        )\n",
    "        \n",
    "        # Get model artifacts location from Model Registry\n",
    "        artifact_info = get_model_artifacts_from_registry(model_package_arn)\n",
    "        \n",
    "        # Extract training job name from tags or metadata\n",
    "        training_job_name = None\n",
    "        mlflow_run_id = None\n",
    "        \n",
    "        if 'Tags' in response:\n",
    "            for tag in response['Tags']:\n",
    "                if tag['Key'] == 'TrainingJob':\n",
    "                    training_job_name = tag['Value']\n",
    "                elif tag['Key'] == 'MLFlowRunId':\n",
    "                    mlflow_run_id = tag['Value']\n",
    "        \n",
    "        # Also check metadata properties\n",
    "        if 'MetadataProperties' in response:\n",
    "            metadata = response['MetadataProperties']\n",
    "            if not training_job_name and 'GeneratedBy' in metadata:\n",
    "                # Extract from GeneratedBy field\n",
    "                generated_by = metadata['GeneratedBy']\n",
    "                if 'sagemaker-training-job-' in generated_by:\n",
    "                    training_job_name = generated_by.replace('sagemaker-training-job-', '')\n",
    "        \n",
    "        if not training_job_name:\n",
    "            print(f\"‚ö†Ô∏è  No training job found for model: {model_package_arn}\")\n",
    "            return None\n",
    "        \n",
    "        # Get training job details\n",
    "        training_response = sagemaker_client.describe_training_job(\n",
    "            TrainingJobName=training_job_name\n",
    "        )\n",
    "        \n",
    "        # Extract hyperparameters\n",
    "        hyperparameters = training_response.get('HyperParameters', {})\n",
    "        \n",
    "        # Get MLFlow metrics if available\n",
    "        mlflow_metrics = {}\n",
    "        if mlflow_run_id:\n",
    "            try:\n",
    "                run = mlflow.get_run(mlflow_run_id)\n",
    "                mlflow_metrics = run.data.metrics\n",
    "            except Exception as e:\n",
    "                print(f\"Could not retrieve MLFlow metrics: {e}\")\n",
    "        \n",
    "        # Get CloudWatch metrics\n",
    "        cloudwatch_metrics = get_final_training_metrics(training_job_name)\n",
    "        \n",
    "        # Combine all metrics with Model Registry information\n",
    "        performance_data = {\n",
    "            'model_package_arn': model_package_arn,\n",
    "            'training_job_name': training_job_name,\n",
    "            'mlflow_run_id': mlflow_run_id,\n",
    "            'creation_time': response['CreationTime'],\n",
    "            'approval_status': response['ModelApprovalStatus'],\n",
    "            'model_artifacts_s3_uri': artifact_info['model_data_url'] if artifact_info else None,\n",
    "            'registry_managed': True,  # Indicates artifacts are managed through Model Registry\n",
    "            'hyperparameters': hyperparameters,\n",
    "            'mlflow_metrics': mlflow_metrics,\n",
    "            'cloudwatch_metrics': cloudwatch_metrics,\n",
    "            'training_duration': training_response.get('TrainingTimeInSeconds', 0),\n",
    "            'billable_duration': training_response.get('BillableTimeInSeconds', 0)\n",
    "        }\n",
    "        \n",
    "        return performance_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting performance metrics for {model_package_arn}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_final_training_metrics(job_name):\n",
    "    \"\"\"Get final training metrics from CloudWatch\"\"\"\n",
    "    cloudwatch = session.client('cloudwatch')\n",
    "    \n",
    "    # Get job details for time range\n",
    "    try:\n",
    "        job_response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "        start_time = job_response['CreationTime']\n",
    "        end_time = job_response.get('TrainingEndTime', job_response['LastModifiedTime'])\n",
    "    except:\n",
    "        return {}\n",
    "    \n",
    "    metrics = ['train:loss', 'val:loss', 'val:mAP50', 'val:mAP50-95']\n",
    "    final_metrics = {}\n",
    "    \n",
    "    for metric_name in metrics:\n",
    "        try:\n",
    "            response = cloudwatch.get_metric_statistics(\n",
    "                Namespace='SageMaker',\n",
    "                MetricName=metric_name,\n",
    "                Dimensions=[{'Name': 'TrainingJobName', 'Value': job_name}],\n",
    "                StartTime=start_time,\n",
    "                EndTime=end_time,\n",
    "                Period=300,\n",
    "                Statistics=['Average']\n",
    "            )\n",
    "            \n",
    "            datapoints = response.get('Datapoints', [])\n",
    "            if datapoints:\n",
    "                # Get the last (most recent) value\n",
    "                datapoints.sort(key=lambda x: x['Timestamp'])\n",
    "                final_metrics[metric_name] = datapoints[-1]['Average']\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Could not retrieve {metric_name}: {e}\")\n",
    "    \n",
    "    return final_metrics\n",
    "\n",
    "print(\"‚úÖ Performance comparison framework loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2: Model Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_versions(model_package_group_name, max_models=10):\n",
    "    \"\"\"Compare multiple model versions with comprehensive analysis\"\"\"\n",
    "    \n",
    "    # Get all models in the group\n",
    "    models = list_models_in_registry(model_package_group_name)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Limit to recent models\n",
    "    models = models[:max_models]\n",
    "    \n",
    "    print(f\"Analyzing {len(models)} model versions...\")\n",
    "    \n",
    "    # Collect performance data for all models\n",
    "    performance_data = []\n",
    "    for model in models:\n",
    "        model_arn = model['ModelPackageArn']\n",
    "        perf_data = get_model_performance_metrics(model_arn)\n",
    "        if perf_data:\n",
    "            performance_data.append(perf_data)\n",
    "    \n",
    "    if not performance_data:\n",
    "        print(\"No performance data available for comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_rows = []\n",
    "    for data in performance_data:\n",
    "        row = {\n",
    "            'model_arn': data['model_package_arn'].split('/')[-1],  # Short version\n",
    "            'training_job': data['training_job_name'],\n",
    "            'creation_time': data['creation_time'],\n",
    "            'approval_status': data['approval_status'],\n",
    "            'training_duration_min': data['training_duration'] / 60,\n",
    "            'billable_duration_min': data['billable_duration'] / 60\n",
    "        }\n",
    "        \n",
    "        # Add hyperparameters\n",
    "        hyperparams = data['hyperparameters']\n",
    "        row.update({\n",
    "            'model_variant': hyperparams.get('model_variant', 'unknown'),\n",
    "            'batch_size': int(hyperparams.get('batch_size', 0)),\n",
    "            'epochs': int(hyperparams.get('epochs', 0)),\n",
    "            'learning_rate': float(hyperparams.get('learning_rate', 0))\n",
    "        })\n",
    "        \n",
    "        # Add performance metrics (prefer CloudWatch over MLFlow)\n",
    "        cw_metrics = data['cloudwatch_metrics']\n",
    "        mlflow_metrics = data['mlflow_metrics']\n",
    "        \n",
    "        row.update({\n",
    "            'final_train_loss': cw_metrics.get('train:loss', mlflow_metrics.get('train_loss')),\n",
    "            'final_val_loss': cw_metrics.get('val:loss', mlflow_metrics.get('val_loss')),\n",
    "            'final_map50': cw_metrics.get('val:mAP50', mlflow_metrics.get('val_mAP50')),\n",
    "            'final_map50_95': cw_metrics.get('val:mAP50-95', mlflow_metrics.get('val_mAP50_95'))\n",
    "        })\n",
    "        \n",
    "        comparison_rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_rows)\n",
    "    comparison_df = comparison_df.sort_values('creation_time', ascending=False)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Perform model comparison\n",
    "if models:\n",
    "    comparison_df = compare_model_versions(MODEL_PACKAGE_GROUP_NAME)\n",
    "    \n",
    "    if comparison_df is not None:\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        # Display key metrics\n",
    "        display_cols = [\n",
    "            'training_job', 'approval_status', 'model_variant', \n",
    "            'final_map50', 'final_map50_95', 'final_val_loss',\n",
    "            'training_duration_min', 'epochs', 'learning_rate'\n",
    "        ]\n",
    "        \n",
    "        available_cols = [col for col in display_cols if col in comparison_df.columns]\n",
    "        display_df = comparison_df[available_cols].copy()\n",
    "        \n",
    "        # Format numeric columns\n",
    "        numeric_cols = ['final_map50', 'final_map50_95', 'final_val_loss', 'training_duration_min', 'learning_rate']\n",
    "        for col in numeric_cols:\n",
    "            if col in display_df.columns:\n",
    "                display_df[col] = pd.to_numeric(display_df[col], errors='coerce')\n",
    "                display_df[col] = display_df[col].round(4)\n",
    "        \n",
    "        display(display_df)\n",
    "        \n",
    "        # Store for visualization\n",
    "        global model_comparison_df\n",
    "        model_comparison_df = comparison_df\n",
    "        \n",
    "    else:\n",
    "        print(\"No comparison data available\")\n",
    "else:\n",
    "    print(\"No models available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3: Performance Visualization and Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_performance(comparison_df):\n",
    "    \"\"\"Create comprehensive performance visualizations\"\"\"\n",
    "    \n",
    "    if comparison_df is None or comparison_df.empty:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Model Performance Comparison Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. mAP Performance Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    if 'final_map50' in comparison_df.columns and comparison_df['final_map50'].notna().any():\n",
    "        x_pos = range(len(comparison_df))\n",
    "        bars1 = ax1.bar([x - 0.2 for x in x_pos], comparison_df['final_map50'].fillna(0), \n",
    "                       width=0.4, label='mAP@0.5', alpha=0.8, color='skyblue')\n",
    "        \n",
    "        if 'final_map50_95' in comparison_df.columns:\n",
    "            bars2 = ax1.bar([x + 0.2 for x in x_pos], comparison_df['final_map50_95'].fillna(0), \n",
    "                           width=0.4, label='mAP@0.5:0.95', alpha=0.8, color='lightcoral')\n",
    "        \n",
    "        ax1.set_xlabel('Model Version')\n",
    "        ax1.set_ylabel('mAP Score')\n",
    "        ax1.set_title('Model Accuracy Comparison (mAP)')\n",
    "        ax1.set_xticks(x_pos)\n",
    "        ax1.set_xticklabels([f\"v{i+1}\" for i in range(len(comparison_df))], rotation=45)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar1, bar2) in enumerate(zip(bars1, bars2 if 'final_map50_95' in comparison_df.columns else bars1)):\n",
    "            height1 = bar1.get_height()\n",
    "            height2 = bar2.get_height() if 'final_map50_95' in comparison_df.columns else 0\n",
    "            if height1 > 0:\n",
    "                ax1.text(bar1.get_x() + bar1.get_width()/2., height1 + 0.01,\n",
    "                        f'{height1:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "            if height2 > 0:\n",
    "                ax1.text(bar2.get_x() + bar2.get_width()/2., height2 + 0.01,\n",
    "                        f'{height2:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No mAP data available', ha='center', va='center', transform=ax1.transAxes)\n",
    "        ax1.set_title('Model Accuracy Comparison (mAP) - No Data')\n",
    "    \n",
    "    # 2. Training Efficiency Analysis\n",
    "    ax2 = axes[0, 1]\n",
    "    if 'training_duration_min' in comparison_df.columns and 'final_map50' in comparison_df.columns:\n",
    "        valid_data = comparison_df.dropna(subset=['training_duration_min', 'final_map50'])\n",
    "        if not valid_data.empty:\n",
    "            scatter = ax2.scatter(valid_data['training_duration_min'], valid_data['final_map50'], \n",
    "                                 c=range(len(valid_data)), cmap='viridis', s=100, alpha=0.7)\n",
    "            \n",
    "            # Add trend line\n",
    "            if len(valid_data) > 1:\n",
    "                z = np.polyfit(valid_data['training_duration_min'], valid_data['final_map50'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                ax2.plot(valid_data['training_duration_min'], p(valid_data['training_duration_min']), \n",
    "                        \"r--\", alpha=0.8, linewidth=2)\n",
    "            \n",
    "            ax2.set_xlabel('Training Duration (minutes)')\n",
    "            ax2.set_ylabel('Final mAP@0.5')\n",
    "            ax2.set_title('Training Efficiency Analysis')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(scatter, ax=ax2)\n",
    "            cbar.set_label('Model Version (newer ‚Üí older)')\n",
    "            \n",
    "            # Annotate points\n",
    "            for i, row in valid_data.iterrows():\n",
    "                ax2.annotate(f'v{list(valid_data.index).index(i)+1}', \n",
    "                           (row['training_duration_min'], row['final_map50']),\n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, 'Insufficient data for efficiency analysis', \n",
    "                    ha='center', va='center', transform=ax2.transAxes)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No efficiency data available', \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title('Training Efficiency Analysis - No Data')\n",
    "    \n",
    "    # 3. Hyperparameter Impact Analysis\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'learning_rate' in comparison_df.columns and 'final_map50' in comparison_df.columns:\n",
    "        valid_data = comparison_df.dropna(subset=['learning_rate', 'final_map50'])\n",
    "        if not valid_data.empty and len(valid_data) > 1:\n",
    "            # Group by learning rate and show performance\n",
    "            lr_performance = valid_data.groupby('learning_rate')['final_map50'].agg(['mean', 'std', 'count'])\n",
    "            \n",
    "            bars = ax3.bar(range(len(lr_performance)), lr_performance['mean'], \n",
    "                          yerr=lr_performance['std'], capsize=5, alpha=0.7, color='lightgreen')\n",
    "            \n",
    "            ax3.set_xlabel('Learning Rate')\n",
    "            ax3.set_ylabel('Average mAP@0.5')\n",
    "            ax3.set_title('Learning Rate Impact on Performance')\n",
    "            ax3.set_xticks(range(len(lr_performance)))\n",
    "            ax3.set_xticklabels([f'{lr:.4f}' for lr in lr_performance.index], rotation=45)\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, (bar, mean_val, count) in enumerate(zip(bars, lr_performance['mean'], lr_performance['count'])):\n",
    "                ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                        f'{mean_val:.3f}\\\\n(n={count})', ha='center', va='bottom', fontsize=8)\n",
    "        else:\n",
    "            ax3.text(0.5, 0.5, 'Insufficient hyperparameter data', \n",
    "                    ha='center', va='center', transform=ax3.transAxes)\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No hyperparameter data available', \n",
    "                ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('Hyperparameter Impact Analysis - No Data')\n",
    "    \n",
    "    # 4. Model Approval Status Distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'approval_status' in comparison_df.columns:\n",
    "        status_counts = comparison_df['approval_status'].value_counts()\n",
    "        colors = {'Approved': 'lightgreen', 'PendingManualApproval': 'orange', \n",
    "                 'Rejected': 'lightcoral', 'InProgress': 'lightblue'}\n",
    "        \n",
    "        wedges, texts, autotexts = ax4.pie(status_counts.values, \n",
    "                                          labels=status_counts.index,\n",
    "                                          autopct='%1.1f%%',\n",
    "                                          colors=[colors.get(status, 'gray') for status in status_counts.index],\n",
    "                                          startangle=90)\n",
    "        \n",
    "        ax4.set_title('Model Approval Status Distribution')\n",
    "        \n",
    "        # Enhance text\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No approval status data', \n",
    "                ha='center', va='center', transform=ax4.transAxes)\n",
    "        ax4.set_title('Model Approval Status - No Data')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Generate performance visualizations\n",
    "if 'model_comparison_df' in locals() and model_comparison_df is not None:\n",
    "    performance_fig = visualize_model_performance(model_comparison_df)\n",
    "else:\n",
    "    print(\"No comparison data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4: Statistical Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(comparison_df):\n",
    "    \"\"\"Perform statistical analysis on model performance\"\"\"\n",
    "    \n",
    "    if comparison_df is None or comparison_df.empty:\n",
    "        print(\"No data available for statistical analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"STATISTICAL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    numeric_cols = ['final_map50', 'final_map50_95', 'final_val_loss', 'training_duration_min']\n",
    "    available_cols = [col for col in numeric_cols if col in comparison_df.columns]\n",
    "    \n",
    "    if available_cols:\n",
    "        stats_df = comparison_df[available_cols].describe()\n",
    "        print(\"\\nDescriptive Statistics:\")\n",
    "        print(\"-\" * 40)\n",
    "        display(stats_df.round(4))\n",
    "    \n",
    "    # Performance trends\n",
    "    if 'final_map50' in comparison_df.columns and len(comparison_df) > 2:\n",
    "        print(\"\\nPerformance Trend Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Sort by creation time for trend analysis\n",
    "        trend_df = comparison_df.sort_values('creation_time')\n",
    "        map50_values = trend_df['final_map50'].dropna()\n",
    "        \n",
    "        if len(map50_values) > 1:\n",
    "            # Calculate trend\n",
    "            x = np.arange(len(map50_values))\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, map50_values)\n",
    "            \n",
    "            print(f\"mAP@0.5 Trend:\")\n",
    "            print(f\"  Slope: {slope:.6f} (per model version)\")\n",
    "            print(f\"  R-squared: {r_value**2:.4f}\")\n",
    "            print(f\"  P-value: {p_value:.4f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                trend_direction = \"improving\" if slope > 0 else \"declining\"\n",
    "                print(f\"  Interpretation: Statistically significant {trend_direction} trend\")\n",
    "            else:\n",
    "                print(f\"  Interpretation: No statistically significant trend\")\n",
    "            \n",
    "            # Best and worst performing models\n",
    "            best_idx = map50_values.idxmax()\n",
    "            worst_idx = map50_values.idxmin()\n",
    "            \n",
    "            print(f\"\\nBest performing model:\")\n",
    "            print(f\"  Job: {trend_df.loc[best_idx, 'training_job']}\")\n",
    "            print(f\"  mAP@0.5: {map50_values[best_idx]:.4f}\")\n",
    "            \n",
    "            print(f\"\\nWorst performing model:\")\n",
    "            print(f\"  Job: {trend_df.loc[worst_idx, 'training_job']}\")\n",
    "            print(f\"  mAP@0.5: {map50_values[worst_idx]:.4f}\")\n",
    "            \n",
    "            # Performance improvement\n",
    "            improvement = map50_values[best_idx] - map50_values[worst_idx]\n",
    "            print(f\"\\nPerformance range: {improvement:.4f} mAP@0.5 points\")\n",
    "    \n",
    "    # Hyperparameter correlation analysis\n",
    "    if len(comparison_df) > 3:\n",
    "        print(\"\\nHyperparameter Correlation Analysis:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        correlation_cols = ['final_map50', 'learning_rate', 'batch_size', 'epochs', 'training_duration_min']\n",
    "        available_corr_cols = [col for col in correlation_cols if col in comparison_df.columns]\n",
    "        \n",
    "        if len(available_corr_cols) > 2:\n",
    "            corr_df = comparison_df[available_corr_cols].corr()\n",
    "            \n",
    "            # Focus on correlations with performance metrics\n",
    "            if 'final_map50' in corr_df.columns:\n",
    "                map50_corr = corr_df['final_map50'].drop('final_map50').sort_values(key=abs, ascending=False)\n",
    "                \n",
    "                print(\"Correlation with mAP@0.5:\")\n",
    "                for param, corr_val in map50_corr.items():\n",
    "                    if not pd.isna(corr_val):\n",
    "                        strength = \"strong\" if abs(corr_val) > 0.7 else \"moderate\" if abs(corr_val) > 0.4 else \"weak\"\n",
    "                        direction = \"positive\" if corr_val > 0 else \"negative\"\n",
    "                        print(f\"  {param}: {corr_val:.4f} ({strength} {direction})\")\n",
    "    \n",
    "    # Model recommendation\n",
    "    print(\"\\nMODEL RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'final_map50' in comparison_df.columns:\n",
    "        # Find best performing approved model\n",
    "        approved_models = comparison_df[comparison_df['approval_status'] == 'Approved']\n",
    "        \n",
    "        if not approved_models.empty:\n",
    "            best_approved = approved_models.loc[approved_models['final_map50'].idxmax()]\n",
    "            print(f\"‚úÖ Best approved model for deployment:\")\n",
    "            print(f\"   Job: {best_approved['training_job']}\")\n",
    "            print(f\"   mAP@0.5: {best_approved['final_map50']:.4f}\")\n",
    "        \n",
    "        # Find best pending model\n",
    "        pending_models = comparison_df[comparison_df['approval_status'] == 'PendingManualApproval']\n",
    "        \n",
    "        if not pending_models.empty:\n",
    "            best_pending = pending_models.loc[pending_models['final_map50'].idxmax()]\n",
    "            print(f\"‚è≥ Best pending model for approval:\")\n",
    "            print(f\"   Job: {best_pending['training_job']}\")\n",
    "            print(f\"   mAP@0.5: {best_pending['final_map50']:.4f}\")\n",
    "            \n",
    "            # Check if it's better than approved models\n",
    "            if not approved_models.empty:\n",
    "                improvement = best_pending['final_map50'] - approved_models['final_map50'].max()\n",
    "                if improvement > 0.01:  # 1% improvement threshold\n",
    "                    print(f\"   üí° Recommendation: Consider approving (improvement: +{improvement:.4f})\")\n",
    "\n",
    "# Perform statistical analysis\n",
    "if 'model_comparison_df' in locals() and model_comparison_df is not None:\n",
    "    perform_statistical_analysis(model_comparison_df)\n",
    "else:\n",
    "    print(\"No comparison data available for statistical analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "In this enhanced notebook, we've executed and monitored a YOLOv11 training pipeline with comprehensive MLFlow tracking, SageMaker Model Registry integration, and advanced performance analysis capabilities. Here's a summary of what we've accomplished:\n",
    "\n",
    "### Completed Tasks:\n",
    "\n",
    "1. **Model Registry Setup**:\n",
    "   - Created Model Package Group for organizing YOLOv11 models\n",
    "   - Configured model registration workflow\n",
    "\n",
    "2. **Pipeline Configuration with MLFlow**:\n",
    "   - Listed available datasets\n",
    "   - Configured training parameters with MLFlow experiment tracking\n",
    "\n",
    "3. **Enhanced Pipeline Execution**:\n",
    "   - Created and executed SageMaker training job with MLFlow integration\n",
    "   - Logged all parameters, metrics, and metadata to MLFlow\n",
    "\n",
    "4. **Comprehensive Monitoring**:\n",
    "   - Monitored training job status with real-time updates to MLFlow\n",
    "   - Tracked training duration and job status\n",
    "\n",
    "5. **Automated Model Validation** ‚≠ê NEW:\n",
    "   - Implemented automated validation before model registration\n",
    "   - Performance threshold checking and quality gates\n",
    "   - Automated model evaluation against baseline metrics\n",
    "\n",
    "6. **Model Registration**:\n",
    "   - Registered trained models in SageMaker Model Registry\n",
    "   - Configured approval workflows for production deployment\n",
    "   - Linked MLFlow runs with registered models\n",
    "\n",
    "7. **Model Management**:\n",
    "   - Listed and managed models in the registry\n",
    "   - Implemented model approval workflows\n",
    "   - Retrieved detailed model information\n",
    "\n",
    "8. **Automated Deployment for Approved Models** ‚≠ê NEW:\n",
    "   - Implemented automated deployment triggers for approved models\n",
    "   - SageMaker endpoint creation with auto-scaling configuration\n",
    "   - Blue/green deployment strategy for zero-downtime updates\n",
    "\n",
    "9. **Experiment Management**:\n",
    "   - Listed and compared MLFlow experiments\n",
    "   - Searched and analyzed training runs\n",
    "   - Compared model performance across runs\n",
    "\n",
    "10. **Training Metrics Visualization**:\n",
    "    - Retrieved training metrics from CloudWatch\n",
    "    - Logged final metrics to MLFlow\n",
    "    - Visualized training progress and model performance\n",
    "\n",
    "11. **Advanced Model Performance Comparison** ‚≠ê NEW:\n",
    "    - Comprehensive multi-dimensional model comparison\n",
    "    - Statistical analysis of performance trends\n",
    "    - Hyperparameter impact analysis\n",
    "    - Automated model recommendations\n",
    "\n",
    "### Key Enhanced Features:\n",
    "\n",
    "- **Automated Validation Pipeline**: Models are automatically validated against performance thresholds before registration\n",
    "- **Intelligent Deployment Automation**: Approved models are automatically deployed with proper infrastructure setup\n",
    "- **Advanced Performance Analytics**: Statistical analysis and visualization of model performance across versions\n",
    "- **Production-Ready Workflows**: Complete automation from training to deployment with proper governance\n",
    "- **Comprehensive Model Comparison**: Multi-dimensional analysis including accuracy, efficiency, and hyperparameter impact\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **A/B Testing Framework**: Implement automated A/B testing for model comparison in production\n",
    "2. **Model Monitoring**: Set up comprehensive data drift and model performance monitoring\n",
    "3. **Automated Retraining**: Implement automated retraining based on performance degradation\n",
    "4. **Cost Optimization**: Add cost analysis and optimization recommendations\n",
    "5. **Multi-Region Deployment**: Extend deployment automation to multiple regions\n",
    "\n",
    "### Production Readiness Enhancements:\n",
    "\n",
    "- **Validation Gates**: Automated quality gates prevent poor-performing models from reaching production\n",
    "- **Zero-Downtime Deployment**: Blue/green deployment strategy ensures continuous service availability\n",
    "- **Performance Tracking**: Comprehensive tracking and comparison of model versions over time\n",
    "- **Statistical Analysis**: Data-driven insights for model selection and hyperparameter optimization\n",
    "- **Automated Recommendations**: AI-driven recommendations for model approval and deployment decisions\n",
    "\n",
    "This enhanced workflow provides a complete, production-ready foundation for YOLOv11 model development and deployment with advanced analytics, automated validation, and intelligent deployment capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
