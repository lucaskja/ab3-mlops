{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engineer Core Workflow with MLFlow and Model Registry\n",
    "\n",
    "This notebook provides enhanced functionality for ML Engineers to execute and manage YOLOv11 training pipelines with MLFlow experiment tracking and SageMaker Model Registry integration.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Pipeline Configuration**: Set up YOLOv11 training pipeline parameters\n",
    "2. **Pipeline Execution**: Execute the training pipeline with MLFlow tracking\n",
    "3. **Pipeline Monitoring**: Monitor training progress and results\n",
    "4. **Model Registration**: Register trained models in SageMaker Model Registry\n",
    "5. **Model Management**: Manage model versions and approval workflows\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with appropriate permissions\n",
    "- AWS CLI configured with \"ab\" profile\n",
    "- SageMaker Studio access with ML Engineer role\n",
    "- Access to the drone imagery dataset in S3 bucket: `lucaskle-ab3-project-pv`\n",
    "- Labeled data in YOLOv11 format\n",
    "- SageMaker managed MLFlow tracking server\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet mlflow>=3.0.0 requests-auth-aws-sigv4>=0.7 boto3>=1.28.0 sagemaker>=2.190.0 pandas>=2.0.0 matplotlib>=3.7.0 numpy>=1.24.0 PyYAML>=6.0\n",
    "\n",
    "print(\"✅ Required packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "import mlflow\n",
    "import mlflow.sagemaker\n",
    "from sagemaker.model_registry import ModelPackage\n",
    "from sagemaker.model_registry.model_registry import ModelRegistry\n",
    "\n",
    "# Set up AWS session with \"ab\" profile\n",
    "session = boto3.Session(profile_name='ab')\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "sagemaker_client = session.client('sagemaker')\n",
    "region = session.region_name\n",
    "account_id = session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Set up MLFlow tracking with SageMaker managed server\n",
    "# Download the SageMaker MLFlow helper\n",
    "try:\n",
    "    s3_client.download_file(\n",
    "        'lucaskle-ab3-project-pv', \n",
    "        'mlflow-sagemaker/utils/sagemaker_mlflow_helper.py', \n",
    "        'sagemaker_mlflow_helper.py'\n",
    "    )\n",
    "    \n",
    "    # Import the helper\n",
    "    from sagemaker_mlflow_helper import get_sagemaker_mlflow_helper\n",
    "    \n",
    "    # Initialize SageMaker MLflow helper\n",
    "    mlflow_helper = get_sagemaker_mlflow_helper(aws_profile='ab')\n",
    "    \n",
    "    # Get server info\n",
    "    server_info = mlflow_helper.get_tracking_server_info()\n",
    "    mlflow_tracking_uri = server_info.get('url', 'https://t-2vktx6phiclp.us-east-1.experiments.sagemaker.aws')\n",
    "    \n",
    "    print(f\"✅ Connected to SageMaker managed MLflow server\")\n",
    "    print(f\"Server Status: {server_info.get('status', 'Unknown')}\")\n",
    "    print(f\"MLflow Version: {server_info.get('mlflow_version', 'Unknown')}\")\n",
    "    \n",
    "    # Create experiment using helper\n",
    "    experiment_name = \"yolov11-drone-detection\"\n",
    "    mlflow_helper.create_experiment(experiment_name)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not connect to SageMaker managed MLflow: {e}\")\n",
    "    print(\"Using basic MLflow setup as fallback\")\n",
    "    experiment_name = \"yolov11-drone-detection\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    mlflow_tracking_uri = \"file:///tmp/mlruns\"\n",
    "\n",
    "# Set up visualization\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Define bucket name and role\n",
    "BUCKET_NAME = 'lucaskle-ab3-project-pv'\n",
    "ROLE_ARN = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "# Model Registry configuration\n",
    "MODEL_PACKAGE_GROUP_NAME = \"yolov11-drone-detection-models\"\n",
    "\n",
    "print(f\"Data Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Role ARN: {ROLE_ARN}\")\n",
    "print(f\"MLFlow Experiment: {experiment_name}\")\n",
    "print(f\"MLFlow Tracking URI: {mlflow_tracking_uri}\")\n",
    "print(f\"Model Package Group: {MODEL_PACKAGE_GROUP_NAME}\")\n",
    "\n",
    "# Helper functions for MLflow logging (works with both managed and direct MLflow)\n",
    "def log_params(params_dict):\n",
    "    \"\"\"Log parameters using available MLflow method\"\"\"\n",
    "    if 'mlflow_helper' in locals() and mlflow_helper:\n",
    "        mlflow_helper.log_params(params_dict)\n",
    "    else:\n",
    "        mlflow.log_params(params_dict)\n",
    "\n",
    "def log_metrics(metrics_dict, step=None):\n",
    "    \"\"\"Log metrics using available MLflow method\"\"\"\n",
    "    if 'mlflow_helper' in locals() and mlflow_helper:\n",
    "        mlflow_helper.log_metrics(metrics_dict, step=step)\n",
    "    else:\n",
    "        for key, value in metrics_dict.items():\n",
    "            mlflow.log_metric(key, value, step=step)\n",
    "\n",
    "def log_artifact(local_path, artifact_path=None):\n",
    "    \"\"\"Log artifact using available MLflow method\"\"\"\n",
    "    if 'mlflow_helper' in locals() and mlflow_helper:\n",
    "        mlflow_helper.log_artifact(local_path, artifact_path)\n",
    "    else:\n",
    "        mlflow.log_artifact(local_path, artifact_path)\n",
    "\n",
    "def start_run(run_name=None, experiment_name=None, tags=None):\n",
    "    \"\"\"Start MLflow run using available method\"\"\"\n",
    "    if 'mlflow_helper' in locals() and mlflow_helper:\n",
    "        return mlflow_helper.start_run(run_name=run_name, experiment_name=experiment_name, tags=tags)\n",
    "    else:\n",
    "        return mlflow.start_run(run_name=run_name, tags=tags)\n",
    "\n",
    "print(\"✅ MLflow helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Model Registry\n",
    "\n",
    "First, let's create the Model Package Group in SageMaker Model Registry if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model package group\n",
    "def create_model_package_group(group_name, description=\"YOLOv11 drone detection models\"):\n",
    "    \"\"\"Create a model package group in SageMaker Model Registry\"\"\"\n",
    "    try:\n",
    "        # Check if group already exists\n",
    "        response = sagemaker_client.describe_model_package_group(\n",
    "            ModelPackageGroupName=group_name\n",
    "        )\n",
    "        print(f\"Model package group '{group_name}' already exists.\")\n",
    "        print(f\"Status: {response['ModelPackageGroupStatus']}\")\n",
    "        return response\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ValidationException':\n",
    "            # Group doesn't exist, create it\n",
    "            print(f\"Creating model package group: {group_name}\")\n",
    "            response = sagemaker_client.create_model_package_group(\n",
    "                ModelPackageGroupName=group_name,\n",
    "                ModelPackageGroupDescription=description\n",
    "            )\n",
    "            print(f\"Created model package group: {response['ModelPackageGroupArn']}\")\n",
    "            return response\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# Create model package group\n",
    "model_package_group = create_model_package_group(MODEL_PACKAGE_GROUP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Configuration\n",
    "\n",
    "Let's configure our YOLOv11 training pipeline parameters with MLFlow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list available datasets\n",
    "def list_datasets(bucket, prefix=\"datasets/\"):\n",
    "    \"\"\"List available datasets in S3\"\"\"\n",
    "    s3_client = session.client('s3')\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "        Delimiter='/'\n",
    "    )\n",
    "    \n",
    "    datasets = []\n",
    "    if 'CommonPrefixes' in response:\n",
    "        for obj in response['CommonPrefixes']:\n",
    "            dataset_prefix = obj['Prefix']\n",
    "            dataset_name = dataset_prefix.split('/')[-2]\n",
    "            datasets.append({\n",
    "                'name': dataset_name,\n",
    "                'prefix': dataset_prefix\n",
    "            })\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# List available datasets\n",
    "datasets = list_datasets(BUCKET_NAME)\n",
    "\n",
    "print(f\"Found {len(datasets)} datasets:\")\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f\"  {i+1}. {dataset['name']} - s3://{BUCKET_NAME}/{dataset['prefix']}\")\n",
    "\n",
    "# If no datasets found, provide instructions\n",
    "if not datasets:\n",
    "    print(\"\\nNo datasets found. Please prepare a dataset using the Data Scientist notebook first.\")\n",
    "    print(\"The dataset should be organized in the following structure:\")\n",
    "    print(\"s3://lucaskle-ab3-project-pv/datasets/your_dataset_name/\")\n",
    "    print(\"├── train/\")\n",
    "    print(\"│   ├── images/\")\n",
    "    print(\"│   └── labels/\")\n",
    "    print(\"└── val/\")\n",
    "    print(\"    ├── images/\")\n",
    "    print(\"    └── labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters with MLFlow tracking\n",
    "training_params = {\n",
    "    # Dataset parameters\n",
    "    'dataset_name': datasets[0]['name'] if datasets else 'your_dataset_name',\n",
    "    'dataset_prefix': datasets[0]['prefix'] if datasets else 'datasets/your_dataset_name/',\n",
    "    \n",
    "    # Model parameters\n",
    "    'model_variant': 'yolov11n',  # Options: yolov11n, yolov11s, yolov11m, yolov11l, yolov11x\n",
    "    'image_size': 640,  # Input image size (px)\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 16,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    \n",
    "    # Infrastructure parameters\n",
    "    'instance_type': 'ml.g4dn.xlarge',\n",
    "    'instance_count': 1,\n",
    "    'use_spot': True,\n",
    "    'max_wait': 36000,  # Max wait time for spot instances (seconds)\n",
    "    'max_run': 3600,    # Max run time (seconds)\n",
    "    \n",
    "    # Output parameters\n",
    "    'output_path': f\"s3://{BUCKET_NAME}/model-artifacts/\",\n",
    "    'job_name': f\"yolov11-training-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\",\n",
    "    \n",
    "    # MLFlow parameters\n",
    "    'experiment_name': experiment_name,\n",
    "    'run_name': f\"yolov11-run-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "}\n",
    "\n",
    "# Display training parameters\n",
    "print(\"YOLOv11 Training Parameters:\")\n",
    "for key, value in training_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Execution with MLFlow Tracking\n",
    "\n",
    "Now let's execute the YOLOv11 training pipeline with comprehensive MLFlow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and execute training job with MLFlow tracking\n",
    "def execute_training_job_with_mlflow(params):\n",
    "    \"\"\"Create and execute SageMaker training job for YOLOv11 with MLFlow tracking\"\"\"\n",
    "    \n",
    "    # Start MLFlow run\n",
    "    with mlflow.start_run(run_name=params['run_name']) as run:\n",
    "        # Log parameters to MLFlow\n",
    "        mlflow.log_param(\"model_variant\", params['model_variant'])\n",
    "        mlflow.log_param(\"image_size\", params['image_size'])\n",
    "        mlflow.log_param(\"batch_size\", params['batch_size'])\n",
    "        mlflow.log_param(\"epochs\", params['epochs'])\n",
    "        mlflow.log_param(\"learning_rate\", params['learning_rate'])\n",
    "        mlflow.log_param(\"instance_type\", params['instance_type'])\n",
    "        mlflow.log_param(\"instance_count\", params['instance_count'])\n",
    "        mlflow.log_param(\"use_spot\", params['use_spot'])\n",
    "        mlflow.log_param(\"dataset_name\", params['dataset_name'])\n",
    "        mlflow.log_param(\"dataset_prefix\", params['dataset_prefix'])\n",
    "        \n",
    "        # Define hyperparameters for SageMaker\n",
    "        hyperparameters = {\n",
    "            \"model_variant\": params['model_variant'],\n",
    "            \"image_size\": str(params['image_size']),\n",
    "            \"batch_size\": str(params['batch_size']),\n",
    "            \"epochs\": str(params['epochs']),\n",
    "            \"learning_rate\": str(params['learning_rate']),\n",
    "            \"mlflow_run_id\": run.info.run_id,\n",
    "            \"mlflow_experiment_id\": run.info.experiment_id\n",
    "        }\n",
    "        \n",
    "        # Define input data channels\n",
    "        input_data = {\n",
    "            'training': f\"s3://{BUCKET_NAME}/{params['dataset_prefix']}\"\n",
    "        }\n",
    "        \n",
    "        # Create SageMaker estimator\n",
    "        estimator = sagemaker.estimator.Estimator(\n",
    "            image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-training:latest\",\n",
    "            role=ROLE_ARN,\n",
    "            instance_count=params['instance_count'],\n",
    "            instance_type=params['instance_type'],\n",
    "            hyperparameters=hyperparameters,\n",
    "            output_path=params['output_path'],\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            use_spot_instances=params['use_spot'],\n",
    "            max_wait=params['max_wait'] if params['use_spot'] else None,\n",
    "            max_run=params['max_run']\n",
    "        )\n",
    "        \n",
    "        # Start training job\n",
    "        print(f\"Starting training job: {params['job_name']}\")\n",
    "        print(f\"MLFlow Run ID: {run.info.run_id}\")\n",
    "        \n",
    "        # Log training job details to MLFlow\n",
    "        mlflow.log_param(\"sagemaker_job_name\", params['job_name'])\n",
    "        mlflow.log_param(\"output_path\", params['output_path'])\n",
    "        \n",
    "        # Start the training job\n",
    "        estimator.fit(input_data, job_name=params['job_name'], wait=False)\n",
    "        \n",
    "        # Log additional metadata\n",
    "        mlflow.set_tag(\"sagemaker_job_name\", params['job_name'])\n",
    "        mlflow.set_tag(\"model_type\", \"YOLOv11\")\n",
    "        mlflow.set_tag(\"task_type\", \"object_detection\")\n",
    "        mlflow.set_tag(\"dataset\", params['dataset_name'])\n",
    "        \n",
    "        return params['job_name'], run.info.run_id\n",
    "\n",
    "# Execute training job with MLFlow tracking\n",
    "try:\n",
    "    job_name, mlflow_run_id = execute_training_job_with_mlflow(training_params)\n",
    "    print(f\"\\nTraining job started: {job_name}\")\n",
    "    print(f\"MLFlow Run ID: {mlflow_run_id}\")\n",
    "    print(f\"You can monitor the job in the SageMaker console or using the cell below.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error starting training job: {str(e)}\")\n",
    "    print(\"\\nPossible causes:\")\n",
    "    print(\"1. The dataset doesn't exist or has incorrect structure\")\n",
    "    print(\"2. The YOLOv11 training container doesn't exist in ECR\")\n",
    "    print(\"3. Insufficient permissions to start training job\")\n",
    "    print(\"\\nPlease check the error message and try again.\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Monitoring with Enhanced Metrics\n",
    "\n",
    "Let's monitor the progress of our training job and update MLFlow with metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor training job and update MLFlow\n",
    "def monitor_training_job_with_mlflow(job_name, mlflow_run_id):\n",
    "    \"\"\"Monitor SageMaker training job status and update MLFlow\"\"\"\n",
    "    # Get job description\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Extract job status\n",
    "    status = response['TrainingJobStatus']\n",
    "    creation_time = response['CreationTime']\n",
    "    last_modified_time = response.get('LastModifiedTime', creation_time)\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = last_modified_time - creation_time\n",
    "    duration_minutes = duration.total_seconds() / 60\n",
    "    \n",
    "    # Display job information\n",
    "    print(f\"Job Name: {job_name}\")\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Creation Time: {creation_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Last Modified: {last_modified_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Duration: {duration_minutes:.2f} minutes\")\n",
    "    print(f\"MLFlow Run ID: {mlflow_run_id}\")\n",
    "    \n",
    "    # Update MLFlow with job status\n",
    "    with mlflow.start_run(run_id=mlflow_run_id):\n",
    "        mlflow.log_metric(\"training_duration_minutes\", duration_minutes)\n",
    "        mlflow.set_tag(\"job_status\", status)\n",
    "        mlflow.set_tag(\"last_updated\", last_modified_time.isoformat())\n",
    "    \n",
    "    # Display additional information based on status\n",
    "    if status == 'InProgress':\n",
    "        print(\"\\nJob is still running. Check back later for results.\")\n",
    "    elif status == 'Completed':\n",
    "        print(\"\\nJob completed successfully!\")\n",
    "        model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "        print(f\"Model artifacts: {model_artifacts}\")\n",
    "        \n",
    "        # Update MLFlow with completion details\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.log_param(\"model_artifacts_path\", model_artifacts)\n",
    "            mlflow.set_tag(\"training_completed\", \"true\")\n",
    "            \n",
    "    elif status == 'Failed':\n",
    "        print(\"\\nJob failed!\")\n",
    "        failure_reason = response.get('FailureReason', 'Unknown')\n",
    "        print(f\"Failure reason: {failure_reason}\")\n",
    "        \n",
    "        # Update MLFlow with failure details\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.set_tag(\"failure_reason\", failure_reason)\n",
    "            mlflow.set_tag(\"training_failed\", \"true\")\n",
    "            \n",
    "    elif status == 'Stopped':\n",
    "        print(\"\\nJob was stopped.\")\n",
    "        \n",
    "        # Update MLFlow with stopped status\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.set_tag(\"training_stopped\", \"true\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Monitor the training job\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        job_response = monitor_training_job_with_mlflow(job_name, mlflow_run_id)\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error monitoring training job: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh job status (run this cell to update status)\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        job_response = monitor_training_job_with_mlflow(job_name, mlflow_run_id)\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error monitoring training job: {str(e)}\")"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Registration in SageMaker Model Registry\n",
    "\n",
    "Once the training job is complete, let's register the model in SageMaker Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to register model in Model Registry\n",
    "def register_model_in_registry(job_name, mlflow_run_id, model_package_group_name):\n",
    "    \"\"\"Register trained model in SageMaker Model Registry\"\"\"\n",
    "    \n",
    "    # Get training job details\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Check if job is completed\n",
    "    if response['TrainingJobStatus'] != 'Completed':\n",
    "        print(f\"Training job is not completed yet. Status: {response['TrainingJobStatus']}\")\n",
    "        return None\n",
    "    \n",
    "    # Get model artifacts\n",
    "    model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    # Create model package\n",
    "    model_package_name = f\"yolov11-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "    \n",
    "    # Define inference specification\n",
    "    inference_specification = {\n",
    "        'Containers': [\n",
    "            {\n",
    "                'Image': f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-inference:latest\",\n",
    "                'ModelDataUrl': model_artifacts,\n",
    "                'Environment': {\n",
    "                    'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "                    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code'\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'SupportedContentTypes': ['application/json', 'image/jpeg', 'image/png'],\n",
    "        'SupportedResponseMIMETypes': ['application/json']\n",
    "    }\n",
    "    \n",
    "    # Create model package\n",
    "    try:\n",
    "        create_response = sagemaker_client.create_model_package(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelPackageDescription=f\"YOLOv11 drone detection model trained from job {job_name}\",\n",
    "            InferenceSpecification=inference_specification,\n",
    "            ModelApprovalStatus='PendingManualApproval',\n",
    "            MetadataProperties={\n",
    "                'GeneratedBy': f'sagemaker-training-job-{job_name}',\n",
    "                'ProjectId': 'yolov11-drone-detection',\n",
    "                'Repository': 'mlops-sagemaker-demo'\n",
    "            },\n",
    "            Tags=[\n",
    "                {'Key': 'Project', 'Value': 'MLOps-SageMaker-Demo'},\n",
    "                {'Key': 'Model', 'Value': 'YOLOv11'},\n",
    "                {'Key': 'Task', 'Value': 'ObjectDetection'},\n",
    "                {'Key': 'TrainingJob', 'Value': job_name},\n",
    "                {'Key': 'MLFlowRunId', 'Value': mlflow_run_id}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        model_package_arn = create_response['ModelPackageArn']\n",
    "        print(f\"Model registered successfully!\")\n",
    "        print(f\"Model Package ARN: {model_package_arn}\")\n",
    "        \n",
    "        # Update MLFlow with model registration details\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            mlflow.log_param(\"model_package_arn\", model_package_arn)\n",
    "            mlflow.log_param(\"model_package_group\", model_package_group_name)\n",
    "            mlflow.set_tag(\"model_registered\", \"true\")\n",
    "            mlflow.set_tag(\"model_approval_status\", \"PendingManualApproval\")\n",
    "            \n",
    "            # Log model to MLFlow\n",
    "            mlflow.log_artifact(model_artifacts, \"model_artifacts\")\n",
    "        \n",
    "        return model_package_arn\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error registering model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Register the model\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        model_package_arn = register_model_in_registry(job_name, mlflow_run_id, MODEL_PACKAGE_GROUP_NAME)\n",
    "        if model_package_arn:\n",
    "            print(f\"\\nModel registration completed!\")\n",
    "            print(f\"You can view the model in the SageMaker Model Registry console.\")\n",
    "    else:\n",
    "        print(\"No completed training job to register.\")\n",
    "        print(\"Please complete a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model registration: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Management and Approval Workflow\n",
    "\n",
    "Let's manage the registered models and handle approval workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list models in the registry\n",
    "def list_models_in_registry(model_package_group_name):\n",
    "    \"\"\"List all models in the Model Registry\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.list_model_packages(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending'\n",
    "        )\n",
    "        \n",
    "        models = response.get('ModelPackageSummaryList', [])\n",
    "        \n",
    "        if not models:\n",
    "            print(f\"No models found in group: {model_package_group_name}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(models)} models in group: {model_package_group_name}\")\n",
    "        print(\"\\nModel List:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            print(f\"{i+1}. Model Package ARN: {model['ModelPackageArn']}\")\n",
    "            print(f\"   Status: {model['ModelPackageStatus']}\")\n",
    "            print(f\"   Approval Status: {model['ModelApprovalStatus']}\")\n",
    "            print(f\"   Creation Time: {model['CreationTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            if 'ModelPackageDescription' in model:\n",
    "                print(f\"   Description: {model['ModelPackageDescription']}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        return models\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing models: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# List models in registry\n",
    "models = list_models_in_registry(MODEL_PACKAGE_GROUP_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to approve a model\n",
    "def approve_model(model_package_arn, approval_description=\"Model approved for deployment\"):\n",
    "    \"\"\"Approve a model in the Model Registry\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.update_model_package(\n",
    "            ModelPackageArn=model_package_arn,\n",
    "            ModelApprovalStatus='Approved',\n",
    "            ApprovalDescription=approval_description\n",
    "        )\n",
    "        \n",
    "        print(f\"Model approved successfully!\")\n",
    "        print(f\"Model Package ARN: {model_package_arn}\")\n",
    "        print(f\"Approval Description: {approval_description}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error approving model: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Example: Approve the latest model (uncomment to use)\n",
    "# if models:\n",
    "#     latest_model_arn = models[0]['ModelPackageArn']\n",
    "#     approve_model(latest_model_arn, \"Model approved after validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get model details\n",
    "def get_model_details(model_package_arn):\n",
    "    \"\"\"Get detailed information about a model\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.describe_model_package(\n",
    "            ModelPackageName=model_package_arn\n",
    "        )\n",
    "        \n",
    "        print(f\"Model Package Details:\")\n",
    "        print(f\"ARN: {response['ModelPackageArn']}\")\n",
    "        print(f\"Status: {response['ModelPackageStatus']}\")\n",
    "        print(f\"Approval Status: {response['ModelApprovalStatus']}\")\n",
    "        print(f\"Creation Time: {response['CreationTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if 'ModelPackageDescription' in response:\n",
    "            print(f\"Description: {response['ModelPackageDescription']}\")\n",
    "        \n",
    "        if 'InferenceSpecification' in response:\n",
    "            containers = response['InferenceSpecification']['Containers']\n",
    "            print(f\"\\nInference Specification:\")\n",
    "            for i, container in enumerate(containers):\n",
    "                print(f\"  Container {i+1}:\")\n",
    "                print(f\"    Image: {container['Image']}\")\n",
    "                print(f\"    Model Data: {container['ModelDataUrl']}\")\n",
    "        \n",
    "        if 'Tags' in response:\n",
    "            print(f\"\\nTags:\")\n",
    "            for tag in response['Tags']:\n",
    "                print(f\"  {tag['Key']}: {tag['Value']}\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting model details: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example: Get details of the latest model (uncomment to use)\n",
    "# if models:\n",
    "#     latest_model_arn = models[0]['ModelPackageArn']\n",
    "#     model_details = get_model_details(latest_model_arn)"
   ]
  }
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MLFlow Experiment Management\n",
    "\n",
    "Let's explore and compare experiments in MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list MLFlow experiments\n",
    "def list_mlflow_experiments():\n",
    "    \"\"\"List all MLFlow experiments\"\"\"\n",
    "    try:\n",
    "        experiments = mlflow.search_experiments()\n",
    "        \n",
    "        print(f\"Found {len(experiments)} experiments:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for exp in experiments:\n",
    "            print(f\"Experiment ID: {exp.experiment_id}\")\n",
    "            print(f\"Name: {exp.name}\")\n",
    "            print(f\"Lifecycle Stage: {exp.lifecycle_stage}\")\n",
    "            if exp.tags:\n",
    "                print(f\"Tags: {exp.tags}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        return experiments\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing experiments: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# List experiments\n",
    "experiments = list_mlflow_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search MLFlow runs\n",
    "def search_mlflow_runs(experiment_name, max_results=10):\n",
    "    \"\"\"Search MLFlow runs in an experiment\"\"\"\n",
    "    try:\n",
    "        # Get experiment by name\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if not experiment:\n",
    "            print(f\"Experiment '{experiment_name}' not found\")\n",
    "            return []\n",
    "        \n",
    "        # Search runs\n",
    "        runs = mlflow.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            max_results=max_results,\n",
    "            order_by=[\"start_time DESC\"]\n",
    "        )\n",
    "        \n",
    "        if runs.empty:\n",
    "            print(f\"No runs found in experiment '{experiment_name}'\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(runs)} runs in experiment '{experiment_name}':\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Display run information\n",
    "        for idx, run in runs.iterrows():\n",
    "            print(f\"Run ID: {run['run_id']}\")\n",
    "            print(f\"Status: {run['status']}\")\n",
    "            print(f\"Start Time: {run['start_time']}\")\n",
    "            \n",
    "            # Display parameters\n",
    "            param_cols = [col for col in runs.columns if col.startswith('params.')]\n",
    "            if param_cols:\n",
    "                print(\"Parameters:\")\n",
    "                for param_col in param_cols:\n",
    "                    param_name = param_col.replace('params.', '')\n",
    "                    param_value = run[param_col]\n",
    "                    if pd.notna(param_value):\n",
    "                        print(f\"  {param_name}: {param_value}\")\n",
    "            \n",
    "            # Display metrics\n",
    "            metric_cols = [col for col in runs.columns if col.startswith('metrics.')]\n",
    "            if metric_cols:\n",
    "                print(\"Metrics:\")\n",
    "                for metric_col in metric_cols:\n",
    "                    metric_name = metric_col.replace('metrics.', '')\n",
    "                    metric_value = run[metric_col]\n",
    "                    if pd.notna(metric_value):\n",
    "                        print(f\"  {metric_name}: {metric_value}\")\n",
    "            \n",
    "            # Display tags\n",
    "            tag_cols = [col for col in runs.columns if col.startswith('tags.')]\n",
    "            if tag_cols:\n",
    "                print(\"Tags:\")\n",
    "                for tag_col in tag_cols:\n",
    "                    tag_name = tag_col.replace('tags.', '')\n",
    "                    tag_value = run[tag_col]\n",
    "                    if pd.notna(tag_value):\n",
    "                        print(f\"  {tag_name}: {tag_value}\")\n",
    "            \n",
    "            print(\"-\" * 100)\n",
    "        \n",
    "        return runs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error searching runs: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Search runs in the current experiment\n",
    "runs_df = search_mlflow_runs(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compare runs\n",
    "def compare_runs(runs_df, metrics_to_compare=['training_duration_minutes']):\n",
    "    \"\"\"Compare MLFlow runs\"\"\"\n",
    "    if runs_df.empty:\n",
    "        print(\"No runs to compare\")\n",
    "        return\n",
    "    \n",
    "    print(\"Run Comparison:\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    # Select relevant columns for comparison\n",
    "    comparison_cols = ['run_id', 'status', 'start_time']\n",
    "    \n",
    "    # Add parameter columns\n",
    "    param_cols = [col for col in runs_df.columns if col.startswith('params.')]\n",
    "    comparison_cols.extend(param_cols)\n",
    "    \n",
    "    # Add metric columns\n",
    "    for metric in metrics_to_compare:\n",
    "        metric_col = f'metrics.{metric}'\n",
    "        if metric_col in runs_df.columns:\n",
    "            comparison_cols.append(metric_col)\n",
    "    \n",
    "    # Display comparison table\n",
    "    comparison_df = runs_df[comparison_cols].copy()\n",
    "    \n",
    "    # Rename columns for better display\n",
    "    column_mapping = {}\n",
    "    for col in comparison_df.columns:\n",
    "        if col.startswith('params.'):\n",
    "            column_mapping[col] = col.replace('params.', 'param_')\n",
    "        elif col.startswith('metrics.'):\n",
    "            column_mapping[col] = col.replace('metrics.', 'metric_')\n",
    "    \n",
    "    comparison_df = comparison_df.rename(columns=column_mapping)\n",
    "    \n",
    "    # Display the comparison\n",
    "    display(comparison_df)\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare runs if available\n",
    "if not runs_df.empty:\n",
    "    comparison_df = compare_runs(runs_df)\n",
    "else:\n",
    "    print(\"No runs available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Metrics Visualization\n",
    "\n",
    "Let's visualize training metrics from completed jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get training metrics from CloudWatch\n",
    "def get_training_metrics_with_mlflow(job_name, mlflow_run_id):\n",
    "    \"\"\"Get training metrics from CloudWatch and log to MLFlow\"\"\"\n",
    "    # Get job description\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Check if job is complete\n",
    "    if response['TrainingJobStatus'] != 'Completed':\n",
    "        print(f\"Job is not yet complete. Current status: {response['TrainingJobStatus']}\")\n",
    "        return None\n",
    "    \n",
    "    # Get CloudWatch metrics\n",
    "    cloudwatch = session.client('cloudwatch')\n",
    "    \n",
    "    # Define metrics to retrieve\n",
    "    metrics = [\n",
    "        'train:loss',\n",
    "        'val:loss',\n",
    "        'val:mAP50',\n",
    "        'val:mAP50-95'\n",
    "    ]\n",
    "    \n",
    "    # Get metrics data\n",
    "    metrics_data = {}\n",
    "    final_metrics = {}\n",
    "    \n",
    "    for metric_name in metrics:\n",
    "        try:\n",
    "            cw_response = cloudwatch.get_metric_statistics(\n",
    "                Namespace='SageMaker',\n",
    "                MetricName=metric_name,\n",
    "                Dimensions=[\n",
    "                    {\n",
    "                        'Name': 'TrainingJobName',\n",
    "                        'Value': job_name\n",
    "                    }\n",
    "                ],\n",
    "                StartTime=response['CreationTime'],\n",
    "                EndTime=response['LastModifiedTime'],\n",
    "                Period=60,  # 1-minute periods\n",
    "                Statistics=['Average']\n",
    "            )\n",
    "            \n",
    "            # Extract datapoints\n",
    "            datapoints = cw_response.get('Datapoints', [])\n",
    "            if datapoints:\n",
    "                # Sort by timestamp\n",
    "                datapoints.sort(key=lambda x: x['Timestamp'])\n",
    "                \n",
    "                # Extract values\n",
    "                timestamps = [dp['Timestamp'] for dp in datapoints]\n",
    "                values = [dp['Average'] for dp in datapoints]\n",
    "                \n",
    "                metrics_data[metric_name] = {\n",
    "                    'timestamps': timestamps,\n",
    "                    'values': values\n",
    "                }\n",
    "                \n",
    "                # Store final metric value\n",
    "                if values:\n",
    "                    final_metrics[metric_name] = values[-1]\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving metric {metric_name}: {str(e)}\")\n",
    "    \n",
    "    # Log final metrics to MLFlow\n",
    "    if final_metrics:\n",
    "        with mlflow.start_run(run_id=mlflow_run_id):\n",
    "            for metric_name, value in final_metrics.items():\n",
    "                # Clean metric name for MLFlow\n",
    "                clean_name = metric_name.replace(':', '_')\n",
    "                mlflow.log_metric(clean_name, value)\n",
    "    \n",
    "    return metrics_data, final_metrics\n",
    "\n",
    "# Get and visualize training metrics\n",
    "try:\n",
    "    if 'job_name' in locals() and 'mlflow_run_id' in locals():\n",
    "        metrics_data, final_metrics = get_training_metrics_with_mlflow(job_name, mlflow_run_id)\n",
    "        \n",
    "        if metrics_data:\n",
    "            # Plot metrics\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "            \n",
    "            # Plot loss\n",
    "            if 'train:loss' in metrics_data:\n",
    "                axes[0].plot(\n",
    "                    metrics_data['train:loss']['timestamps'],\n",
    "                    metrics_data['train:loss']['values'],\n",
    "                    label='Train Loss',\n",
    "                    marker='o'\n",
    "                )\n",
    "            \n",
    "            if 'val:loss' in metrics_data:\n",
    "                axes[0].plot(\n",
    "                    metrics_data['val:loss']['timestamps'],\n",
    "                    metrics_data['val:loss']['values'],\n",
    "                    label='Validation Loss',\n",
    "                    marker='s'\n",
    "                )\n",
    "            \n",
    "            axes[0].set_title('Training and Validation Loss')\n",
    "            axes[0].set_xlabel('Time')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot mAP\n",
    "            if 'val:mAP50' in metrics_data:\n",
    "                axes[1].plot(\n",
    "                    metrics_data['val:mAP50']['timestamps'],\n",
    "                    metrics_data['val:mAP50']['values'],\n",
    "                    label='mAP@0.5',\n",
    "                    marker='o'\n",
    "                )\n",
    "            \n",
    "            if 'val:mAP50-95' in metrics_data:\n",
    "                axes[1].plot(\n",
    "                    metrics_data['val:mAP50-95']['timestamps'],\n",
    "                    metrics_data['val:mAP50-95']['values'],\n",
    "                    label='mAP@0.5:0.95',\n",
    "                    marker='s'\n",
    "                )\n",
    "            \n",
    "            axes[1].set_title('Validation mAP')\n",
    "            axes[1].set_xlabel('Time')\n",
    "            axes[1].set_ylabel('mAP')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display final metrics\n",
    "            if final_metrics:\n",
    "                print(\"\\nFinal Training Metrics:\")\n",
    "                print(\"=\" * 40)\n",
    "                for metric_name, value in final_metrics.items():\n",
    "                    print(f\"{metric_name}: {value:.4f}\")\n",
    "        else:\n",
    "            print(\"No metrics available yet. Job may still be running or has failed.\")\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving training metrics: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "In this enhanced notebook, we've executed and monitored a YOLOv11 training pipeline with comprehensive MLFlow tracking and SageMaker Model Registry integration. Here's a summary of what we've accomplished:\n",
    "\n",
    "### Completed Tasks:\n",
    "\n",
    "1. **Model Registry Setup**:\n",
    "   - Created Model Package Group for organizing YOLOv11 models\n",
    "   - Configured model registration workflow\n",
    "\n",
    "2. **Pipeline Configuration with MLFlow**:\n",
    "   - Listed available datasets\n",
    "   - Configured training parameters with MLFlow experiment tracking\n",
    "\n",
    "3. **Enhanced Pipeline Execution**:\n",
    "   - Created and executed SageMaker training job with MLFlow integration\n",
    "   - Logged all parameters, metrics, and metadata to MLFlow\n",
    "\n",
    "4. **Comprehensive Monitoring**:\n",
    "   - Monitored training job status with real-time updates to MLFlow\n",
    "   - Tracked training duration and job status\n",
    "\n",
    "5. **Model Registration**:\n",
    "   - Registered trained models in SageMaker Model Registry\n",
    "   - Configured approval workflows for production deployment\n",
    "   - Linked MLFlow runs with registered models\n",
    "\n",
    "6. **Model Management**:\n",
    "   - Listed and managed models in the registry\n",
    "   - Implemented model approval workflows\n",
    "   - Retrieved detailed model information\n",
    "\n",
    "7. **Experiment Management**:\n",
    "   - Listed and compared MLFlow experiments\n",
    "   - Searched and analyzed training runs\n",
    "   - Compared model performance across runs\n",
    "\n",
    "8. **Metrics Visualization**:\n",
    "   - Retrieved training metrics from CloudWatch\n",
    "   - Logged final metrics to MLFlow\n",
    "   - Visualized training progress and model performance\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Complete MLFlow Integration**: All training parameters, metrics, and artifacts are tracked\n",
    "- **Model Registry Integration**: Trained models are automatically registered with proper metadata\n",
    "- **Approval Workflows**: Models require manual approval before deployment\n",
    "- **Experiment Comparison**: Easy comparison of different training runs\n",
    "- **Comprehensive Monitoring**: Real-time tracking of training progress\n",
    "- **Visualization**: Training metrics are visualized for better understanding\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Model Deployment**: Deploy approved models to SageMaker endpoints\n",
    "2. **A/B Testing**: Set up A/B testing for model comparison\n",
    "3. **Model Monitoring**: Implement data drift and model performance monitoring\n",
    "4. **Automated Retraining**: Set up automated retraining based on performance degradation\n",
    "5. **CI/CD Integration**: Integrate with CI/CD pipelines for automated model deployment\n",
    "\n",
    "### Best Practices Implemented:\n",
    "\n",
    "- **Experiment Tracking**: All experiments are tracked with MLFlow\n",
    "- **Model Versioning**: Models are versioned in the Model Registry\n",
    "- **Approval Workflows**: Manual approval required for production deployment\n",
    "- **Metadata Management**: Comprehensive metadata tracking for reproducibility\n",
    "- **Cost Optimization**: Use of spot instances for training\n",
    "- **Error Handling**: Robust error handling throughout the workflow\n",
    "\n",
    "This enhanced workflow provides a production-ready foundation for YOLOv11 model development and deployment with proper governance, tracking, and management capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
