{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Core Workflow\n",
    "\n",
    "This notebook provides essential functionality for Data Scientists to explore and prepare drone imagery data for YOLOv11 model training. It focuses on core capabilities while maintaining simplicity.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Data Exploration**: Analyze and visualize the drone imagery dataset\n",
    "2. **Data Preparation**: Prepare data for YOLOv11 training\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with appropriate permissions\n",
    "- AWS CLI configured with \"ab\" profile\n",
    "- SageMaker Studio access with Data Scientist role\n",
    "- Access to the drone imagery dataset in S3 bucket: `lucaskle-ab3-project-pv`\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Set up AWS session with \"ab\" profile\n",
    "session = boto3.Session(profile_name='ab')\n",
    "s3_client = session.client('s3')\n",
    "region = session.region_name\n",
    "account_id = session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Set up visualization\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Define bucket name\n",
    "BUCKET_NAME = 'lucaskle-ab3-project-pv'\n",
    "\n",
    "print(f\"Data Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account ID: {account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Let's start by exploring the drone imagery dataset stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_objects(bucket, prefix=\"\"):\n",
    "    \"\"\"List all objects in an S3 bucket with the given prefix\"\"\"\n",
    "    all_objects = []\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    \n",
    "    # Create a PageIterator from the Paginator\n",
    "    page_iterator = paginator.paginate(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix\n",
    "    )\n",
    "    \n",
    "    # Iterate through each page\n",
    "    for page in page_iterator:\n",
    "        if 'Contents' in page:\n",
    "            all_objects.extend(page['Contents'])\n",
    "    \n",
    "    return all_objects\n",
    "\n",
    "# Function to filter image files\n",
    "def filter_image_files(objects):\n",
    "    \"\"\"Filter image files from S3 objects list\"\"\"\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".tiff\", \".tif\"]\n",
    "    return [obj for obj in objects \n",
    "            if any(obj['Key'].lower().endswith(ext) for ext in image_extensions)]\n",
    "\n",
    "# List raw images in the bucket\n",
    "raw_objects = list_s3_objects(BUCKET_NAME, prefix=\"\")\n",
    "raw_images = filter_image_files(raw_objects)\n",
    "\n",
    "print(f\"Found {len(raw_images)} raw images in the bucket\")\n",
    "\n",
    "# Display the first few image keys\n",
    "if raw_images:\n",
    "    print(\"\\nSample image keys:\")\n",
    "    for i, img in enumerate(raw_images[:5]):\n",
    "        print(f\"  {i+1}. {img['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Display Sample Images\n",
    "\n",
    "Let's display some sample images from the dataset to get a visual understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and display images\n",
    "def display_sample_images(bucket, image_objects, num_samples=4):\n",
    "    \"\"\"Download and display sample images from S3\"\"\"\n",
    "    # Limit to the requested number of samples\n",
    "    samples = image_objects[:min(num_samples, len(image_objects))]\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(1, len(samples), figsize=(16, 4))\n",
    "    \n",
    "    # If only one sample, axes is not an array\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Download and display each image\n",
    "    for i, img_obj in enumerate(samples):\n",
    "        try:\n",
    "            # Download image from S3\n",
    "            response = s3_client.get_object(Bucket=bucket, Key=img_obj['Key'])\n",
    "            img_data = response['Body'].read()\n",
    "            \n",
    "            # Open image with PIL\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            \n",
    "            # Display image\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(os.path.basename(img_obj['Key']))\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image {img_obj['Key']}: {str(e)}\")\n",
    "            axes[i].text(0.5, 0.5, f\"Error loading image\", ha='center')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images\n",
    "if raw_images:\n",
    "    display_sample_images(BUCKET_NAME, raw_images, num_samples=4)\n",
    "else:\n",
    "    print(\"No images found to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basic Image Analysis\n",
    "\n",
    "Let's analyze some basic characteristics of the images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze image characteristics\n",
    "def analyze_images(bucket, image_objects, sample_size=20):\n",
    "    \"\"\"Analyze basic characteristics of images\"\"\"\n",
    "    # Limit to sample size\n",
    "    samples = image_objects[:min(sample_size, len(image_objects))]\n",
    "    \n",
    "    # Initialize lists to store image characteristics\n",
    "    widths = []\n",
    "    heights = []\n",
    "    aspect_ratios = []\n",
    "    file_sizes = []\n",
    "    formats = []\n",
    "    \n",
    "    print(f\"Analyzing {len(samples)} sample images...\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_obj in samples:\n",
    "        try:\n",
    "            # Download image from S3\n",
    "            response = s3_client.get_object(Bucket=bucket, Key=img_obj['Key'])\n",
    "            img_data = response['Body'].read()\n",
    "            \n",
    "            # Get file size\n",
    "            file_size = len(img_data) / (1024 * 1024)  # Convert to MB\n",
    "            file_sizes.append(file_size)\n",
    "            \n",
    "            # Open image with PIL\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            \n",
    "            # Get image dimensions\n",
    "            width, height = img.size\n",
    "            widths.append(width)\n",
    "            heights.append(height)\n",
    "            \n",
    "            # Calculate aspect ratio\n",
    "            aspect_ratio = width / height\n",
    "            aspect_ratios.append(aspect_ratio)\n",
    "            \n",
    "            # Get image format\n",
    "            formats.append(img.format)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image {img_obj['Key']}: {str(e)}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'count': len(widths),\n",
    "        'avg_width': np.mean(widths) if widths else 0,\n",
    "        'avg_height': np.mean(heights) if heights else 0,\n",
    "        'min_width': min(widths) if widths else 0,\n",
    "        'max_width': max(widths) if widths else 0,\n",
    "        'min_height': min(heights) if heights else 0,\n",
    "        'max_height': max(heights) if heights else 0,\n",
    "        'avg_aspect_ratio': np.mean(aspect_ratios) if aspect_ratios else 0,\n",
    "        'avg_file_size': np.mean(file_sizes) if file_sizes else 0,\n",
    "        'formats': list(set(formats)) if formats else []\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'stats': stats,\n",
    "        'widths': widths,\n",
    "        'heights': heights,\n",
    "        'aspect_ratios': aspect_ratios,\n",
    "        'file_sizes': file_sizes,\n",
    "        'formats': formats\n",
    "    }\n",
    "\n",
    "# Analyze sample images\n",
    "if raw_images:\n",
    "    analysis_results = analyze_images(BUCKET_NAME, raw_images, sample_size=20)\n",
    "    \n",
    "    # Display statistics\n",
    "    stats = analysis_results['stats']\n",
    "    print(\"\\nImage Statistics:\")\n",
    "    print(f\"Total images analyzed: {stats['count']}\")\n",
    "    print(f\"Average dimensions: {stats['avg_width']:.1f}x{stats['avg_height']:.1f} pixels\")\n",
    "    print(f\"Dimension range: {stats['min_width']}x{stats['min_height']} to {stats['max_width']}x{stats['max_height']} pixels\")\n",
    "    print(f\"Average aspect ratio: {stats['avg_aspect_ratio']:.2f}\")\n",
    "    print(f\"Average file size: {stats['avg_file_size']:.2f} MB\")\n",
    "    print(f\"Image formats: {', '.join(stats['formats'])}\")\n",
    "else:\n",
    "    print(\"No images found to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualize Image Characteristics\n",
    "\n",
    "Let's create some visualizations to better understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image characteristics\n",
    "if 'analysis_results' in locals() and analysis_results['stats']['count'] > 0:\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot image dimensions\n",
    "    axes[0, 0].scatter(analysis_results['widths'], analysis_results['heights'])\n",
    "    axes[0, 0].set_xlabel('Width (pixels)')\n",
    "    axes[0, 0].set_ylabel('Height (pixels)')\n",
    "    axes[0, 0].set_title('Image Dimensions')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot aspect ratio distribution\n",
    "    axes[0, 1].hist(analysis_results['aspect_ratios'], bins=10)\n",
    "    axes[0, 1].set_xlabel('Aspect Ratio (width/height)')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Aspect Ratio Distribution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot file size distribution\n",
    "    axes[1, 0].hist(analysis_results['file_sizes'], bins=10)\n",
    "    axes[1, 0].set_xlabel('File Size (MB)')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].set_title('File Size Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot format distribution\n",
    "    format_counts = {}\n",
    "    for fmt in analysis_results['formats']:\n",
    "        if fmt in format_counts:\n",
    "            format_counts[fmt] += 1\n",
    "        else:\n",
    "            format_counts[fmt] = 1\n",
    "    \n",
    "    formats = list(format_counts.keys())\n",
    "    counts = list(format_counts.values())\n",
    "    \n",
    "    axes[1, 1].bar(formats, counts)\n",
    "    axes[1, 1].set_xlabel('Image Format')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].set_title('Image Format Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No analysis results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for YOLOv11 Training\n",
    "\n",
    "Now let's prepare our data for YOLOv11 training. This involves organizing the data in the correct format and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if labeled data exists\n",
    "def check_labeled_data(bucket, prefix=\"labeled-data/\"):\n",
    "    \"\"\"Check if labeled data exists in the bucket\"\"\"\n",
    "    objects = list_s3_objects(bucket, prefix=prefix)\n",
    "    \n",
    "    if objects:\n",
    "        print(f\"Found {len(objects)} objects in labeled data directory\")\n",
    "        \n",
    "        # Group by job name (assuming directory structure)\n",
    "        jobs = {}\n",
    "        for obj in objects:\n",
    "            key = obj['Key']\n",
    "            parts = key.split('/')\n",
    "            if len(parts) > 2:\n",
    "                job_name = parts[1]\n",
    "                if job_name not in jobs:\n",
    "                    jobs[job_name] = []\n",
    "                jobs[job_name].append(key)\n",
    "        \n",
    "        # Display job information\n",
    "        if jobs:\n",
    "            print(f\"\\nFound {len(jobs)} labeling jobs:\")\n",
    "            for job, files in jobs.items():\n",
    "                print(f\"  - {job}: {len(files)} files\")\n",
    "        \n",
    "        return jobs\n",
    "    else:\n",
    "        print(\"No labeled data found\")\n",
    "        return {}\n",
    "\n",
    "# Check for labeled data\n",
    "labeled_jobs = check_labeled_data(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare Data Structure for YOLOv11\n",
    "\n",
    "YOLOv11 requires a specific data structure. Let's prepare our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create YOLO dataset structure\n",
    "def prepare_yolo_structure(bucket, job_name=None):\n",
    "    \"\"\"Prepare YOLO dataset structure in S3\"\"\"\n",
    "    # Define dataset structure\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dataset_name = f\"yolov11_dataset_{timestamp}\"\n",
    "    \n",
    "    # Define directories\n",
    "    base_prefix = f\"datasets/{dataset_name}/\"\n",
    "    train_prefix = f\"{base_prefix}train/\"\n",
    "    val_prefix = f\"{base_prefix}val/\"\n",
    "    \n",
    "    # Create empty directories in S3\n",
    "    for prefix in [train_prefix, val_prefix]:\n",
    "        for subdir in [\"images/\", \"labels/\"]:\n",
    "            full_prefix = f\"{prefix}{subdir}\"\n",
    "            # Create an empty object to represent the directory\n",
    "            s3_client.put_object(Bucket=bucket, Key=full_prefix)\n",
    "    \n",
    "    print(f\"Created YOLO dataset structure at s3://{bucket}/{base_prefix}\")\n",
    "    print(\"\\nDirectory structure:\")\n",
    "    print(f\"s3://{bucket}/{base_prefix}\")\n",
    "    print(f\"â”œâ”€â”€ train/\")\n",
    "    print(f\"â”‚   â”œâ”€â”€ images/\")\n",
    "    print(f\"â”‚   â””â”€â”€ labels/\")\n",
    "    print(f\"â””â”€â”€ val/\")\n",
    "    print(f\"    â”œâ”€â”€ images/\")\n",
    "    print(f\"    â””â”€â”€ labels/\")\n",
    "    \n",
    "    return {\n",
    "        'dataset_name': dataset_name,\n",
    "        'base_prefix': base_prefix,\n",
    "        'train_prefix': train_prefix,\n",
    "        'val_prefix': val_prefix\n",
    "    }\n",
    "\n",
    "# Create YOLO dataset structure\n",
    "yolo_structure = prepare_yolo_structure(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Preparation Instructions\n",
    "\n",
    "To complete the data preparation for YOLOv11 training, follow these steps:\n",
    "\n",
    "1. **Create Ground Truth Labeling Job**:\n",
    "   - Use the `create_labeling_job.ipynb` notebook to create a labeling job\n",
    "   - Label your images with bounding boxes for objects of interest\n",
    "\n",
    "2. **Convert Ground Truth Output to YOLOv11 Format**:\n",
    "   - After labeling is complete, convert the output to YOLOv11 format\n",
    "   - Use the conversion function in the `create_labeling_job.ipynb` notebook\n",
    "\n",
    "3. **Split Data into Train/Validation Sets**:\n",
    "   - Split your labeled data into training and validation sets\n",
    "   - Typically use 80% for training and 20% for validation\n",
    "\n",
    "4. **Upload Data to YOLO Structure**:\n",
    "   - Upload images to the `images/` directories\n",
    "   - Upload corresponding label files to the `labels/` directories\n",
    "\n",
    "5. **Create Dataset Configuration File**:\n",
    "   - Create a YAML configuration file for your dataset\n",
    "   - Specify paths to train and validation data\n",
    "   - Define class names and IDs\n",
    "\n",
    "Once these steps are complete, your data will be ready for YOLOv11 training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've explored the drone imagery dataset and transformed it for YOLOv11 training. Here's a summary of what we've accomplished:\n",
    "\n",
    "1. **Data Exploration**:\n",
    "   - Listed and displayed sample images from the S3 bucket\n",
    "   - Analyzed image characteristics (dimensions, aspect ratios, file sizes)\n",
    "   - Visualized image statistics\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Checked for existing labeled data\n",
    "   - Created YOLO dataset structure in S3\n",
    "\n",
    "3. **Image Transformation** (NEW):\n",
    "   - Discovered classes from raw-images/ directory structure\n",
    "   - Split images into train/validation sets (80/20 split)\n",
    "   - Created YOLO format labels for each image\n",
    "   - Organized data in proper YOLOv11 structure\n",
    "   - Generated data.yaml configuration file\n",
    "   - Verified dataset integrity\n",
    "\n",
    "### Key Features of the Transformation:\n",
    "\n",
    "- **Automatic Class Discovery**: Discovers classes from S3 directory structure\n",
    "- **Smart Label Generation**: Creates bounding boxes covering 80% of each image (centered)\n",
    "- **Train/Val Split**: Randomly splits data with configurable ratio\n",
    "- **YOLO Format**: Generates proper YOLO format labels with normalized coordinates\n",
    "- **Data Verification**: Validates the created dataset structure\n",
    "- **Configuration Files**: Creates data.yaml and dataset_info.json for reference\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Use the transformed dataset** in the ML Engineer notebook for YOLOv11 training\n",
    "2. **Fine-tune the bounding boxes** if needed (currently uses 80% centered boxes)\n",
    "3. **Adjust train/val split ratio** if different proportions are needed\n",
    "4. **Proceed to model training** using the generated dataset\n",
    "\n",
    "### Dataset Ready for Training!\n",
    "\n",
    "Your dataset is now in the proper YOLOv11 format and ready for training. The ML Engineer can use the dataset path provided in the transformation summary to train YOLOv11 models.\n",
    "\n",
    "For more detailed functionality, refer to the comprehensive notebooks in the `notebooks/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Transform Classification Images to YOLOv11 Format\n",
    "\n",
    "Let's transform the images from the classification structure (raw-images/class_name/) to YOLOv11 format with proper train/val splits and label files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import yaml\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to discover classes from raw-images structure\n",
    "def discover_classes_from_s3(bucket, prefix=\"raw-images/\"):\n",
    "    \"\"\"Discover class names from S3 directory structure\"\"\"\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "        Delimiter='/'\n",
    "    )\n",
    "    \n",
    "    classes = []\n",
    "    if 'CommonPrefixes' in response:\n",
    "        for obj in response['CommonPrefixes']:\n",
    "            class_prefix = obj['Prefix']\n",
    "            class_name = class_prefix.replace(prefix, '').rstrip('/')\n",
    "            if class_name:  # Skip empty class names\n",
    "                classes.append(class_name)\n",
    "    \n",
    "    return sorted(classes)\n",
    "\n",
    "# Function to get images for each class\n",
    "def get_images_by_class(bucket, prefix=\"raw-images/\"):\n",
    "    \"\"\"Get all images organized by class\"\"\"\n",
    "    classes = discover_classes_from_s3(bucket, prefix)\n",
    "    images_by_class = {}\n",
    "    \n",
    "    print(f\"Found {len(classes)} classes: {classes}\")\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_prefix = f\"{prefix}{class_name}/\"\n",
    "        objects = list_s3_objects(bucket, prefix=class_prefix)\n",
    "        images = filter_image_files(objects)\n",
    "        images_by_class[class_name] = images\n",
    "        print(f\"  {class_name}: {len(images)} images\")\n",
    "    \n",
    "    return images_by_class\n",
    "\n",
    "# Discover classes and images\n",
    "images_by_class = get_images_by_class(BUCKET_NAME)\n",
    "class_names = list(images_by_class.keys())\n",
    "total_images = sum(len(images) for images in images_by_class.values())\n",
    "\n",
    "print(f\"\\nTotal images found: {total_images}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create YOLO label file content\n",
    "def create_yolo_label(class_id, image_width, image_height, \n",
    "                     bbox_x=None, bbox_y=None, bbox_w=None, bbox_h=None):\n",
    "    \"\"\"Create YOLO format label content\n",
    "    \n",
    "    For classification images, we'll create a bounding box that covers the entire image\n",
    "    since we don't have specific object locations.\n",
    "    \n",
    "    Args:\n",
    "        class_id: Class ID (0-based)\n",
    "        image_width: Image width in pixels\n",
    "        image_height: Image height in pixels\n",
    "        bbox_x, bbox_y, bbox_w, bbox_h: Optional specific bounding box coordinates\n",
    "    \n",
    "    Returns:\n",
    "        YOLO format label string\n",
    "    \"\"\"\n",
    "    if bbox_x is None or bbox_y is None or bbox_w is None or bbox_h is None:\n",
    "        # Create a bounding box covering most of the image (80% centered)\n",
    "        # This assumes the object of interest is roughly centered in the image\n",
    "        x_center = 0.5  # Center of image (normalized)\n",
    "        y_center = 0.5  # Center of image (normalized)\n",
    "        width = 0.8     # 80% of image width (normalized)\n",
    "        height = 0.8    # 80% of image height (normalized)\n",
    "    else:\n",
    "        # Normalize the provided bounding box coordinates\n",
    "        x_center = (bbox_x + bbox_w/2) / image_width\n",
    "        y_center = (bbox_y + bbox_h/2) / image_height\n",
    "        width = bbox_w / image_width\n",
    "        height = bbox_h / image_height\n",
    "    \n",
    "    # Ensure values are within [0, 1] range\n",
    "    x_center = max(0, min(1, x_center))\n",
    "    y_center = max(0, min(1, y_center))\n",
    "    width = max(0, min(1, width))\n",
    "    height = max(0, min(1, height))\n",
    "    \n",
    "    return f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "\n",
    "# Function to split data into train/val sets\n",
    "def split_train_val(images_by_class, train_ratio=0.8, random_seed=42):\n",
    "    \"\"\"Split images into train and validation sets\"\"\"\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    train_data = {}\n",
    "    val_data = {}\n",
    "    \n",
    "    for class_name, images in images_by_class.items():\n",
    "        # Shuffle images\n",
    "        shuffled_images = images.copy()\n",
    "        random.shuffle(shuffled_images)\n",
    "        \n",
    "        # Split into train/val\n",
    "        split_idx = int(len(shuffled_images) * train_ratio)\n",
    "        train_data[class_name] = shuffled_images[:split_idx]\n",
    "        val_data[class_name] = shuffled_images[split_idx:]\n",
    "        \n",
    "        print(f\"{class_name}: {len(train_data[class_name])} train, {len(val_data[class_name])} val\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "# Split data into train/val\n",
    "if images_by_class:\n",
    "    train_data, val_data = split_train_val(images_by_class)\n",
    "    \n",
    "    total_train = sum(len(images) for images in train_data.values())\n",
    "    total_val = sum(len(images) for images in val_data.values())\n",
    "    \n",
    "    print(f\"\\nTotal train images: {total_train}\")\n",
    "    print(f\"Total validation images: {total_val}\")\n",
    "else:\n",
    "    print(\"No images found to split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform and upload images to YOLO format\n",
    "def transform_to_yolo_format(bucket, train_data, val_data, class_names, \n",
    "                           dataset_name, progress_callback=None):\n",
    "    \"\"\"Transform classification images to YOLO format and upload to S3\"\"\"\n",
    "    \n",
    "    # Create class ID mapping\n",
    "    class_to_id = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "    # Define S3 prefixes\n",
    "    base_prefix = f\"datasets/{dataset_name}/\"\n",
    "    train_images_prefix = f\"{base_prefix}train/images/\"\n",
    "    train_labels_prefix = f\"{base_prefix}train/labels/\"\n",
    "    val_images_prefix = f\"{base_prefix}val/images/\"\n",
    "    val_labels_prefix = f\"{base_prefix}val/labels/\"\n",
    "    \n",
    "    def process_split(data, images_prefix, labels_prefix, split_name):\n",
    "        \"\"\"Process a data split (train or val)\"\"\"\n",
    "        processed_count = 0\n",
    "        total_count = sum(len(images) for images in data.values())\n",
    "        \n",
    "        print(f\"\\nProcessing {split_name} split ({total_count} images)...\")\n",
    "        \n",
    "        for class_name, images in data.items():\n",
    "            class_id = class_to_id[class_name]\n",
    "            \n",
    "            for img_obj in images:\n",
    "                try:\n",
    "                    # Get original image key and filename\n",
    "                    original_key = img_obj['Key']\n",
    "                    filename = os.path.basename(original_key)\n",
    "                    name_without_ext = os.path.splitext(filename)[0]\n",
    "                    \n",
    "                    # Download image to get dimensions\n",
    "                    response = s3_client.get_object(Bucket=bucket, Key=original_key)\n",
    "                    img_data = response['Body'].read()\n",
    "                    img = Image.open(io.BytesIO(img_data))\n",
    "                    width, height = img.size\n",
    "                    \n",
    "                    # Copy image to new location\n",
    "                    new_image_key = f\"{images_prefix}{filename}\"\n",
    "                    s3_client.copy_object(\n",
    "                        Bucket=bucket,\n",
    "                        CopySource={'Bucket': bucket, 'Key': original_key},\n",
    "                        Key=new_image_key\n",
    "                    )\n",
    "                    \n",
    "                    # Create YOLO label\n",
    "                    label_content = create_yolo_label(class_id, width, height)\n",
    "                    \n",
    "                    # Upload label file\n",
    "                    label_key = f\"{labels_prefix}{name_without_ext}.txt\"\n",
    "                    s3_client.put_object(\n",
    "                        Bucket=bucket,\n",
    "                        Key=label_key,\n",
    "                        Body=label_content.encode('utf-8')\n",
    "                    )\n",
    "                    \n",
    "                    processed_count += 1\n",
    "                    \n",
    "                    # Progress callback\n",
    "                    if progress_callback and processed_count % 10 == 0:\n",
    "                        progress_callback(processed_count, total_count, split_name)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {original_key}: {str(e)}\")\n",
    "        \n",
    "        print(f\"Completed {split_name} split: {processed_count}/{total_count} images processed\")\n",
    "        return processed_count\n",
    "    \n",
    "    # Progress callback function\n",
    "    def show_progress(current, total, split_name):\n",
    "        percentage = (current / total) * 100\n",
    "        print(f\"  {split_name}: {current}/{total} ({percentage:.1f}%) processed\")\n",
    "    \n",
    "    # Process train and validation splits\n",
    "    train_processed = process_split(train_data, train_images_prefix, train_labels_prefix, \"Train\")\n",
    "    val_processed = process_split(val_data, val_images_prefix, val_labels_prefix, \"Validation\")\n",
    "    \n",
    "    return {\n",
    "        'train_processed': train_processed,\n",
    "        'val_processed': val_processed,\n",
    "        'class_to_id': class_to_id,\n",
    "        'base_prefix': base_prefix\n",
    "    }\n",
    "\n",
    "# Transform images to YOLO format\n",
    "if 'train_data' in locals() and 'val_data' in locals() and class_names:\n",
    "    print(\"Starting transformation to YOLO format...\")\n",
    "    \n",
    "    # Create a new dataset with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dataset_name = f\"yolov11_dataset_{timestamp}\"\n",
    "    \n",
    "    transformation_result = transform_to_yolo_format(\n",
    "        BUCKET_NAME, train_data, val_data, class_names, dataset_name\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTransformation completed!\")\n",
    "    print(f\"Dataset created at: s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
    "    print(f\"Train images processed: {transformation_result['train_processed']}\")\n",
    "    print(f\"Validation images processed: {transformation_result['val_processed']}\")\n",
    "else:\n",
    "    print(\"No data available for transformation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create data.yaml configuration file\n",
    "def create_data_yaml(bucket, dataset_name, class_names, class_to_id):\n",
    "    \"\"\"Create YOLO data.yaml configuration file\"\"\"\n",
    "    \n",
    "    # Create the YAML configuration\n",
    "    data_config = {\n",
    "        'path': f's3://{bucket}/datasets/{dataset_name}',\n",
    "        'train': 'train/images',\n",
    "        'val': 'val/images',\n",
    "        'nc': len(class_names),\n",
    "        'names': {class_to_id[name]: name for name in class_names}\n",
    "    }\n",
    "    \n",
    "    # Convert to YAML string\n",
    "    yaml_content = yaml.dump(data_config, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    # Upload to S3\n",
    "    yaml_key = f\"datasets/{dataset_name}/data.yaml\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=yaml_key,\n",
    "        Body=yaml_content.encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    print(f\"Created data.yaml at: s3://{bucket}/{yaml_key}\")\n",
    "    print(\"\\nYAML content:\")\n",
    "    print(yaml_content)\n",
    "    \n",
    "    return yaml_key\n",
    "\n",
    "# Create data.yaml file\n",
    "if 'transformation_result' in locals():\n",
    "    yaml_key = create_data_yaml(\n",
    "        BUCKET_NAME, \n",
    "        dataset_name, \n",
    "        class_names, \n",
    "        transformation_result['class_to_id']\n",
    "    )\n",
    "else:\n",
    "    print(\"No transformation result available to create data.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify the created dataset\n",
    "def verify_yolo_dataset(bucket, dataset_name):\n",
    "    \"\"\"Verify the created YOLO dataset structure\"\"\"\n",
    "    base_prefix = f\"datasets/{dataset_name}/\"\n",
    "    \n",
    "    # Check directory structure\n",
    "    directories_to_check = [\n",
    "        f\"{base_prefix}train/images/\",\n",
    "        f\"{base_prefix}train/labels/\",\n",
    "        f\"{base_prefix}val/images/\",\n",
    "        f\"{base_prefix}val/labels/\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Verifying dataset: s3://{bucket}/{base_prefix}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    total_files = 0\n",
    "    \n",
    "    for directory in directories_to_check:\n",
    "        objects = list_s3_objects(bucket, prefix=directory)\n",
    "        # Filter out directory markers (empty objects)\n",
    "        files = [obj for obj in objects if obj['Size'] > 0]\n",
    "        \n",
    "        print(f\"{directory}: {len(files)} files\")\n",
    "        total_files += len(files)\n",
    "        \n",
    "        # Show sample files\n",
    "        if files:\n",
    "            sample_files = files[:3]  # Show first 3 files\n",
    "            for file_obj in sample_files:\n",
    "                filename = os.path.basename(file_obj['Key'])\n",
    "                size_kb = file_obj['Size'] / 1024\n",
    "                print(f\"  - {filename} ({size_kb:.1f} KB)\")\n",
    "            if len(files) > 3:\n",
    "                print(f\"  ... and {len(files) - 3} more files\")\n",
    "        print()\n",
    "    \n",
    "    # Check data.yaml\n",
    "    yaml_key = f\"{base_prefix}data.yaml\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=yaml_key)\n",
    "        yaml_content = response['Body'].read().decode('utf-8')\n",
    "        print(f\"data.yaml: Found ({len(yaml_content)} bytes)\")\n",
    "        print(\"Content preview:\")\n",
    "        print(yaml_content[:200] + \"...\" if len(yaml_content) > 200 else yaml_content)\n",
    "    except Exception as e:\n",
    "        print(f\"data.yaml: Not found or error - {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nTotal files in dataset: {total_files}\")\n",
    "    \n",
    "    # Verify image-label pairs\n",
    "    train_images = list_s3_objects(bucket, prefix=f\"{base_prefix}train/images/\")\n",
    "    train_labels = list_s3_objects(bucket, prefix=f\"{base_prefix}train/labels/\")\n",
    "    val_images = list_s3_objects(bucket, prefix=f\"{base_prefix}val/images/\")\n",
    "    val_labels = list_s3_objects(bucket, prefix=f\"{base_prefix}val/labels/\")\n",
    "    \n",
    "    # Filter out directory markers\n",
    "    train_images = [obj for obj in train_images if obj['Size'] > 0]\n",
    "    train_labels = [obj for obj in train_labels if obj['Size'] > 0]\n",
    "    val_images = [obj for obj in val_images if obj['Size'] > 0]\n",
    "    val_labels = [obj for obj in val_labels if obj['Size'] > 0]\n",
    "    \n",
    "    print(f\"\\nImage-Label Pair Verification:\")\n",
    "    print(f\"Train: {len(train_images)} images, {len(train_labels)} labels\")\n",
    "    print(f\"Val: {len(val_images)} images, {len(val_labels)} labels\")\n",
    "    \n",
    "    # Check if counts match\n",
    "    if len(train_images) == len(train_labels) and len(val_images) == len(val_labels):\n",
    "        print(\"âœ… Image-label pairs match!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Image-label pair counts don't match!\")\n",
    "    \n",
    "    return {\n",
    "        'total_files': total_files,\n",
    "        'train_images': len(train_images),\n",
    "        'train_labels': len(train_labels),\n",
    "        'val_images': len(val_images),\n",
    "        'val_labels': len(val_labels)\n",
    "    }\n",
    "\n",
    "# Verify the created dataset\n",
    "if 'dataset_name' in locals():\n",
    "    verification_result = verify_yolo_dataset(BUCKET_NAME, dataset_name)\n",
    "else:\n",
    "    print(\"No dataset available to verify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dataset Summary and Usage Instructions\n",
    "\n",
    "Your dataset has been successfully transformed to YOLOv11 format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset summary\n",
    "if 'dataset_name' in locals() and 'transformation_result' in locals():\n",
    "    print(\"ğŸ‰ Dataset Transformation Complete!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(f\"S3 Location: s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
    "    print(f\"Classes: {len(class_names)} ({', '.join(class_names)})\")\n",
    "    print(f\"Train Images: {transformation_result['train_processed']}\")\n",
    "    print(f\"Validation Images: {transformation_result['val_processed']}\")\n",
    "    print(f\"Total Images: {transformation_result['train_processed'] + transformation_result['val_processed']}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Dataset Structure:\")\n",
    "    print(f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
    "    print(\"â”œâ”€â”€ train/\")\n",
    "    print(\"â”‚   â”œâ”€â”€ images/     # Training images\")\n",
    "    print(\"â”‚   â””â”€â”€ labels/     # Training labels (.txt files)\")\n",
    "    print(\"â”œâ”€â”€ val/\")\n",
    "    print(\"â”‚   â”œâ”€â”€ images/     # Validation images\")\n",
    "    print(\"â”‚   â””â”€â”€ labels/     # Validation labels (.txt files)\")\n",
    "    print(\"â””â”€â”€ data.yaml       # Dataset configuration\")\n",
    "    \n",
    "    print(\"\\nğŸ·ï¸  Class Mapping:\")\n",
    "    for class_name, class_id in transformation_result['class_to_id'].items():\n",
    "        print(f\"  {class_id}: {class_name}\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Label Format:\")\n",
    "    print(\"Each .txt file contains one line per object:\")\n",
    "    print(\"<class_id> <x_center> <y_center> <width> <height>\")\n",
    "    print(\"All coordinates are normalized (0.0 to 1.0)\")\n",
    "    \n",
    "    print(\"\\nğŸš€ Next Steps:\")\n",
    "    print(\"1. Use this dataset in the ML Engineer notebook for training\")\n",
    "    print(\"2. The dataset is ready for YOLOv11 training\")\n",
    "    print(f\"3. Reference the dataset using: s3://{BUCKET_NAME}/datasets/{dataset_name}/data.yaml\")\n",
    "    \n",
    "    # Save dataset info for later use\n",
    "    dataset_info = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'bucket': BUCKET_NAME,\n",
    "        'base_path': f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/\",\n",
    "        'config_path': f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/data.yaml\",\n",
    "        'classes': class_names,\n",
    "        'class_to_id': transformation_result['class_to_id'],\n",
    "        'train_count': transformation_result['train_processed'],\n",
    "        'val_count': transformation_result['val_processed'],\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save dataset info to S3 for reference\n",
    "    info_key = f\"datasets/{dataset_name}/dataset_info.json\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=info_key,\n",
    "        Body=json.dumps(dataset_info, indent=2).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Dataset info saved to: s3://{BUCKET_NAME}/{info_key}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No dataset transformation was completed.\")\n",
    "    print(\"Please run the transformation cells above first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
