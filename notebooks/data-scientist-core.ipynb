{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist Core Workflow\n",
    "\n",
    "This notebook provides essential functionality for Data Scientists to explore and prepare drone imagery data for YOLOv11 model training. It focuses on core capabilities while maintaining simplicity.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Data Exploration**: Analyze and visualize the drone imagery dataset\n",
    "2. **Data Preparation**: Prepare data for YOLOv11 training\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with appropriate permissions\n",
    "- AWS CLI configured with \"ab\" profile\n",
    "- SageMaker Studio access with Data Scientist role\n",
    "- Access to the drone imagery dataset in S3 bucket: `lucaskle-ab3-project-pv`\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Set up AWS session with \"ab\" profile\n",
    "session = boto3.Session(profile_name='ab')\n",
    "s3_client = session.client('s3')\n",
    "region = session.region_name\n",
    "account_id = session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Set up visualization\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Define bucket name\n",
    "BUCKET_NAME = 'lucaskle-ab3-project-pv'\n",
    "\n",
    "print(f\"Data Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account ID: {account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "Let's start by exploring the drone imagery dataset stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list objects in S3 bucket\n",
    "def list_s3_objects(bucket, prefix=\"\", max_items=100):\n",
    "    \"\"\"List objects in an S3 bucket with the given prefix\"\"\"\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "        MaxKeys=max_items\n",
    "    )\n",
    "    \n",
    "    if 'Contents' in response:\n",
    "        return response['Contents']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Function to filter image files\n",
    "def filter_image_files(objects):\n",
    "    \"\"\"Filter image files from S3 objects list\"\"\"\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".tiff\", \".tif\"]\n",
    "    return [obj for obj in objects \n",
    "            if any(obj['Key'].lower().endswith(ext) for ext in image_extensions)]\n",
    "\n",
    "# List raw images in the bucket\n",
    "raw_objects = list_s3_objects(BUCKET_NAME, prefix=\"raw-images/\")\n",
    "raw_images = filter_image_files(raw_objects)\n",
    "\n",
    "print(f\"Found {len(raw_images)} raw images in the bucket\")\n",
    "\n",
    "# Display the first few image keys\n",
    "if raw_images:\n",
    "    print(\"\\nSample image keys:\")\n",
    "    for i, img in enumerate(raw_images[:5]):\n",
    "        print(f\"  {i+1}. {img['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Display Sample Images\n",
    "\n",
    "Let's display some sample images from the dataset to get a visual understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and display images\n",
    "def display_sample_images(bucket, image_objects, num_samples=4):\n",
    "    \"\"\"Download and display sample images from S3\"\"\"\n",
    "    # Limit to the requested number of samples\n",
    "    samples = image_objects[:min(num_samples, len(image_objects))]\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(1, len(samples), figsize=(16, 4))\n",
    "    \n",
    "    # If only one sample, axes is not an array\n",
    "    if len(samples) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Download and display each image\n",
    "    for i, img_obj in enumerate(samples):\n",
    "        try:\n",
    "            # Download image from S3\n",
    "            response = s3_client.get_object(Bucket=bucket, Key=img_obj['Key'])\n",
    "            img_data = response['Body'].read()\n",
    "            \n",
    "            # Open image with PIL\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            \n",
    "            # Display image\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(os.path.basename(img_obj['Key']))\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error displaying image {img_obj['Key']}: {str(e)}\")\n",
    "            axes[i].text(0.5, 0.5, f\"Error loading image\", ha='center')\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images\n",
    "if raw_images:\n",
    "    display_sample_images(BUCKET_NAME, raw_images, num_samples=4)\n",
    "else:\n",
    "    print(\"No images found to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basic Image Analysis\n",
    "\n",
    "Let's analyze some basic characteristics of the images in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze image characteristics\n",
    "def analyze_images(bucket, image_objects, sample_size=20):\n",
    "    \"\"\"Analyze basic characteristics of images\"\"\"\n",
    "    # Limit to sample size\n",
    "    samples = image_objects[:min(sample_size, len(image_objects))]\n",
    "    \n",
    "    # Initialize lists to store image characteristics\n",
    "    widths = []\n",
    "    heights = []\n",
    "    aspect_ratios = []\n",
    "    file_sizes = []\n",
    "    formats = []\n",
    "    \n",
    "    print(f\"Analyzing {len(samples)} sample images...\")\n",
    "    \n",
    "    # Process each image\n",
    "    for img_obj in samples:\n",
    "        try:\n",
    "            # Download image from S3\n",
    "            response = s3_client.get_object(Bucket=bucket, Key=img_obj['Key'])\n",
    "            img_data = response['Body'].read()\n",
    "            \n",
    "            # Get file size\n",
    "            file_size = len(img_data) / (1024 * 1024)  # Convert to MB\n",
    "            file_sizes.append(file_size)\n",
    "            \n",
    "            # Open image with PIL\n",
    "            img = Image.open(io.BytesIO(img_data))\n",
    "            \n",
    "            # Get image dimensions\n",
    "            width, height = img.size\n",
    "            widths.append(width)\n",
    "            heights.append(height)\n",
    "            \n",
    "            # Calculate aspect ratio\n",
    "            aspect_ratio = width / height\n",
    "            aspect_ratios.append(aspect_ratio)\n",
    "            \n",
    "            # Get image format\n",
    "            formats.append(img.format)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image {img_obj['Key']}: {str(e)}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'count': len(widths),\n",
    "        'avg_width': np.mean(widths) if widths else 0,\n",
    "        'avg_height': np.mean(heights) if heights else 0,\n",
    "        'min_width': min(widths) if widths else 0,\n",
    "        'max_width': max(widths) if widths else 0,\n",
    "        'min_height': min(heights) if heights else 0,\n",
    "        'max_height': max(heights) if heights else 0,\n",
    "        'avg_aspect_ratio': np.mean(aspect_ratios) if aspect_ratios else 0,\n",
    "        'avg_file_size': np.mean(file_sizes) if file_sizes else 0,\n",
    "        'formats': list(set(formats)) if formats else []\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'stats': stats,\n",
    "        'widths': widths,\n",
    "        'heights': heights,\n",
    "        'aspect_ratios': aspect_ratios,\n",
    "        'file_sizes': file_sizes,\n",
    "        'formats': formats\n",
    "    }\n",
    "\n",
    "# Analyze sample images\n",
    "if raw_images:\n",
    "    analysis_results = analyze_images(BUCKET_NAME, raw_images, sample_size=20)\n",
    "    \n",
    "    # Display statistics\n",
    "    stats = analysis_results['stats']\n",
    "    print(\"\\nImage Statistics:\")\n",
    "    print(f\"Total images analyzed: {stats['count']}\")\n",
    "    print(f\"Average dimensions: {stats['avg_width']:.1f}x{stats['avg_height']:.1f} pixels\")\n",
    "    print(f\"Dimension range: {stats['min_width']}x{stats['min_height']} to {stats['max_width']}x{stats['max_height']} pixels\")\n",
    "    print(f\"Average aspect ratio: {stats['avg_aspect_ratio']:.2f}\")\n",
    "    print(f\"Average file size: {stats['avg_file_size']:.2f} MB\")\n",
    "    print(f\"Image formats: {', '.join(stats['formats'])}\")\n",
    "else:\n",
    "    print(\"No images found to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualize Image Characteristics\n",
    "\n",
    "Let's create some visualizations to better understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize image characteristics\n",
    "if 'analysis_results' in locals() and analysis_results['stats']['count'] > 0:\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot image dimensions\n",
    "    axes[0, 0].scatter(analysis_results['widths'], analysis_results['heights'])\n",
    "    axes[0, 0].set_xlabel('Width (pixels)')\n",
    "    axes[0, 0].set_ylabel('Height (pixels)')\n",
    "    axes[0, 0].set_title('Image Dimensions')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot aspect ratio distribution\n",
    "    axes[0, 1].hist(analysis_results['aspect_ratios'], bins=10)\n",
    "    axes[0, 1].set_xlabel('Aspect Ratio (width/height)')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Aspect Ratio Distribution')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot file size distribution\n",
    "    axes[1, 0].hist(analysis_results['file_sizes'], bins=10)\n",
    "    axes[1, 0].set_xlabel('File Size (MB)')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].set_title('File Size Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot format distribution\n",
    "    format_counts = {}\n",
    "    for fmt in analysis_results['formats']:\n",
    "        if fmt in format_counts:\n",
    "            format_counts[fmt] += 1\n",
    "        else:\n",
    "            format_counts[fmt] = 1\n",
    "    \n",
    "    formats = list(format_counts.keys())\n",
    "    counts = list(format_counts.values())\n",
    "    \n",
    "    axes[1, 1].bar(formats, counts)\n",
    "    axes[1, 1].set_xlabel('Image Format')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    axes[1, 1].set_title('Image Format Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No analysis results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for YOLOv11 Training\n",
    "\n",
    "Now let's prepare our data for YOLOv11 training. This involves organizing the data in the correct format and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if labeled data exists\n",
    "def check_labeled_data(bucket, prefix=\"labeled-data/\"):\n",
    "    \"\"\"Check if labeled data exists in the bucket\"\"\"\n",
    "    objects = list_s3_objects(bucket, prefix=prefix)\n",
    "    \n",
    "    if objects:\n",
    "        print(f\"Found {len(objects)} objects in labeled data directory\")\n",
    "        \n",
    "        # Group by job name (assuming directory structure)\n",
    "        jobs = {}\n",
    "        for obj in objects:\n",
    "            key = obj['Key']\n",
    "            parts = key.split('/')\n",
    "            if len(parts) > 2:\n",
    "                job_name = parts[1]\n",
    "                if job_name not in jobs:\n",
    "                    jobs[job_name] = []\n",
    "                jobs[job_name].append(key)\n",
    "        \n",
    "        # Display job information\n",
    "        if jobs:\n",
    "            print(f\"\\nFound {len(jobs)} labeling jobs:\")\n",
    "            for job, files in jobs.items():\n",
    "                print(f\"  - {job}: {len(files)} files\")\n",
    "        \n",
    "        return jobs\n",
    "    else:\n",
    "        print(\"No labeled data found\")\n",
    "        return {}\n",
    "\n",
    "# Check for labeled data\n",
    "labeled_jobs = check_labeled_data(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Prepare Data Structure for YOLOv11\n",
    "\n",
    "YOLOv11 requires a specific data structure. Let's prepare our data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create YOLO dataset structure\n",
    "def prepare_yolo_structure(bucket, job_name=None):\n",
    "    \"\"\"Prepare YOLO dataset structure in S3\"\"\"\n",
    "    # Define dataset structure\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    dataset_name = f\"yolov11_dataset_{timestamp}\"\n",
    "    \n",
    "    # Define directories\n",
    "    base_prefix = f\"datasets/{dataset_name}/\"\n",
    "    train_prefix = f\"{base_prefix}train/\"\n",
    "    val_prefix = f\"{base_prefix}val/\"\n",
    "    \n",
    "    # Create empty directories in S3\n",
    "    for prefix in [train_prefix, val_prefix]:\n",
    "        for subdir in [\"images/\", \"labels/\"]:\n",
    "            full_prefix = f\"{prefix}{subdir}\"\n",
    "            # Create an empty object to represent the directory\n",
    "            s3_client.put_object(Bucket=bucket, Key=full_prefix)\n",
    "    \n",
    "    print(f\"Created YOLO dataset structure at s3://{bucket}/{base_prefix}\")\n",
    "    print(\"\\nDirectory structure:\")\n",
    "    print(f\"s3://{bucket}/{base_prefix}\")\n",
    "    print(f\"├── train/\")\n",
    "    print(f\"│   ├── images/\")\n",
    "    print(f\"│   └── labels/\")\n",
    "    print(f\"└── val/\")\n",
    "    print(f\"    ├── images/\")\n",
    "    print(f\"    └── labels/\")\n",
    "    \n",
    "    return {\n",
    "        'dataset_name': dataset_name,\n",
    "        'base_prefix': base_prefix,\n",
    "        'train_prefix': train_prefix,\n",
    "        'val_prefix': val_prefix\n",
    "    }\n",
    "\n",
    "# Create YOLO dataset structure\n",
    "yolo_structure = prepare_yolo_structure(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Preparation Instructions\n",
    "\n",
    "To complete the data preparation for YOLOv11 training, follow these steps:\n",
    "\n",
    "1. **Create Ground Truth Labeling Job**:\n",
    "   - Use the `create_labeling_job.ipynb` notebook to create a labeling job\n",
    "   - Label your images with bounding boxes for objects of interest\n",
    "\n",
    "2. **Convert Ground Truth Output to YOLOv11 Format**:\n",
    "   - After labeling is complete, convert the output to YOLOv11 format\n",
    "   - Use the conversion function in the `create_labeling_job.ipynb` notebook\n",
    "\n",
    "3. **Split Data into Train/Validation Sets**:\n",
    "   - Split your labeled data into training and validation sets\n",
    "   - Typically use 80% for training and 20% for validation\n",
    "\n",
    "4. **Upload Data to YOLO Structure**:\n",
    "   - Upload images to the `images/` directories\n",
    "   - Upload corresponding label files to the `labels/` directories\n",
    "\n",
    "5. **Create Dataset Configuration File**:\n",
    "   - Create a YAML configuration file for your dataset\n",
    "   - Specify paths to train and validation data\n",
    "   - Define class names and IDs\n",
    "\n",
    "Once these steps are complete, your data will be ready for YOLOv11 training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've explored the drone imagery dataset and prepared the structure for YOLOv11 training. Here's a summary of what we've accomplished:\n",
    "\n",
    "1. **Data Exploration**:\n",
    "   - Listed and displayed sample images from the S3 bucket\n",
    "   - Analyzed image characteristics (dimensions, aspect ratios, file sizes)\n",
    "   - Visualized image statistics\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - Checked for existing labeled data\n",
    "   - Created YOLO dataset structure in S3\n",
    "   - Provided instructions for completing data preparation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Create or use existing Ground Truth labeling jobs** to annotate your images\n",
    "2. **Convert labeled data to YOLOv11 format** using the provided tools\n",
    "3. **Organize your data** in the YOLO structure we created\n",
    "4. **Proceed to model training** using the ML Engineer notebook\n",
    "\n",
    "For more detailed functionality, refer to the comprehensive notebooks in the `notebooks/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}