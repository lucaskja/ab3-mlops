{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Scientist Core Workflow with MLFlow\n",
        "\n",
        "This notebook provides essential functionality for Data Scientists to explore and prepare drone imagery data for YOLOv11 model training, with MLFlow experiment tracking integration.\n",
        "\n",
        "## Workflow Overview\n",
        "\n",
        "1. **Data Exploration**: Analyze and visualize the drone imagery dataset\n",
        "2. **Data Preparation**: Prepare data for YOLOv11 training\n",
        "3. **Ground Truth Labeling**: Create labeling jobs for annotation\n",
        "4. **Experiment Tracking**: Track data exploration experiments with MLFlow\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- AWS account with appropriate permissions\n",
        "- AWS CLI configured with \"ab\" profile\n",
        "- SageMaker Studio access with Data Scientist role\n",
        "- Access to the drone imagery dataset in S3 bucket: `lucaskle-ab3-project-pv`\n",
        "- SageMaker managed MLFlow tracking server\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install --quiet mlflow>=3.0.0 requests-auth-aws-sigv4>=0.7 boto3>=1.28.0 sagemaker>=2.190.0 pandas>=2.0.0 matplotlib>=3.7.0 seaborn>=0.12.0 numpy>=1.24.0 PyYAML>=6.0 Pillow>=9.0.0\n",
        "\n",
        "print(\"✅ Required packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from IPython.display import display, HTML\n",
        "import io\n",
        "import json\n",
        "from PIL import Image\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import sagemaker\n",
        "import random\n",
        "import re\n",
        "import yaml\n",
        "from collections import defaultdict\n",
        "\n",
        "# Reliable progress bars for SageMaker Studio\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"✅ All packages imported successfully with SageMaker Studio compatibility\")\n",
        "\n",
        "# Set up AWS session with \"ab\" profile\n",
        "session = boto3.Session(profile_name='ab')\n",
        "s3_client = session.client('s3')\n",
        "sagemaker_client = session.client('sagemaker')\n",
        "sagemaker_session = sagemaker.Session(boto_session=session)\n",
        "region = session.region_name\n",
        "account_id = session.client('sts').get_caller_identity()['Account']\n",
        "\n",
        "# Set up MLFlow tracking with SageMaker managed server\n",
        "try:\n",
        "    # Use the correct tracking server ARN format for SageMaker managed MLflow\n",
        "    tracking_server_arn = \"arn:aws:sagemaker:us-east-1:192771711075:mlflow-tracking-server/sagemaker-core-setup-mlflow-server\"\n",
        "    mlflow.set_tracking_uri(tracking_server_arn)\n",
        "    mlflow_tracking_uri = tracking_server_arn\n",
        "    \n",
        "    print(f\"✅ Connected to SageMaker managed MLflow server\")\n",
        "    print(f\"Tracking Server ARN: {tracking_server_arn}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Could not connect to SageMaker managed MLflow: {e}\")\n",
        "    print(\"Using basic MLflow setup as fallback\")\n",
        "    mlflow_tracking_uri = \"file:///tmp/mlruns\"\n",
        "    mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
        "\n",
        "# Set up visualization\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "\n",
        "# Define bucket name\n",
        "BUCKET_NAME = 'lucaskle-ab3-project-pv'\n",
        "\n",
        "print(f\"Data Bucket: {BUCKET_NAME}\")\n",
        "print(f\"Region: {region}\")\n",
        "print(f\"Account ID: {account_id}\")\n",
        "print(f\"MLFlow Tracking URI: {mlflow_tracking_uri}\")\n",
        "\n",
        "# Helper functions for MLflow logging\n",
        "def log_params(params_dict):\n",
        "    \"\"\"Log parameters using MLflow\"\"\"\n",
        "    mlflow.log_params(params_dict)\n",
        "\n",
        "def log_metrics(metrics_dict, step=None):\n",
        "    \"\"\"Log metrics using MLflow\"\"\"\n",
        "    for key, value in metrics_dict.items():\n",
        "        mlflow.log_metric(key, value, step=step)\n",
        "\n",
        "def log_artifact(local_path, artifact_path=None):\n",
        "    \"\"\"Log artifact using MLflow\"\"\"\n",
        "    mlflow.log_artifact(local_path, artifact_path)\n",
        "\n",
        "print(\"✅ MLflow helper functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Exploration with MLFlow Tracking\n",
        "\n",
        "Let's start by exploring the drone imagery dataset stored in S3 and track our exploration with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start MLFlow experiment for data exploration\n",
        "experiment_name = \"drone-imagery-data-exploration\"\n",
        "\n",
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(experiment_name)\n",
        "mlflow_run = mlflow.start_run(run_name=f\"data-exploration-{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "# Start MLFlow run\n",
        "def list_s3_objects(bucket, prefix=\"\"):\n",
        "    \"\"\"List all objects in an S3 bucket with the given prefix\"\"\"\n",
        "    all_objects = []\n",
        "    paginator = s3_client.get_paginator('list_objects_v2')\n",
        "    \n",
        "    # Create a PageIterator from the Paginator\n",
        "    page_iterator = paginator.paginate(\n",
        "        Bucket=bucket,\n",
        "        Prefix=prefix\n",
        "    )\n",
        "    \n",
        "    # Iterate through each page\n",
        "    for page in page_iterator:\n",
        "        if 'Contents' in page:\n",
        "            all_objects.extend(page['Contents'])\n",
        "    \n",
        "    return all_objects\n",
        "\n",
        "# Function to filter image files\n",
        "def filter_image_files(objects):\n",
        "    \"\"\"Filter image files from S3 objects list\"\"\"\n",
        "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".tiff\", \".tif\"]\n",
        "    return [obj for obj in objects \n",
        "            if any(obj['Key'].lower().endswith(ext) for ext in image_extensions)]\n",
        "\n",
        "# List raw images in the bucket\n",
        "raw_objects = list_s3_objects(BUCKET_NAME, prefix=\"raw-image\")\n",
        "raw_images = filter_image_files(raw_objects)\n",
        "\n",
        "print(f\"Found {len(raw_images)} raw images in the bucket\")\n",
        "\n",
        "# Log dataset statistics to MLFlow\n",
        "mlflow.log_param(\"bucket_name\", BUCKET_NAME)\n",
        "mlflow.log_param(\"data_prefix\", \"raw-images/\")\n",
        "mlflow.log_metric(\"total_images\", len(raw_images))\n",
        "\n",
        "# Display the first few image keys\n",
        "if raw_images:\n",
        "    print(\"\\nSample image keys:\")\n",
        "    for i, img in enumerate(raw_images[:5]):\n",
        "        print(f\"  {i+1}. {img['Key']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Display Sample Images\n",
        "\n",
        "Let's display some sample images from the dataset to get a visual understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to validate image quality (needed for display function)\n",
        "def validate_image_quality(img, min_size=50, max_aspect_ratio=10):\n",
        "    \"\"\"Validate image quality and characteristics\"\"\"\n",
        "    width, height = img.size\n",
        "    aspect_ratio = max(width, height) / min(width, height)\n",
        "    \n",
        "    issues = []\n",
        "    if min(width, height) < min_size:\n",
        "        issues.append(f\"Too small: {width}x{height}\")\n",
        "    if aspect_ratio > max_aspect_ratio:\n",
        "        issues.append(f\"Extreme aspect ratio: {aspect_ratio:.2f}\")\n",
        "    \n",
        "    return len(issues) == 0, issues\n",
        "\n",
        "# Function to get random samples from all classes\n",
        "def get_random_samples_from_classes(image_objects, samples_per_class=1, max_total_samples=4):\n",
        "    \"\"\"Get random samples from different classes/directories\"\"\"\n",
        "    \n",
        "    # Group images by class/directory\n",
        "    class_groups = {}\n",
        "    for img_obj in image_objects:\n",
        "        # Extract class from path (e.g., 'raw-images/Apple___Apple_scab/image.jpg' -> 'Apple___Apple_scab')\n",
        "        key_parts = img_obj['Key'].split('/')\n",
        "        if len(key_parts) > 2:\n",
        "            class_name = key_parts[-2]  # Second to last part is class\n",
        "        else:\n",
        "            class_name = 'unknown'\n",
        "        \n",
        "        if class_name not in class_groups:\n",
        "            class_groups[class_name] = []\n",
        "        class_groups[class_name].append(img_obj)\n",
        "    \n",
        "    print(f\"📊 Found {len(class_groups)} classes in dataset\")\n",
        "    \n",
        "    # Sample from each class\n",
        "    selected_samples = []\n",
        "    classes_sampled = []\n",
        "    \n",
        "    # If we have more classes than max samples, randomly select classes\n",
        "    available_classes = list(class_groups.keys())\n",
        "    if len(available_classes) > max_total_samples:\n",
        "        selected_classes = random.sample(available_classes, max_total_samples)\n",
        "        samples_per_class = 1\n",
        "    else:\n",
        "        selected_classes = available_classes\n",
        "        samples_per_class = min(samples_per_class, max_total_samples // len(selected_classes))\n",
        "    \n",
        "    for class_name in selected_classes:\n",
        "        class_images = class_groups[class_name]\n",
        "        sample_size = min(samples_per_class, len(class_images))\n",
        "        \n",
        "        # Randomly sample from this class\n",
        "        sampled = random.sample(class_images, sample_size)\n",
        "        selected_samples.extend(sampled)\n",
        "        classes_sampled.append(f\"{class_name} ({sample_size})\")\n",
        "        \n",
        "        if len(selected_samples) >= max_total_samples:\n",
        "            break\n",
        "    \n",
        "    # Limit to max_total_samples\n",
        "    selected_samples = selected_samples[:max_total_samples]\n",
        "    \n",
        "    print(f\"🎯 Sampled from classes: {', '.join(classes_sampled)}\")\n",
        "    \n",
        "    return selected_samples, class_groups\n",
        "\n",
        "# Enhanced function to download and display images with validation and progress tracking\n",
        "def display_sample_images_enhanced(bucket, image_objects, num_samples=4):\n",
        "    \"\"\"Download and display sample images from S3 with validation, progress tracking, and class diversity\"\"\"\n",
        "    \n",
        "    # Get random samples from different classes\n",
        "    samples, class_info = get_random_samples_from_classes(\n",
        "        image_objects, \n",
        "        samples_per_class=1, \n",
        "        max_total_samples=num_samples\n",
        "    )\n",
        "    \n",
        "    print(f\"Loading {len(samples)} sample images from different classes...\")\n",
        "    \n",
        "    # Create a figure with subplots\n",
        "    fig, axes = plt.subplots(1, len(samples), figsize=(16, 4))\n",
        "    \n",
        "    # If only one sample, axes is not an array\n",
        "    if len(samples) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    # Track image loading with progress\n",
        "    loaded_images = []\n",
        "    failed_images = []\n",
        "    \n",
        "    # Download and display each image with progress tracking\n",
        "    # Use regular tqdm instead of notebook version to avoid widget issues\n",
        "    print(\"📥 Downloading and processing images...\")\n",
        "    for i, img_obj in enumerate(samples):\n",
        "        try:\n",
        "            print(f\"  Processing image {i+1}/{len(samples)}: {os.path.basename(img_obj['Key'])}\")\n",
        "            \n",
        "            # Download image from S3\n",
        "            response = s3_client.get_object(Bucket=bucket, Key=img_obj['Key'])\n",
        "            img_data = response['Body'].read()\n",
        "            \n",
        "            # Open image with PIL\n",
        "            img = Image.open(io.BytesIO(img_data))\n",
        "            \n",
        "            # Validate image quality\n",
        "            is_valid, issues = validate_image_quality(img)\n",
        "            \n",
        "            # Extract class name for display\n",
        "            key_parts = img_obj['Key'].split('/')\n",
        "            class_name = key_parts[-2] if len(key_parts) > 2 else 'unknown'\n",
        "            \n",
        "            # Display image\n",
        "            axes[i].imshow(img)\n",
        "            title = f\"{class_name}\\n{os.path.basename(img_obj['Key'])}\"\n",
        "            if not is_valid:\n",
        "                title += f\" ⚠️\"  # Add warning for quality issues\n",
        "            axes[i].set_title(title, fontsize=9)\n",
        "            axes[i].axis('off')\n",
        "            \n",
        "            # Add image info as text\n",
        "            width, height = img.size\n",
        "            file_size = len(img_data) / 1024  # KB\n",
        "            info_text = f\"{width}×{height}\\n{file_size:.1f}KB\"\n",
        "            if not is_valid:\n",
        "                info_text += f\"\\nIssues: {len(issues)}\"\n",
        "            axes[i].text(0.02, 0.98, info_text, transform=axes[i].transAxes, \n",
        "                       fontsize=8, verticalalignment='top', \n",
        "                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "            \n",
        "            loaded_images.append({\n",
        "                'key': img_obj['Key'],\n",
        "                'class': class_name,\n",
        "                'size': (width, height),\n",
        "                'file_size_kb': file_size,\n",
        "                'valid': is_valid,\n",
        "                'issues': issues\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error displaying image {img_obj['Key']}: {str(e)}\")\n",
        "            axes[i].text(0.5, 0.5, f\"Error loading image\\n{str(e)[:50]}...\", \n",
        "                       ha='center', va='center', fontsize=8, wrap=True)\n",
        "            axes[i].axis('off')\n",
        "            failed_images.append({'key': img_obj['Key'], 'error': str(e)})\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Display validation summary\n",
        "    print(f\"\\n📊 Image Loading Summary:\")\n",
        "    print(f\"✅ Successfully loaded: {len(loaded_images)}\")\n",
        "    print(f\"❌ Failed to load: {len(failed_images)}\")\n",
        "    \n",
        "    if loaded_images:\n",
        "        valid_count = sum(1 for img in loaded_images if img['valid'])\n",
        "        print(f\"✅ Quality validation passed: {valid_count}/{len(loaded_images)}\")\n",
        "        \n",
        "        # Show class distribution\n",
        "        class_counts = {}\n",
        "        for img in loaded_images:\n",
        "            class_name = img['class']\n",
        "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
        "        \n",
        "        print(f\"🏷️  Classes represented: {', '.join([f'{cls}({cnt})' for cls, cnt in class_counts.items()])}\")\n",
        "        \n",
        "        # Show quality issues if any\n",
        "        quality_issues = [img for img in loaded_images if not img['valid']]\n",
        "        if quality_issues:\n",
        "            print(f\"⚠️  Images with quality issues:\")\n",
        "            for img in quality_issues[:3]:  # Show first 3\n",
        "                print(f\"  • {os.path.basename(img['key'])}: {', '.join(img['issues'])}\")\n",
        "    \n",
        "    # Save plot as artifact in MLFlow with metadata\n",
        "    plt.savefig('sample_images.png', dpi=150, bbox_inches='tight')\n",
        "    mlflow.log_artifact('sample_images.png')\n",
        "    \n",
        "    # Log image loading statistics\n",
        "    mlflow.log_metric(\"sample_images_loaded\", len(loaded_images))\n",
        "    mlflow.log_metric(\"sample_images_failed\", len(failed_images))\n",
        "    mlflow.log_metric(\"sample_classes_represented\", len(set(img['class'] for img in loaded_images)))\n",
        "    \n",
        "    if loaded_images:\n",
        "        mlflow.log_metric(\"sample_images_valid\", sum(1 for img in loaded_images if img['valid']))\n",
        "        avg_width = np.mean([img['size'][0] for img in loaded_images])\n",
        "        avg_height = np.mean([img['size'][1] for img in loaded_images])\n",
        "        avg_file_size = np.mean([img['file_size_kb'] for img in loaded_images])\n",
        "        mlflow.log_metric(\"sample_avg_width\", avg_width)\n",
        "        mlflow.log_metric(\"sample_avg_height\", avg_height)\n",
        "        mlflow.log_metric(\"sample_avg_file_size_kb\", avg_file_size)\n",
        "        \n",
        "        # Log class distribution\n",
        "        class_counts = {}\n",
        "        for img in loaded_images:\n",
        "            class_name = img['class']\n",
        "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
        "        \n",
        "        for class_name, count in class_counts.items():\n",
        "            # Sanitize class name for MLflow\n",
        "            sanitized_name = re.sub(r'[^a-zA-Z0-9_\\-\\.\\s:/]', '_', str(class_name))\n",
        "            mlflow.log_metric(f\"sample_class_{sanitized_name}\", count)\n",
        "    \n",
        "    return loaded_images, failed_images, class_info\n",
        "\n",
        "# Display sample images with enhanced validation and class diversity\n",
        "if raw_images:\n",
        "    print(\"🖼️  Displaying sample images from different classes...\")\n",
        "    loaded_imgs, failed_imgs, class_distribution = display_sample_images_enhanced(\n",
        "        BUCKET_NAME, raw_images, num_samples=4\n",
        "    )\n",
        "    \n",
        "    if loaded_imgs:\n",
        "        print(f\"\\n✅ Successfully displayed {len(loaded_imgs)} images from {len(set(img['class'] for img in loaded_imgs))} different classes\")\n",
        "    else:\n",
        "        print(\"❌ No images could be displayed\")\n",
        "        \n",
        "else:\n",
        "    print(\"No images found to display\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Basic Image Analysis with MLFlow Tracking\n",
        "\n",
        "Let's analyze some basic characteristics of the images in our dataset and track the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue with the current active MLFlow run\n",
        "\n",
        "# Function to sanitize names for MLflow (fixes the error)\n",
        "def sanitize_mlflow_name(name):\n",
        "    \"\"\"Sanitize names for MLflow compatibility - only alphanumerics, underscores, dashes, periods, spaces, colons, slashes\"\"\"\n",
        "    # Replace problematic characters with underscores\n",
        "    sanitized = re.sub(r'[^a-zA-Z0-9_\\-\\.\\s:/]', '_', str(name))\n",
        "    # Remove multiple consecutive underscores\n",
        "    sanitized = re.sub(r'_+', '_', sanitized)\n",
        "    # Remove leading/trailing underscores\n",
        "    sanitized = sanitized.strip('_')\n",
        "    return sanitized\n",
        "\n",
        "# Enhanced validation functions\n",
        "def validate_image_quality(img, min_size=50, max_aspect_ratio=10):\n",
        "    \"\"\"Validate image quality and characteristics\"\"\"\n",
        "    width, height = img.size\n",
        "    aspect_ratio = max(width, height) / min(width, height)\n",
        "    \n",
        "    issues = []\n",
        "    if min(width, height) < min_size:\n",
        "        issues.append(f\"Too small: {width}x{height}\")\n",
        "    if aspect_ratio > max_aspect_ratio:\n",
        "        issues.append(f\"Extreme aspect ratio: {aspect_ratio:.2f}\")\n",
        "    \n",
        "    return len(issues) == 0, issues\n",
        "\n",
        "def validate_dataset_integrity(class_distribution, min_samples_per_class=5):\n",
        "    \"\"\"Validate dataset has sufficient samples per class\"\"\"\n",
        "    issues = []\n",
        "    for class_name, count in class_distribution.items():\n",
        "        if count < min_samples_per_class:\n",
        "            issues.append(f\"Class '{class_name}' has only {count} samples (minimum: {min_samples_per_class})\")\n",
        "    \n",
        "    return len(issues) == 0, issues\n",
        "\n",
        "# Function to get stratified sample by class/directory\n",
        "def get_stratified_sample(image_objects, samples_per_class=10):\n",
        "    \"\"\"Get stratified sample from images organized by directory/class\"\"\"\n",
        "    # Group images by directory (assuming directory represents class)\n",
        "    class_groups = {}\n",
        "    for img_obj in image_objects:\n",
        "        # Extract directory/class from key (e.g., 'raw-images/class1/image.jpg' -> 'class1')\n",
        "        key_parts = img_obj['Key'].split('/')\n",
        "        if len(key_parts) > 2:  # Has subdirectory structure\n",
        "            class_name = key_parts[-2]  # Second to last part is class\n",
        "        else:\n",
        "            class_name = 'unknown'  # Flat structure\n",
        "        \n",
        "        if class_name not in class_groups:\n",
        "            class_groups[class_name] = []\n",
        "        class_groups[class_name].append(img_obj)\n",
        "    \n",
        "    # Sample from each class\n",
        "    stratified_sample = []\n",
        "    print(f\"Found {len(class_groups)} classes/directories:\")\n",
        "    for class_name, class_images in class_groups.items():\n",
        "        sample_size = min(samples_per_class, len(class_images))\n",
        "        # Use random sampling instead of just taking first N\n",
        "        sampled = random.sample(class_images, sample_size)\n",
        "        stratified_sample.extend(sampled)\n",
        "        print(f\"  '{class_name}': {len(class_images):,} images → sampling {sample_size}\")\n",
        "    \n",
        "    return stratified_sample, class_groups\n",
        "\n",
        "# Multiple bounding box strategies (Recommendation 3)\n",
        "def get_bounding_box_strategy(strategy='centered_80', image_width=None, image_height=None):\n",
        "    \"\"\"Generate different bounding box strategies for object detection\"\"\"\n",
        "    strategies = {\n",
        "        'centered_80': {\n",
        "            'x_center': 0.5, 'y_center': 0.5, 'width': 0.8, 'height': 0.8,\n",
        "            'description': '80% centered box (default)'\n",
        "        },\n",
        "        'full_image': {\n",
        "            'x_center': 0.5, 'y_center': 0.5, 'width': 0.95, 'height': 0.95,\n",
        "            'description': '95% full image coverage'\n",
        "        },\n",
        "        'conservative': {\n",
        "            'x_center': 0.5, 'y_center': 0.5, 'width': 0.6, 'height': 0.6,\n",
        "            'description': '60% conservative box'\n",
        "        },\n",
        "        'adaptive': {\n",
        "            'x_center': 0.5, 'y_center': 0.5, \n",
        "            'width': 0.9 if image_width and image_width < 300 else 0.8,\n",
        "            'height': 0.9 if image_height and image_height < 300 else 0.8,\n",
        "            'description': 'Adaptive based on image size'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return strategies.get(strategy, strategies['centered_80'])\n",
        "\n",
        "# Function to analyze image characteristics (optimized with progress bars and validation)\n",
        "def analyze_images_optimized(bucket, image_objects, max_total_samples=200, bbox_strategy='centered_80'):\n",
        "    \"\"\"Analyze basic characteristics of images with optimized sampling, validation, and progress tracking\"\"\"\n",
        "    \n",
        "    # First, get a quick peek at class structure\n",
        "    sample_for_classes = image_objects[:min(100, len(image_objects))]\n",
        "    unique_classes = set()\n",
        "    for obj in sample_for_classes:\n",
        "        key_parts = obj['Key'].split('/')\n",
        "        class_name = key_parts[-2] if len(key_parts) > 2 else 'unknown'\n",
        "        unique_classes.add(class_name)\n",
        "    \n",
        "    # Calculate samples per class\n",
        "    samples_per_class = max(5, max_total_samples // len(unique_classes)) if unique_classes else max_total_samples\n",
        "    \n",
        "    # Get stratified sample\n",
        "    samples, class_info = get_stratified_sample(image_objects, samples_per_class=samples_per_class)\n",
        "    \n",
        "    # Initialize lists to store image characteristics\n",
        "    widths = []\n",
        "    heights = []\n",
        "    aspect_ratios = []\n",
        "    file_sizes = []\n",
        "    formats = []\n",
        "    class_distribution = {}\n",
        "    quality_issues = []\n",
        "    \n",
        "    print(f\"\\nAnalyzing {len(samples)} strategically sampled images with validation...\")\n",
        "    \n",
        "    # Progress bar for image analysis (Recommendation 1)\n",
        "    with tqdm(total=len(samples), desc=\"Processing images\", unit=\"img\") as pbar:\n",
        "        for i, img_obj in enumerate(samples):\n",
        "            try:\n",
        "                # Get class from path\n",
        "                key_parts = img_obj['Key'].split('/')\n",
        "                class_name = key_parts[-2] if len(key_parts) > 2 else 'unknown'\n",
        "                class_distribution[class_name] = class_distribution.get(class_name, 0) + 1\n",
        "                \n",
        "                # Download image from S3\n",
        "                response = s3_client.get_object(Bucket=bucket, Key=img_obj['Key'])\n",
        "                img_data = response['Body'].read()\n",
        "                \n",
        "                # Get file size\n",
        "                file_size = len(img_data) / (1024 * 1024)  # Convert to MB\n",
        "                file_sizes.append(file_size)\n",
        "                \n",
        "                # Open image with PIL\n",
        "                img = Image.open(io.BytesIO(img_data))\n",
        "                \n",
        "                # Get image dimensions\n",
        "                width, height = img.size\n",
        "                widths.append(width)\n",
        "                heights.append(height)\n",
        "                \n",
        "                # Calculate aspect ratio\n",
        "                aspect_ratio = width / height\n",
        "                aspect_ratios.append(aspect_ratio)\n",
        "                \n",
        "                # Get image format\n",
        "                formats.append(img.format)\n",
        "                \n",
        "                # Validate image quality (Recommendation 2)\n",
        "                is_valid, issues = validate_image_quality(img)\n",
        "                if not is_valid:\n",
        "                    quality_issues.extend([f\"{img_obj['Key']}: {issue}\" for issue in issues])\n",
        "                \n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\n",
        "                    'Class': class_name[:10] + '...' if len(class_name) > 10 else class_name,\n",
        "                    'Size': f\"{width}x{height}\",\n",
        "                    'Issues': len(quality_issues)\n",
        "                })\n",
        "                pbar.update(1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                quality_issues.append(f\"Error processing {img_obj['Key']}: {str(e)}\")\n",
        "                pbar.update(1)\n",
        "    \n",
        "    # Validate dataset integrity (Recommendation 2)\n",
        "    dataset_valid, dataset_issues = validate_dataset_integrity(class_distribution)\n",
        "    \n",
        "    # Get bounding box strategy info (Recommendation 3)\n",
        "    bbox_info = get_bounding_box_strategy(bbox_strategy, np.mean(widths), np.mean(heights))\n",
        "    \n",
        "    # Calculate statistics\n",
        "    stats = {\n",
        "        'count': len(widths),\n",
        "        'total_dataset_size': len(image_objects),\n",
        "        'classes_found': len(class_info),\n",
        "        'class_distribution': class_distribution,\n",
        "        'avg_width': np.mean(widths) if widths else 0,\n",
        "        'avg_height': np.mean(heights) if heights else 0,\n",
        "        'min_width': min(widths) if widths else 0,\n",
        "        'max_width': max(widths) if widths else 0,\n",
        "        'min_height': min(heights) if heights else 0,\n",
        "        'max_height': max(heights) if heights else 0,\n",
        "        'avg_aspect_ratio': np.mean(aspect_ratios) if aspect_ratios else 0,\n",
        "        'avg_file_size': np.mean(file_sizes) if file_sizes else 0,\n",
        "        'formats': list(set(formats)) if formats else [],\n",
        "        'quality_issues': quality_issues,\n",
        "        'dataset_valid': dataset_valid,\n",
        "        'dataset_issues': dataset_issues,\n",
        "        'bbox_strategy': bbox_info\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        'stats': stats,\n",
        "        'widths': widths,\n",
        "        'heights': heights,\n",
        "        'aspect_ratios': aspect_ratios,\n",
        "        'file_sizes': file_sizes,\n",
        "        'formats': formats,\n",
        "        'class_info': class_info\n",
        "    }\n",
        "\n",
        "# Analyze sample images with optimized approach\n",
        "if raw_images:\n",
        "    print(f\"Found {len(raw_images):,} total images in dataset\")\n",
        "    \n",
        "    # Choose bounding box strategy based on image characteristics\n",
        "    # Since your images are small (251×249 avg), use adaptive strategy\n",
        "    bbox_strategy = 'adaptive'\n",
        "    \n",
        "    analysis_results = analyze_images_optimized(\n",
        "        BUCKET_NAME, \n",
        "        raw_images, \n",
        "        max_total_samples=200,\n",
        "        bbox_strategy=bbox_strategy\n",
        "    )\n",
        "    \n",
        "    # Display statistics\n",
        "    stats = analysis_results['stats']\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DATASET ANALYSIS SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total dataset size: {stats['total_dataset_size']:,} images\")\n",
        "    print(f\"Sample analyzed: {stats['count']} images ({stats['count']/stats['total_dataset_size']*100:.1f}%)\")\n",
        "    print(f\"Classes/directories found: {stats['classes_found']}\")\n",
        "    \n",
        "    # Validation Results (Recommendation 2)\n",
        "    print(f\"\\n📋 VALIDATION RESULTS:\")\n",
        "    print(f\"Dataset integrity: {'✅ VALID' if stats['dataset_valid'] else '⚠️  ISSUES FOUND'}\")\n",
        "    if stats['dataset_issues']:\n",
        "        for issue in stats['dataset_issues']:\n",
        "            print(f\"  • {issue}\")\n",
        "    \n",
        "    if stats['quality_issues']:\n",
        "        print(f\"Image quality issues: {len(stats['quality_issues'])} found\")\n",
        "        for issue in stats['quality_issues'][:5]:  # Show first 5\n",
        "            print(f\"  • {issue}\")\n",
        "        if len(stats['quality_issues']) > 5:\n",
        "            print(f\"  ... and {len(stats['quality_issues']) - 5} more\")\n",
        "    else:\n",
        "        print(\"Image quality: ✅ All samples passed validation\")\n",
        "    \n",
        "    print(f\"\\nClass Distribution in Sample:\")\n",
        "    for class_name, count in sorted(stats['class_distribution'].items()):\n",
        "        total_in_class = len(analysis_results['class_info'][class_name])\n",
        "        percentage = (count / stats['count']) * 100\n",
        "        print(f\"  {class_name}: {count} samples ({percentage:.1f}%) from {total_in_class:,} total images\")\n",
        "    \n",
        "    print(f\"\\nImage Characteristics:\")\n",
        "    print(f\"  Average dimensions: {stats['avg_width']:.0f}×{stats['avg_height']:.0f} pixels\")\n",
        "    print(f\"  Dimension range: {stats['min_width']}×{stats['min_height']} to {stats['max_width']}×{stats['max_height']} pixels\")\n",
        "    print(f\"  Average aspect ratio: {stats['avg_aspect_ratio']:.2f}\")\n",
        "    print(f\"  Average file size: {stats['avg_file_size']:.2f} MB\")\n",
        "    print(f\"  Image formats: {', '.join(stats['formats'])}\")\n",
        "    \n",
        "    # Bounding Box Strategy Info (Recommendation 3)\n",
        "    bbox_info = stats['bbox_strategy']\n",
        "    print(f\"\\n🎯 BOUNDING BOX STRATEGY: {bbox_strategy}\")\n",
        "    print(f\"  Strategy: {bbox_info['description']}\")\n",
        "    print(f\"  Box dimensions: {bbox_info['width']*100:.0f}% × {bbox_info['height']*100:.0f}%\")\n",
        "    print(f\"  Center position: ({bbox_info['x_center']}, {bbox_info['y_center']})\")\n",
        "    \n",
        "    # Log comprehensive statistics to MLFlow (with sanitized names to fix error)\n",
        "    mlflow.log_param(\"total_dataset_size\", stats['total_dataset_size'])\n",
        "    mlflow.log_param(\"sample_size\", stats['count'])\n",
        "    mlflow.log_param(\"sampling_percentage\", f\"{stats['count']/stats['total_dataset_size']*100:.1f}%\")\n",
        "    mlflow.log_param(\"classes_found\", stats['classes_found'])\n",
        "    mlflow.log_param(\"class_names\", [sanitize_mlflow_name(name) for name in stats['class_distribution'].keys()])\n",
        "    mlflow.log_param(\"dataset_valid\", stats['dataset_valid'])\n",
        "    mlflow.log_param(\"quality_issues_count\", len(stats['quality_issues']))\n",
        "    mlflow.log_param(\"bbox_strategy\", bbox_strategy)\n",
        "    mlflow.log_param(\"bbox_description\", bbox_info['description'])\n",
        "    \n",
        "    # Log image characteristics\n",
        "    mlflow.log_metric(\"avg_width\", stats['avg_width'])\n",
        "    mlflow.log_metric(\"avg_height\", stats['avg_height'])\n",
        "    mlflow.log_metric(\"min_width\", stats['min_width'])\n",
        "    mlflow.log_metric(\"max_width\", stats['max_width'])\n",
        "    mlflow.log_metric(\"min_height\", stats['min_height'])\n",
        "    mlflow.log_metric(\"max_height\", stats['max_height'])\n",
        "    mlflow.log_metric(\"avg_aspect_ratio\", stats['avg_aspect_ratio'])\n",
        "    mlflow.log_metric(\"avg_file_size_mb\", stats['avg_file_size'])\n",
        "    mlflow.log_param(\"image_formats\", \", \".join(stats['formats']))\n",
        "    \n",
        "    # Log class distribution and dataset composition (with sanitized names - FIXES THE ERROR)\n",
        "    for class_name, count in stats['class_distribution'].items():\n",
        "        sanitized_name = sanitize_mlflow_name(class_name)\n",
        "        mlflow.log_metric(f\"sample_count_{sanitized_name}\", count)\n",
        "        total_in_class = len(analysis_results['class_info'][class_name])\n",
        "        mlflow.log_metric(f\"total_count_{sanitized_name}\", total_in_class)\n",
        "    \n",
        "    # Log bounding box strategy parameters\n",
        "    mlflow.log_metric(\"bbox_width_ratio\", bbox_info['width'])\n",
        "    mlflow.log_metric(\"bbox_height_ratio\", bbox_info['height'])\n",
        "    mlflow.log_metric(\"bbox_x_center\", bbox_info['x_center'])\n",
        "    mlflow.log_metric(\"bbox_y_center\", bbox_info['y_center'])\n",
        "    \n",
        "    print(f\"\\n✅ Analysis complete and logged to MLFlow experiment\")\n",
        "    print(f\"📊 Analyzed {stats['count']} representative samples from {stats['total_dataset_size']:,} total images\")\n",
        "    print(f\"🎯 Using {bbox_strategy} bounding box strategy optimized for your image dimensions\")\n",
        "    \n",
        "else:\n",
        "    print(\"No images found to analyze\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Visualize Image Characteristics\n",
        "\n",
        "Let's create some visualizations to better understand our dataset and save them as MLFlow artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize image characteristics\n",
        "if 'analysis_results' in locals() and analysis_results['stats']['count'] > 0:\n",
        "    # Create a figure with subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Plot image dimensions\n",
        "    axes[0, 0].scatter(analysis_results['widths'], analysis_results['heights'])\n",
        "    axes[0, 0].set_xlabel('Width (pixels)')\n",
        "    axes[0, 0].set_ylabel('Height (pixels)')\n",
        "    axes[0, 0].set_title('Image Dimensions')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot aspect ratio distribution\n",
        "    axes[0, 1].hist(analysis_results['aspect_ratios'], bins=10)\n",
        "    axes[0, 1].set_xlabel('Aspect Ratio (width/height)')\n",
        "    axes[0, 1].set_ylabel('Count')\n",
        "    axes[0, 1].set_title('Aspect Ratio Distribution')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot file size distribution\n",
        "    axes[1, 0].hist(analysis_results['file_sizes'], bins=10)\n",
        "    axes[1, 0].set_xlabel('File Size (MB)')\n",
        "    axes[1, 0].set_ylabel('Count')\n",
        "    axes[1, 0].set_title('File Size Distribution')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot format distribution\n",
        "    format_counts = {}\n",
        "    for fmt in analysis_results['formats']:\n",
        "        if fmt in format_counts:\n",
        "            format_counts[fmt] += 1\n",
        "        else:\n",
        "            format_counts[fmt] = 1\n",
        "    \n",
        "    formats = list(format_counts.keys())\n",
        "    counts = list(format_counts.values())\n",
        "    \n",
        "    axes[1, 1].bar(formats, counts)\n",
        "    axes[1, 1].set_xlabel('Image Format')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "    axes[1, 1].set_title('Image Format Distribution')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Save visualization as MLFlow artifact\n",
        "    plt.savefig('image_analysis.png', dpi=150, bbox_inches='tight')\n",
        "    mlflow.log_artifact('image_analysis.png')\n",
        "    \n",
        "else:\n",
        "    print(\"No analysis results available for visualization\")\n",
        "    \n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation for YOLOv11 Training\n",
        "\n",
        "Now let's prepare our data for YOLOv11 training and track the preparation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start new MLFlow run for data preparation\n",
        "# Continue with the current active MLFlow run\n",
        "mlflow_run = mlflow.start_run(run_name=f\"data-preparation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "# Enhanced function to check if labeled data exists with validation and progress tracking\n",
        "def check_labeled_data_enhanced(bucket, prefix=\"labeled-data/\"):\n",
        "    \"\"\"Check if labeled data exists in the bucket with comprehensive validation\"\"\"\n",
        "    \n",
        "    print(f\"🔍 Scanning for labeled data in s3://{bucket}/{prefix}...\")\n",
        "    \n",
        "    # Get all objects with progress tracking\n",
        "    objects = list_s3_objects(bucket, prefix=prefix)\n",
        "    \n",
        "    if not objects:\n",
        "        print(\"❌ No labeled data found\")\n",
        "        mlflow.log_metric(\"labeled_jobs_count\", 0)\n",
        "        mlflow.log_metric(\"labeled_files_count\", 0)\n",
        "        mlflow.log_param(\"labeled_data_status\", \"not_found\")\n",
        "        return {}\n",
        "    \n",
        "    print(f\"📁 Found {len(objects)} objects in labeled data directory\")\n",
        "    \n",
        "    # Enhanced validation and grouping with progress tracking\n",
        "    jobs = {}\n",
        "    valid_files = []\n",
        "    invalid_files = []\n",
        "    file_types = {'json': 0, 'txt': 0, 'other': 0}\n",
        "    \n",
        "    print(\"📊 Analyzing labeled data structure...\")\n",
        "    with tqdm(total=len(objects), desc=\"Analyzing files\", unit=\"file\") as pbar:\n",
        "        for obj in objects:\n",
        "            try:\n",
        "                key = obj['Key']\n",
        "                parts = key.split('/')\n",
        "                \n",
        "                # Validate file structure\n",
        "                if len(parts) < 3:\n",
        "                    invalid_files.append({'key': key, 'issue': 'Invalid directory structure'})\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "                \n",
        "                # Extract job name and validate\n",
        "                job_name = parts[1] if len(parts) > 2 else 'unknown'\n",
        "                \n",
        "                # Validate file type\n",
        "                file_ext = key.lower().split('.')[-1] if '.' in key else 'unknown'\n",
        "                if file_ext in ['json', 'jsonl']:\n",
        "                    file_types['json'] += 1\n",
        "                elif file_ext == 'txt':\n",
        "                    file_types['txt'] += 1\n",
        "                else:\n",
        "                    file_types['other'] += 1\n",
        "                \n",
        "                # Group by job\n",
        "                if job_name not in jobs:\n",
        "                    jobs[job_name] = {\n",
        "                        'files': [],\n",
        "                        'json_files': 0,\n",
        "                        'txt_files': 0,\n",
        "                        'other_files': 0,\n",
        "                        'total_size_mb': 0\n",
        "                    }\n",
        "                \n",
        "                jobs[job_name]['files'].append(key)\n",
        "                jobs[job_name][f\"{file_ext}_files\"] = jobs[job_name].get(f\"{file_ext}_files\", 0) + 1\n",
        "                jobs[job_name]['total_size_mb'] += obj.get('Size', 0) / (1024 * 1024)\n",
        "                \n",
        "                valid_files.append(key)\n",
        "                \n",
        "                pbar.set_postfix({\n",
        "                    'Jobs': len(jobs),\n",
        "                    'Valid': len(valid_files),\n",
        "                    'Invalid': len(invalid_files)\n",
        "                })\n",
        "                pbar.update(1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                invalid_files.append({'key': obj.get('Key', 'unknown'), 'issue': str(e)})\n",
        "                pbar.update(1)\n",
        "    \n",
        "    # Display comprehensive analysis\n",
        "    print(f\"\\n📋 Labeled Data Analysis Summary:\")\n",
        "    print(f\"✅ Valid files: {len(valid_files)}\")\n",
        "    print(f\"❌ Invalid files: {len(invalid_files)}\")\n",
        "    print(f\"📊 File types: JSON: {file_types['json']}, TXT: {file_types['txt']}, Other: {file_types['other']}\")\n",
        "    \n",
        "    if jobs:\n",
        "        print(f\"\\n🏷️  Found {len(jobs)} labeling jobs:\")\n",
        "        for job_name, job_info in jobs.items():\n",
        "            total_size_mb = job_info['total_size_mb']\n",
        "            print(f\"  📁 {job_name}:\")\n",
        "            print(f\"    • Files: {len(job_info['files'])}\")\n",
        "            print(f\"    • JSON: {job_info.get('json_files', 0)}, TXT: {job_info.get('txt_files', 0)}\")\n",
        "            print(f\"    • Size: {total_size_mb:.2f} MB\")\n",
        "        \n",
        "        # Enhanced MLFlow logging\n",
        "        mlflow.log_metric(\"labeled_jobs_count\", len(jobs))\n",
        "        mlflow.log_metric(\"labeled_files_count\", len(valid_files))\n",
        "        mlflow.log_metric(\"labeled_invalid_files\", len(invalid_files))\n",
        "        mlflow.log_metric(\"labeled_json_files\", file_types['json'])\n",
        "        mlflow.log_metric(\"labeled_txt_files\", file_types['txt'])\n",
        "        mlflow.log_param(\"labeled_data_status\", \"found\")\n",
        "        \n",
        "        # Log job details\n",
        "        for job_name, job_info in jobs.items():\n",
        "            sanitized_job_name = sanitize_mlflow_name(job_name)\n",
        "            mlflow.log_metric(f\"job_{sanitized_job_name}_files\", len(job_info['files']))\n",
        "            mlflow.log_metric(f\"job_{sanitized_job_name}_size_mb\", job_info['total_size_mb'])\n",
        "    \n",
        "    # Show validation issues if any\n",
        "    if invalid_files:\n",
        "        print(f\"\\n⚠️  Invalid files found:\")\n",
        "        for invalid in invalid_files[:5]:  # Show first 5\n",
        "            print(f\"  • {invalid['key']}: {invalid['issue']}\")\n",
        "        if len(invalid_files) > 5:\n",
        "            print(f\"  ... and {len(invalid_files) - 5} more\")\n",
        "    \n",
        "    return {\n",
        "        'jobs': jobs,\n",
        "        'valid_files': valid_files,\n",
        "        'invalid_files': invalid_files,\n",
        "        'file_types': file_types,\n",
        "        'total_size_mb': sum(job['total_size_mb'] for job in jobs.values())\n",
        "    }\n",
        "\n",
        "# Check for labeled data with enhanced validation\n",
        "print(\"🔍 Checking for existing labeled data...\")\n",
        "labeled_data_analysis = check_labeled_data_enhanced(BUCKET_NAME)\n",
        "\n",
        "# Store results for later use\n",
        "if labeled_data_analysis and labeled_data_analysis.get('jobs'):\n",
        "    labeled_jobs = labeled_data_analysis['jobs']\n",
        "    print(f\"✅ Found labeled data from {len(labeled_jobs)} jobs\")\n",
        "else:\n",
        "    labeled_jobs = {}\n",
        "    print(\"ℹ️  No existing labeled data found - will proceed with data preparation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Prepare Data Structure for YOLOv11\n",
        "\n",
        "YOLOv11 requires a specific data structure. Let's prepare our data accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced function to create YOLO dataset structure with validation and multiple strategies\n",
        "def prepare_yolo_structure_enhanced(bucket, job_name=None, structure_strategy='standard'):\n",
        "    \"\"\"Prepare YOLO dataset structure in S3 with enhanced validation and multiple strategies\"\"\"\n",
        "    \n",
        "    print(f\"🏗️  Preparing YOLO dataset structure...\")\n",
        "    print(f\"📦 Strategy: {structure_strategy}\")\n",
        "    \n",
        "    # Define dataset structure with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    dataset_name = f\"yolov11_dataset_{timestamp}\"\n",
        "    \n",
        "    # Multiple structure strategies\n",
        "    structure_strategies = {\n",
        "        'standard': {\n",
        "            'description': 'Standard YOLOv11 structure with train/val split',\n",
        "            'splits': ['train', 'val'],\n",
        "            'subdirs': ['images', 'labels']\n",
        "        },\n",
        "        'extended': {\n",
        "            'description': 'Extended structure with test split',\n",
        "            'splits': ['train', 'val', 'test'],\n",
        "            'subdirs': ['images', 'labels']\n",
        "        },\n",
        "        'hierarchical': {\n",
        "            'description': 'Hierarchical structure with class subdirectories',\n",
        "            'splits': ['train', 'val'],\n",
        "            'subdirs': ['images', 'labels'],\n",
        "            'class_subdirs': True\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    strategy_config = structure_strategies.get(structure_strategy, structure_strategies['standard'])\n",
        "    print(f\"📋 Using strategy: {strategy_config['description']}\")\n",
        "    \n",
        "    # Define base structure\n",
        "    base_prefix = f\"datasets/{dataset_name}/\"\n",
        "    \n",
        "    # Validate S3 bucket accessibility\n",
        "    print(\"🔍 Validating S3 bucket accessibility...\")\n",
        "    try:\n",
        "        s3_client.head_bucket(Bucket=bucket)\n",
        "        print(f\"✅ S3 bucket '{bucket}' is accessible\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ S3 bucket validation failed: {e}\")\n",
        "        mlflow.log_param(\"structure_creation_error\", str(e))\n",
        "        return None\n",
        "    \n",
        "    # Create directory structure with progress tracking\n",
        "    directories_created = []\n",
        "    creation_errors = []\n",
        "    \n",
        "    total_dirs = len(strategy_config['splits']) * len(strategy_config['subdirs'])\n",
        "    print(f\"📁 Creating {total_dirs} directories...\")\n",
        "    \n",
        "    with tqdm(total=total_dirs, desc=\"Creating directories\", unit=\"dir\") as pbar:\n",
        "        for split in strategy_config['splits']:\n",
        "            split_prefix = f\"{base_prefix}{split}/\"\n",
        "            \n",
        "            for subdir in strategy_config['subdirs']:\n",
        "                try:\n",
        "                    full_prefix = f\"{split_prefix}{subdir}/\"\n",
        "                    \n",
        "                    # Create directory marker object\n",
        "                    s3_client.put_object(\n",
        "                        Bucket=bucket, \n",
        "                        Key=full_prefix,\n",
        "                        Body='',\n",
        "                        ContentType='application/x-directory'\n",
        "                    )\n",
        "                    \n",
        "                    directories_created.append(full_prefix)\n",
        "                    \n",
        "                    pbar.set_postfix({\n",
        "                        'Created': len(directories_created),\n",
        "                        'Errors': len(creation_errors)\n",
        "                    })\n",
        "                    pbar.update(1)\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Failed to create {full_prefix}: {str(e)}\"\n",
        "                    creation_errors.append(error_msg)\n",
        "                    print(f\"❌ {error_msg}\")\n",
        "                    pbar.update(1)\n",
        "    \n",
        "    # Validation of created structure\n",
        "    print(f\"\\n📊 Structure Creation Summary:\")\n",
        "    print(f\"✅ Directories created: {len(directories_created)}\")\n",
        "    print(f\"❌ Creation errors: {len(creation_errors)}\")\n",
        "    \n",
        "    if creation_errors:\n",
        "        print(f\"\\n⚠️  Creation errors:\")\n",
        "        for error in creation_errors:\n",
        "            print(f\"  • {error}\")\n",
        "    \n",
        "    # Display the created structure\n",
        "    print(f\"\\n📁 Created YOLO dataset structure at s3://{bucket}/{base_prefix}\")\n",
        "    print(f\"\\n🌳 Directory structure:\")\n",
        "    print(f\"s3://{bucket}/{base_prefix}\")\n",
        "    \n",
        "    for split in strategy_config['splits']:\n",
        "        print(f\"├── {split}/\")\n",
        "        for i, subdir in enumerate(strategy_config['subdirs']):\n",
        "            connector = \"└──\" if i == len(strategy_config['subdirs']) - 1 else \"├──\"\n",
        "            print(f\"│   {connector} {subdir}/\")\n",
        "    \n",
        "    # Create dataset configuration metadata\n",
        "    dataset_config = {\n",
        "        'dataset_name': dataset_name,\n",
        "        'base_prefix': base_prefix,\n",
        "        'structure_strategy': structure_strategy,\n",
        "        'splits': strategy_config['splits'],\n",
        "        'subdirs': strategy_config['subdirs'],\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'directories_created': len(directories_created),\n",
        "        'creation_errors': len(creation_errors),\n",
        "        'bucket': bucket\n",
        "    }\n",
        "    \n",
        "    # Create split-specific prefixes for easy access\n",
        "    split_prefixes = {}\n",
        "    for split in strategy_config['splits']:\n",
        "        split_prefixes[f'{split}_prefix'] = f\"{base_prefix}{split}/\"\n",
        "    \n",
        "    # Enhanced MLFlow logging\n",
        "    mlflow.log_param(\"dataset_name\", dataset_name)\n",
        "    mlflow.log_param(\"dataset_s3_path\", f\"s3://{bucket}/{base_prefix}\")\n",
        "    mlflow.log_param(\"structure_strategy\", structure_strategy)\n",
        "    mlflow.log_param(\"structure_description\", strategy_config['description'])\n",
        "    mlflow.log_metric(\"directories_created\", len(directories_created))\n",
        "    mlflow.log_metric(\"creation_errors\", len(creation_errors))\n",
        "    mlflow.log_param(\"splits\", ', '.join(strategy_config['splits']))\n",
        "    mlflow.log_param(\"subdirs\", ', '.join(strategy_config['subdirs']))\n",
        "    \n",
        "    # Log split prefixes\n",
        "    for split in strategy_config['splits']:\n",
        "        mlflow.log_param(f\"{split}_prefix\", f\"{base_prefix}{split}/\")\n",
        "    \n",
        "    # Save dataset configuration as artifact\n",
        "    config_filename = 'dataset_structure_config.json'\n",
        "    with open(config_filename, 'w') as f:\n",
        "        json.dump(dataset_config, f, indent=2)\n",
        "    mlflow.log_artifact(config_filename)\n",
        "    \n",
        "    # Validate structure integrity\n",
        "    print(f\"\\n🔍 Validating created structure...\")\n",
        "    validation_result = validate_yolo_structure(bucket, base_prefix, strategy_config)\n",
        "    \n",
        "    if validation_result['valid']:\n",
        "        print(f\"✅ Structure validation passed\")\n",
        "        mlflow.log_param(\"structure_validation\", \"passed\")\n",
        "    else:\n",
        "        print(f\"⚠️  Structure validation issues found:\")\n",
        "        for issue in validation_result['issues']:\n",
        "            print(f\"  • {issue}\")\n",
        "        mlflow.log_param(\"structure_validation\", \"issues_found\")\n",
        "        mlflow.log_param(\"validation_issues\", ', '.join(validation_result['issues']))\n",
        "    \n",
        "    result = {\n",
        "        'dataset_name': dataset_name,\n",
        "        'base_prefix': base_prefix,\n",
        "        'structure_strategy': structure_strategy,\n",
        "        'directories_created': len(directories_created),\n",
        "        'creation_errors': creation_errors,\n",
        "        'validation': validation_result,\n",
        "        **split_prefixes\n",
        "    }\n",
        "    \n",
        "    return result\n",
        "\n",
        "def validate_yolo_structure(bucket, base_prefix, strategy_config):\n",
        "    \"\"\"Validate the created YOLO structure\"\"\"\n",
        "    issues = []\n",
        "    \n",
        "    try:\n",
        "        # Check if all expected directories exist\n",
        "        for split in strategy_config['splits']:\n",
        "            for subdir in strategy_config['subdirs']:\n",
        "                expected_prefix = f\"{base_prefix}{split}/{subdir}/\"\n",
        "                \n",
        "                try:\n",
        "                    # Try to list objects with this prefix\n",
        "                    response = s3_client.list_objects_v2(\n",
        "                        Bucket=bucket,\n",
        "                        Prefix=expected_prefix,\n",
        "                        MaxKeys=1\n",
        "                    )\n",
        "                    # If no error, directory exists\n",
        "                except Exception as e:\n",
        "                    issues.append(f\"Directory {expected_prefix} not accessible: {str(e)}\")\n",
        "        \n",
        "        return {\n",
        "            'valid': len(issues) == 0,\n",
        "            'issues': issues\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'valid': False,\n",
        "            'issues': [f\"Structure validation failed: {str(e)}\"]\n",
        "        }\n",
        "\n",
        "# Create enhanced YOLO dataset structure\n",
        "print(\"🏗️  Creating YOLO dataset structure...\")\n",
        "\n",
        "# Choose structure strategy based on requirements\n",
        "# For drone detection with potential class hierarchies, use 'standard' for simplicity\n",
        "structure_strategy = 'standard'\n",
        "\n",
        "yolo_structure = prepare_yolo_structure_enhanced(\n",
        "    BUCKET_NAME, \n",
        "    structure_strategy=structure_strategy\n",
        ")\n",
        "\n",
        "if yolo_structure:\n",
        "    print(f\"\\n🎉 YOLO dataset structure created successfully!\")\n",
        "    print(f\"📦 Dataset name: {yolo_structure['dataset_name']}\")\n",
        "    print(f\"📁 Base path: s3://{BUCKET_NAME}/{yolo_structure['base_prefix']}\")\n",
        "    print(f\"🏗️  Strategy: {yolo_structure['structure_strategy']}\")\n",
        "    print(f\"📊 Directories created: {yolo_structure['directories_created']}\")\n",
        "    \n",
        "    if yolo_structure['validation']['valid']:\n",
        "        print(f\"✅ Structure validation: PASSED\")\n",
        "    else:\n",
        "        print(f\"⚠️  Structure validation: ISSUES FOUND\")\n",
        "        print(\"💡 Check the validation issues above\")\n",
        "    \n",
        "    # Store for use in subsequent cells\n",
        "    dataset_name = yolo_structure['dataset_name']\n",
        "    base_prefix = yolo_structure['base_prefix']\n",
        "    \n",
        "else:\n",
        "    print(\"❌ Failed to create YOLO dataset structure\")\n",
        "    print(\"💡 Check the error messages above for troubleshooting\")\n",
        "\n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Ground Truth Labeling Job Creation\n",
        "\n",
        "Now let's create a SageMaker Ground Truth labeling job to annotate our drone imagery for object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Configure Labeling Job Parameters\n",
        "\n",
        "Let's configure the parameters for our Ground Truth labeling job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start new MLFlow run for labeling job creation\n",
        "# Continue with the current active MLFlow run\n",
        "mlflow_run = mlflow.start_run(run_name=f\"ground-truth-labeling-{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
        "# Configure labeling job parameters\n",
        "labeling_job_config = {\n",
        "    'job_name': f\"drone-detection-labeling-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    'input_s3_path': f\"s3://{BUCKET_NAME}/raw-images/\",\n",
        "    'output_s3_path': f\"s3://{BUCKET_NAME}/labeled-data/\",\n",
        "    'task_type': 'BoundingBox',\n",
        "    'labels': ['drone', 'vehicle', 'person', 'building'],\n",
        "    'instructions': 'Please draw bounding boxes around all drones and other objects visible in the image.',\n",
        "    'max_budget_usd': 50.00,\n",
        "    'workforce_type': 'private'  # or 'public' for Mechanical Turk\n",
        "}\n",
        "\n",
        "# Display configuration\n",
        "print(\"Labeling Job Configuration:\")\n",
        "for key, value in labeling_job_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Log configuration to MLFlow\n",
        "for key, value in labeling_job_config.items():\n",
        "    if isinstance(value, (str, int, float)):\n",
        "        mlflow.log_param(f\"labeling_{key}\", value)\n",
        "    elif isinstance(value, list):\n",
        "        mlflow.log_param(f\"labeling_{key}\", \", \".join(value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Create Input Manifest for Ground Truth\n",
        "\n",
        "Ground Truth requires an input manifest file that lists all images to be labeled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced function to create input manifest for Ground Truth with validation and progress tracking\n",
        "def create_input_manifest_enhanced(bucket, image_objects, output_key=\"input-manifest.json\", max_images=50):\n",
        "    \"\"\"Create input manifest file for Ground Truth labeling job with validation and progress tracking\"\"\"\n",
        "    \n",
        "    # Limit images for demo and validate selection\n",
        "    selected_images = image_objects[:min(max_images, len(image_objects))]\n",
        "    \n",
        "    print(f\"Creating input manifest for {len(selected_images)} images...\")\n",
        "    \n",
        "    # Create manifest entries with validation\n",
        "    manifest_entries = []\n",
        "    valid_images = []\n",
        "    invalid_images = []\n",
        "    \n",
        "    with tqdm(total=len(selected_images), desc=\"Validating images\", unit=\"img\") as pbar:\n",
        "        for img_obj in selected_images:\n",
        "            try:\n",
        "                s3_uri = f\"s3://{bucket}/{img_obj['Key']}\"\n",
        "                \n",
        "                # Basic validation - check if image exists and is accessible\n",
        "                try:\n",
        "                    # Quick head request to validate accessibility\n",
        "                    s3_client.head_object(Bucket=bucket, Key=img_obj['Key'])\n",
        "                    \n",
        "                    # Validate file extension\n",
        "                    valid_extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.tif']\n",
        "                    if not any(img_obj['Key'].lower().endswith(ext) for ext in valid_extensions):\n",
        "                        raise ValueError(f\"Invalid file extension: {img_obj['Key']}\")\n",
        "                    \n",
        "                    # Create manifest entry\n",
        "                    manifest_entry = {\n",
        "                        \"source-ref\": s3_uri\n",
        "                    }\n",
        "                    manifest_entries.append(manifest_entry)\n",
        "                    valid_images.append(img_obj['Key'])\n",
        "                    \n",
        "                except Exception as validation_error:\n",
        "                    invalid_images.append({\n",
        "                        'key': img_obj['Key'], \n",
        "                        'error': str(validation_error)\n",
        "                    })\n",
        "                \n",
        "                pbar.set_postfix({\n",
        "                    'Valid': len(valid_images), \n",
        "                    'Invalid': len(invalid_images)\n",
        "                })\n",
        "                pbar.update(1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                invalid_images.append({\n",
        "                    'key': img_obj['Key'], \n",
        "                    'error': str(e)\n",
        "                })\n",
        "                pbar.update(1)\n",
        "    \n",
        "    # Validation summary\n",
        "    print(f\"\\n📋 Manifest Validation Summary:\")\n",
        "    print(f\"✅ Valid images: {len(valid_images)}\")\n",
        "    print(f\"❌ Invalid images: {len(invalid_images)}\")\n",
        "    \n",
        "    if invalid_images:\n",
        "        print(f\"\\n⚠️  Invalid images found:\")\n",
        "        for invalid in invalid_images[:5]:  # Show first 5\n",
        "            print(f\"  • {os.path.basename(invalid['key'])}: {invalid['error']}\")\n",
        "        if len(invalid_images) > 5:\n",
        "            print(f\"  ... and {len(invalid_images) - 5} more\")\n",
        "    \n",
        "    if not manifest_entries:\n",
        "        print(\"❌ No valid images found for manifest creation!\")\n",
        "        return None\n",
        "    \n",
        "    # Create manifest file content\n",
        "    manifest_content = \"\\n\".join([json.dumps(entry) for entry in manifest_entries])\n",
        "    \n",
        "    # Upload manifest to S3 with progress tracking\n",
        "    print(f\"\\n📤 Uploading manifest to S3...\")\n",
        "    try:\n",
        "        s3_client.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=output_key,\n",
        "            Body=manifest_content,\n",
        "            ContentType='application/json'\n",
        "        )\n",
        "        \n",
        "        manifest_uri = f\"s3://{bucket}/{output_key}\"\n",
        "        print(f\"✅ Manifest uploaded successfully: {manifest_uri}\")\n",
        "        \n",
        "        # Log manifest creation to MLFlow\n",
        "        mlflow.log_param(\"manifest_s3_uri\", manifest_uri)\n",
        "        mlflow.log_param(\"manifest_total_images\", len(manifest_entries))\n",
        "        mlflow.log_metric(\"manifest_valid_images\", len(valid_images))\n",
        "        mlflow.log_metric(\"manifest_invalid_images\", len(invalid_images))\n",
        "        mlflow.log_param(\"manifest_validation_success_rate\", f\"{len(valid_images)/len(selected_images)*100:.1f}%\")\n",
        "        \n",
        "        # Save manifest locally and log as artifact\n",
        "        with open('input_manifest.json', 'w') as f:\n",
        "            f.write(manifest_content)\n",
        "        mlflow.log_artifact('input_manifest.json')\n",
        "        \n",
        "        return {\n",
        "            'manifest_uri': manifest_uri,\n",
        "            'total_images': len(manifest_entries),\n",
        "            'valid_images': valid_images,\n",
        "            'invalid_images': invalid_images,\n",
        "            'success_rate': len(valid_images)/len(selected_images)*100\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error uploading manifest: {str(e)}\")\n",
        "        mlflow.log_param(\"manifest_creation_error\", str(e))\n",
        "        return None\n",
        "\n",
        "# Create input manifest with enhanced validation\n",
        "if raw_images:\n",
        "    print(\"Creating input manifest for Ground Truth labeling job...\")\n",
        "    manifest_result = create_input_manifest_enhanced(\n",
        "        BUCKET_NAME, \n",
        "        raw_images, \n",
        "        output_key=\"ground-truth/input-manifest.json\",\n",
        "        max_images=20  # Limit for demo\n",
        "    )\n",
        "    \n",
        "    if manifest_result:\n",
        "        print(f\"\\n🎯 Manifest created successfully!\")\n",
        "        print(f\"📊 Success rate: {manifest_result['success_rate']:.1f}%\")\n",
        "        print(f\"📁 Location: {manifest_result['manifest_uri']}\")\n",
        "        \n",
        "        # Store for later use\n",
        "        input_manifest_uri = manifest_result['manifest_uri']\n",
        "    else:\n",
        "        print(\"❌ Failed to create manifest\")\n",
        "else:\n",
        "    print(\"No images available for manifest creation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Create Ground Truth Labeling Job\n",
        "\n",
        "Now let's create the actual Ground Truth labeling job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced function to create Ground Truth labeling job with comprehensive validation\n",
        "def create_ground_truth_job_enhanced(config, manifest_uri):\n",
        "    \"\"\"Create SageMaker Ground Truth labeling job with enhanced validation and monitoring\"\"\"\n",
        "    \n",
        "    print(\"🚀 Creating SageMaker Ground Truth labeling job...\")\n",
        "    \n",
        "    try:\n",
        "        # Enhanced validation of prerequisites\n",
        "        print(\"📋 Validating prerequisites...\")\n",
        "        \n",
        "        # 1. Validate IAM role\n",
        "        try:\n",
        "            role_arn = sagemaker_session.get_caller_identity_arn()\n",
        "            print(f\"✅ IAM role validated: {role_arn}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"IAM role validation failed: {e}\")\n",
        "        \n",
        "        # 2. Validate manifest accessibility\n",
        "        try:\n",
        "            manifest_bucket = manifest_uri.split('/')[2]\n",
        "            manifest_key = '/'.join(manifest_uri.split('/')[3:])\n",
        "            s3_client.head_object(Bucket=manifest_bucket, Key=manifest_key)\n",
        "            print(f\"✅ Input manifest validated: {manifest_uri}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Manifest validation failed: {e}\")\n",
        "        \n",
        "        # 3. Validate S3 output path\n",
        "        output_bucket = config['output_s3_uri'].split('/')[2]\n",
        "        try:\n",
        "            s3_client.head_bucket(Bucket=output_bucket)\n",
        "            print(f\"✅ Output S3 bucket validated: {output_bucket}\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Output S3 bucket validation failed: {e}\")\n",
        "        \n",
        "        # Enhanced job configuration with validation\n",
        "        job_config = {\n",
        "            'LabelingJobName': config['job_name'],\n",
        "            'LabelAttributeName': 'bounding-box',\n",
        "            'InputConfig': {\n",
        "                'DataSource': {\n",
        "                    'S3DataSource': {\n",
        "                        'ManifestS3Uri': manifest_uri\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            'OutputConfig': {\n",
        "                'S3OutputPath': config['output_s3_uri']\n",
        "            },\n",
        "            'RoleArn': role_arn,\n",
        "            'HumanTaskConfig': {\n",
        "                'WorkteamArn': f\"arn:aws:sagemaker:{region}:{account_id}:workteam/private-crowd/default\",\n",
        "                'UiConfig': {\n",
        "                    'UiTemplateS3Uri': 's3://sagemaker-example-files-prod-us-east-1/ground-truth/object-detection/template.liquid'\n",
        "                },\n",
        "                'PreHumanTaskLambdaArn': f'arn:aws:lambda:{region}:432418664414:function:PRE-BoundingBox',\n",
        "                'TaskTitle': config.get('task_title', 'Drone Detection Labeling'),\n",
        "                'TaskDescription': config.get('task_description', 'Draw bounding boxes around drones and other objects in aerial imagery'),\n",
        "                'NumberOfHumanWorkersPerDataObject': config.get('workers_per_object', 1),\n",
        "                'TaskTimeLimitInSeconds': config.get('time_limit_seconds', 3600),\n",
        "                'TaskAvailabilityLifetimeInSeconds': config.get('availability_seconds', 86400),\n",
        "                'MaxConcurrentTaskCount': config.get('max_concurrent_tasks', 10),\n",
        "                'AnnotationConsolidationConfig': {\n",
        "                    'AnnotationConsolidationLambdaArn': f'arn:aws:lambda:{region}:432418664414:function:ACS-BoundingBox'\n",
        "                }\n",
        "            },\n",
        "            'Tags': [\n",
        "                {'Key': 'Project', 'Value': 'drone-detection'},\n",
        "                {'Key': 'Environment', 'Value': 'development'},\n",
        "                {'Key': 'CreatedBy', 'Value': 'data-scientist-notebook'}\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        # Add label categories if provided\n",
        "        if 'labels' in config and config['labels']:\n",
        "            job_config['LabelCategoryConfigS3Uri'] = create_label_categories_file(\n",
        "                config['labels'], \n",
        "                f\"{config['output_s3_uri']}/label-categories.json\"\n",
        "            )\n",
        "            print(f\"✅ Label categories configured: {config['labels']}\")\n",
        "        \n",
        "        print(\"🔄 Submitting labeling job...\")\n",
        "        \n",
        "        # Create the labeling job\n",
        "        response = sagemaker_client.create_labeling_job(**job_config)\n",
        "        \n",
        "        job_arn = response['LabelingJobArn']\n",
        "        print(f\"✅ Labeling job created successfully!\")\n",
        "        print(f\"📋 Job Name: {config['job_name']}\")\n",
        "        print(f\"🔗 Job ARN: {job_arn}\")\n",
        "        \n",
        "        # Log job creation to MLFlow\n",
        "        mlflow.log_param(\"labeling_job_name\", config['job_name'])\n",
        "        mlflow.log_param(\"labeling_job_arn\", job_arn)\n",
        "        mlflow.log_param(\"labeling_job_status\", \"CREATED\")\n",
        "        mlflow.log_param(\"labeling_manifest_uri\", manifest_uri)\n",
        "        mlflow.log_param(\"labeling_output_uri\", config['output_s3_uri'])\n",
        "        mlflow.log_param(\"labeling_workers_per_object\", config.get('workers_per_object', 1))\n",
        "        mlflow.log_param(\"labeling_time_limit\", config.get('time_limit_seconds', 3600))\n",
        "        \n",
        "        if 'labels' in config:\n",
        "            mlflow.log_param(\"labeling_categories\", ', '.join(config['labels']))\n",
        "            mlflow.log_metric(\"labeling_categories_count\", len(config['labels']))\n",
        "        \n",
        "        # Enhanced monitoring setup\n",
        "        print(\"\\n📊 Setting up job monitoring...\")\n",
        "        monitoring_result = setup_enhanced_monitoring(config['job_name'])\n",
        "        \n",
        "        return {\n",
        "            'job_name': config['job_name'],\n",
        "            'job_arn': job_arn,\n",
        "            'status': 'CREATED',\n",
        "            'monitoring': monitoring_result\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        print(f\"❌ Error creating labeling job: {error_msg}\")\n",
        "        \n",
        "        # Enhanced error diagnosis\n",
        "        print(\"\\n🔍 Error Diagnosis:\")\n",
        "        if \"workteam\" in error_msg.lower():\n",
        "            print(\"• Issue: Private workforce not configured\")\n",
        "            print(\"• Solution: Create a private workforce in SageMaker Console\")\n",
        "            print(\"• Steps:\")\n",
        "            print(\"  1. Go to SageMaker Console > Ground Truth > Labeling workforces\")\n",
        "            print(\"  2. Create a private workforce\")\n",
        "            print(\"  3. Add team members to the workforce\")\n",
        "        elif \"role\" in error_msg.lower():\n",
        "            print(\"• Issue: IAM role permissions insufficient\")\n",
        "            print(\"• Solution: Ensure SageMaker execution role has Ground Truth permissions\")\n",
        "        elif \"manifest\" in error_msg.lower():\n",
        "            print(\"• Issue: Input manifest file problem\")\n",
        "            print(\"• Solution: Check manifest file format and S3 accessibility\")\n",
        "        elif \"s3\" in error_msg.lower():\n",
        "            print(\"• Issue: S3 access problem\")\n",
        "            print(\"• Solution: Verify S3 bucket permissions and paths\")\n",
        "        \n",
        "        # Log error to MLFlow\n",
        "        mlflow.log_param(\"labeling_job_error\", error_msg)\n",
        "        mlflow.log_param(\"labeling_job_status\", \"FAILED\")\n",
        "        \n",
        "        return None\n",
        "\n",
        "def create_label_categories_file(labels, s3_uri):\n",
        "    \"\"\"Create label categories file for Ground Truth\"\"\"\n",
        "    categories = {\n",
        "        \"document-version\": \"2018-11-28\",\n",
        "        \"labels\": [{\"label\": label} for label in labels]\n",
        "    }\n",
        "    \n",
        "    # Upload to S3\n",
        "    bucket = s3_uri.split('/')[2]\n",
        "    key = '/'.join(s3_uri.split('/')[3:])\n",
        "    \n",
        "    s3_client.put_object(\n",
        "        Bucket=bucket,\n",
        "        Key=key,\n",
        "        Body=json.dumps(categories),\n",
        "        ContentType='application/json'\n",
        "    )\n",
        "    \n",
        "    return s3_uri\n",
        "\n",
        "def setup_enhanced_monitoring(job_name):\n",
        "    \"\"\"Set up enhanced monitoring for the labeling job\"\"\"\n",
        "    try:\n",
        "        # Create monitoring configuration\n",
        "        monitoring_config = {\n",
        "            'job_name': job_name,\n",
        "            'check_interval_seconds': 300,  # Check every 5 minutes\n",
        "            'notifications_enabled': True\n",
        "        }\n",
        "        \n",
        "        print(f\"✅ Monitoring configured for job: {job_name}\")\n",
        "        return monitoring_config\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Monitoring setup failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Enhanced Ground Truth job creation\n",
        "if 'input_manifest_uri' in locals() and input_manifest_uri:\n",
        "    # Enhanced labeling job configuration with validation\n",
        "    labeling_config = {\n",
        "        'job_name': f\"drone-detection-labeling-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "        'output_s3_uri': f\"s3://{BUCKET_NAME}/ground-truth-output/\",\n",
        "        'labels': ['drone', 'vehicle', 'person', 'building'],\n",
        "        'task_title': 'Drone Detection in Aerial Imagery',\n",
        "        'task_description': 'Draw bounding boxes around drones, vehicles, people, and buildings in aerial imagery. Be precise with bounding box placement.',\n",
        "        'workers_per_object': 1,\n",
        "        'time_limit_seconds': 1800,  # 30 minutes per image\n",
        "        'max_concurrent_tasks': 5,\n",
        "        'availability_seconds': 86400  # 24 hours\n",
        "    }\n",
        "    \n",
        "    # Create the labeling job with enhanced validation\n",
        "    job_result = create_ground_truth_job_enhanced(labeling_config, input_manifest_uri)\n",
        "    \n",
        "    if job_result:\n",
        "        print(f\"\\n🎉 Ground Truth labeling job setup complete!\")\n",
        "        print(f\"📋 Job Name: {job_result['job_name']}\")\n",
        "        print(f\"📊 Monitor progress in SageMaker Console > Ground Truth\")\n",
        "        \n",
        "        # Store job info for monitoring\n",
        "        current_labeling_job = job_result['job_name']\n",
        "    else:\n",
        "        print(\"❌ Failed to create Ground Truth labeling job\")\n",
        "        print(\"💡 Check the error diagnosis above for troubleshooting steps\")\n",
        "else:\n",
        "    print(\"⚠️  No input manifest available. Please create the manifest first.\")\n",
        "    \n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Monitor Labeling Job Progress\n",
        "\n",
        "Let's create a function to monitor the labeling job progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to monitor labeling job\n",
        "def monitor_labeling_job(job_name):\n",
        "    \"\"\"Monitor Ground Truth labeling job progress\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = sagemaker_client.describe_labeling_job(\n",
        "            LabelingJobName=job_name\n",
        "        )\n",
        "        \n",
        "        status = response['LabelingJobStatus']\n",
        "        creation_time = response['CreationTime']\n",
        "        \n",
        "        print(f\"Job Name: {job_name}\")\n",
        "        print(f\"Status: {status}\")\n",
        "        print(f\"Created: {creation_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        \n",
        "        if 'LabelCounters' in response:\n",
        "            counters = response['LabelCounters']\n",
        "            print(f\"Total objects: {counters.get('TotalLabeled', 0) + counters.get('Unlabeled', 0)}\")\n",
        "            print(f\"Labeled: {counters.get('TotalLabeled', 0)}\")\n",
        "            print(f\"Remaining: {counters.get('Unlabeled', 0)}\")\n",
        "        \n",
        "        if status == 'Completed':\n",
        "            output_location = response['LabelingJobOutput']['OutputDatasetS3Uri']\n",
        "            print(f\"✅ Job completed! Output: {output_location}\")\n",
        "            \n",
        "            # Log completion to MLFlow\n",
        "            mlflow.log_param(\"labeling_job_output\", output_location)\n",
        "            mlflow.log_param(\"labeling_job_status\", \"completed\")\n",
        "            \n",
        "        elif status == 'Failed':\n",
        "            failure_reason = response.get('FailureReason', 'Unknown')\n",
        "            print(f\"❌ Job failed: {failure_reason}\")\n",
        "            \n",
        "            # Log failure to MLFlow\n",
        "            mlflow.log_param(\"labeling_job_failure\", failure_reason)\n",
        "            mlflow.log_param(\"labeling_job_status\", \"failed\")\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error monitoring labeling job: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Monitor the job if it was created\n",
        "if 'job_arn' in locals() and job_arn:\n",
        "    job_status = monitor_labeling_job(labeling_job_config['job_name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. View MLFlow Experiments\n",
        "\n",
        "Let's view our MLFlow experiments and runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to list MLFlow experiments\n",
        "def list_mlflow_experiments():\n",
        "    \"\"\"List all MLFlow experiments\"\"\"\n",
        "    experiments = mlflow.search_experiments()\n",
        "    \n",
        "    if experiments:\n",
        "        print(\"MLFlow Experiments:\")\n",
        "        for exp in experiments:\n",
        "            print(f\"  - {exp.name} (ID: {exp.experiment_id})\")\n",
        "            \n",
        "            # Get runs for this experiment\n",
        "            runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])\n",
        "            print(f\"    Runs: {len(runs)}\")\n",
        "            \n",
        "            if len(runs) > 0:\n",
        "                print(\"    Recent runs:\")\n",
        "                for _, run in runs.head(3).iterrows():\n",
        "                    run_name = run.get('tags.mlflow.runName', 'Unnamed')\n",
        "                    status = run.get('status', 'Unknown')\n",
        "                    print(f\"      - {run_name} ({status})\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"No MLFlow experiments found\")\n",
        "\n",
        "# List experiments\n",
        "list_mlflow_experiments()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary and Next Steps\n",
        "\n",
        "In this notebook, we've explored the drone imagery dataset, prepared the structure for YOLOv11 training, created a Ground Truth labeling job, and tracked our work with MLFlow. Here's a summary of what we've accomplished:\n",
        "\n",
        "1. **Data Exploration with MLFlow**:\n",
        "   - Listed and displayed sample images from the S3 bucket\n",
        "   - Analyzed image characteristics (dimensions, aspect ratios, file sizes)\n",
        "   - Visualized image statistics\n",
        "   - Tracked all metrics and artifacts in MLFlow\n",
        "\n",
        "2. **Data Preparation**:\n",
        "   - Checked for existing labeled data\n",
        "   - Created YOLO dataset structure in S3\n",
        "   - Logged dataset information to MLFlow\n",
        "\n",
        "3. **Ground Truth Labeling**:\n",
        "   - Configured labeling job parameters\n",
        "   - Created input manifest for Ground Truth\n",
        "   - Set up Ground Truth labeling job for object detection\n",
        "   - Implemented job monitoring functionality\n",
        "\n",
        "4. **Experiment Tracking**:\n",
        "   - Used MLFlow to track all data exploration and labeling activities\n",
        "   - Saved visualizations as artifacts\n",
        "   - Logged parameters and metrics for reproducibility\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Monitor Labeling Job**: Check the progress of your Ground Truth labeling job\n",
        "2. **Complete Labeling**: Ensure all images are properly labeled\n",
        "3. **Convert Labels**: Convert Ground Truth output to YOLOv11 format\n",
        "4. **Organize Training Data**: Place labeled data in the YOLO structure we created\n",
        "5. **Proceed to Training**: Use the ML Engineer notebook for model training\n",
        "6. **Review MLFlow**: Check all experiments in the SageMaker Studio MLFlow UI\n",
        "\n",
        "### Ground Truth Integration Benefits\n",
        "\n",
        "- **Quality Control**: Professional annotation with quality checks\n",
        "- **Scalability**: Handle large datasets efficiently\n",
        "- **Cost Management**: Budget controls and cost estimation\n",
        "- **Workforce Management**: Private or public workforce options\n",
        "- **Integration**: Seamless integration with SageMaker training pipelines\n",
        "\n",
        "### MLFlow Integration Benefits\n",
        "\n",
        "- **Complete Tracking**: All data exploration and labeling activities are tracked\n",
        "- **Reproducibility**: Parameters and configurations are logged\n",
        "- **Collaboration**: Team members can view and compare experiments\n",
        "- **Artifact Management**: Visualizations and data summaries are stored\n",
        "- **Lineage**: Track data from exploration to training\n",
        "\n",
        "### Accessing Your Work\n",
        "\n",
        "- **MLFlow UI**: Go to \"Experiments and trials\" > \"MLflow\" in SageMaker Studio\n",
        "- **Ground Truth Console**: Monitor labeling jobs in the SageMaker console\n",
        "- **S3 Data**: All data and artifacts are stored in your S3 bucket\n",
        "\n",
        "For more detailed functionality, refer to the comprehensive notebooks in the `notebooks/data-labeling/` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Transform Classification Images to YOLOv11 Format with MLFlow Tracking\n",
        "\n",
        "Let's transform the images from the classification structure (raw-images/class_name/) to YOLOv11 format with proper train/val splits and label files, while tracking everything in MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Start new MLFlow run for image transformation\n",
        "# Continue with the current active MLFlow run\n",
        "# with mlflow.start_run(run_name=f\"image-transformation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\") - REMOVED\n",
        "\n",
        "# Function to discover classes from raw-images structure\n",
        "def discover_classes_from_s3(bucket, prefix=\"raw-images/\"):\n",
        "    \"\"\"Discover class names from S3 directory structure\"\"\"\n",
        "    response = s3_client.list_objects_v2(\n",
        "        Bucket=bucket,\n",
        "        Prefix=prefix,\n",
        "        Delimiter='/'\n",
        "    )\n",
        "    \n",
        "    classes = []\n",
        "    if 'CommonPrefixes' in response:\n",
        "        for obj in response['CommonPrefixes']:\n",
        "            class_prefix = obj['Prefix']\n",
        "            class_name = class_prefix.replace(prefix, '').rstrip('/')\n",
        "            if class_name:  # Skip empty class names\n",
        "                classes.append(class_name)\n",
        "    \n",
        "    return sorted(classes)\n",
        "\n",
        "# Function to get images for each class\n",
        "def get_images_by_class(bucket, prefix=\"raw-images/\"):\n",
        "    \"\"\"Get all images organized by class\"\"\"\n",
        "    classes = discover_classes_from_s3(bucket, prefix)\n",
        "    images_by_class = {}\n",
        "    \n",
        "    print(f\"Found {len(classes)} classes: {classes}\")\n",
        "    \n",
        "    for class_name in classes:\n",
        "        class_prefix = f\"{prefix}{class_name}/\"\n",
        "        objects = list_s3_objects(bucket, prefix=class_prefix)\n",
        "        images = filter_image_files(objects)\n",
        "        images_by_class[class_name] = images\n",
        "        print(f\"  {class_name}: {len(images)} images\")\n",
        "    \n",
        "    return images_by_class\n",
        "\n",
        "# Discover classes and images\n",
        "images_by_class = get_images_by_class(BUCKET_NAME)\n",
        "class_names = list(images_by_class.keys())\n",
        "total_images = sum(len(images) for images in images_by_class.values())\n",
        "\n",
        "print(f\"\\nTotal images found: {total_images}\")\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "# Log class discovery to MLFlow\n",
        "mlflow.log_param(\"num_classes\", len(class_names))\n",
        "mlflow.log_param(\"class_names\", \", \".join(class_names))\n",
        "mlflow.log_metric(\"total_raw_images\", total_images)\n",
        "\n",
        "# Log per-class image counts\n",
        "for class_name, images in images_by_class.items():\n",
        "    mlflow.log_metric(f\"images_{class_name}\", len(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue with the same MLFlow run for transformation\n",
        "# Continue with the current active MLFlow run\n",
        "# with mlflow.start_run(run_name=f\"yolo-transformation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\") - REMOVED\n",
        "\n",
        "# Function to create YOLO label file content\n",
        "def create_yolo_label(class_id, image_width, image_height, \n",
        "                     bbox_x=None, bbox_y=None, bbox_w=None, bbox_h=None):\n",
        "    \"\"\"Create YOLO format label content\n",
        "    \n",
        "    For classification images, we'll create a bounding box that covers the entire image\n",
        "    since we don't have specific object locations.\n",
        "    \n",
        "    Args:\n",
        "        class_id: Class ID (0-based)\n",
        "        image_width: Image width in pixels\n",
        "        image_height: Image height in pixels\n",
        "        bbox_x, bbox_y, bbox_w, bbox_h: Optional specific bounding box coordinates\n",
        "    \n",
        "    Returns:\n",
        "        YOLO format label string\n",
        "    \"\"\"\n",
        "    if bbox_x is None or bbox_y is None or bbox_w is None or bbox_h is None:\n",
        "        # Create a bounding box covering most of the image (80% centered)\n",
        "        # This assumes the object of interest is roughly centered in the image\n",
        "        x_center = 0.5  # Center of image (normalized)\n",
        "        y_center = 0.5  # Center of image (normalized)\n",
        "        width = 0.8     # 80% of image width (normalized)\n",
        "        height = 0.8    # 80% of image height (normalized)\n",
        "    else:\n",
        "        # Normalize the provided bounding box coordinates\n",
        "        x_center = (bbox_x + bbox_w/2) / image_width\n",
        "        y_center = (bbox_y + bbox_h/2) / image_height\n",
        "        width = bbox_w / image_width\n",
        "        height = bbox_h / image_height\n",
        "    \n",
        "    # Ensure values are within [0, 1] range\n",
        "    x_center = max(0, min(1, x_center))\n",
        "    y_center = max(0, min(1, y_center))\n",
        "    width = max(0, min(1, width))\n",
        "    height = max(0, min(1, height))\n",
        "    \n",
        "    return f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "\n",
        "# Function to split data into train/val sets\n",
        "def split_train_val(images_by_class, train_ratio=0.8, random_seed=42):\n",
        "    \"\"\"Split images into train and validation sets\"\"\"\n",
        "    random.seed(random_seed)\n",
        "    \n",
        "    train_data = {}\n",
        "    val_data = {}\n",
        "    \n",
        "    for class_name, images in images_by_class.items():\n",
        "        # Shuffle images\n",
        "        shuffled_images = images.copy()\n",
        "        random.shuffle(shuffled_images)\n",
        "        \n",
        "        # Split into train/val\n",
        "        split_idx = int(len(shuffled_images) * train_ratio)\n",
        "        train_data[class_name] = shuffled_images[:split_idx]\n",
        "        val_data[class_name] = shuffled_images[split_idx:]\n",
        "        \n",
        "        print(f\"{class_name}: {len(train_data[class_name])} train, {len(val_data[class_name])} val\")\n",
        "    \n",
        "    return train_data, val_data\n",
        "\n",
        "# Split data into train/val\n",
        "if images_by_class:\n",
        "    train_data, val_data = split_train_val(images_by_class)\n",
        "    \n",
        "    total_train = sum(len(images) for images in train_data.values())\n",
        "    total_val = sum(len(images) for images in val_data.values())\n",
        "    \n",
        "    print(f\"\\nTotal train images: {total_train}\")\n",
        "    print(f\"Total validation images: {total_val}\")\n",
        "    \n",
        "    # Log split information to MLFlow\n",
        "    mlflow.log_param(\"train_ratio\", 0.8)\n",
        "    mlflow.log_param(\"random_seed\", 42)\n",
        "    mlflow.log_metric(\"train_images_total\", total_train)\n",
        "    mlflow.log_metric(\"val_images_total\", total_val)\n",
        "    \n",
        "    # Log per-class split information\n",
        "    for class_name in class_names:\n",
        "        mlflow.log_metric(f\"train_images_{class_name}\", len(train_data[class_name]))\n",
        "        mlflow.log_metric(f\"val_images_{class_name}\", len(val_data[class_name]))\n",
        "    \n",
        "else:\n",
        "    print(\"No images found to split\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue transformation with MLFlow tracking\n",
        "# Continue with the current active MLFlow run\n",
        "# with mlflow.start_run(run_name=f\"yolo-dataset-creation-{datetime.now().strftime('%Y%m%d-%H%M%S')}\") - REMOVED\n",
        "\n",
        "# Function to transform and upload images to YOLO format\n",
        "def transform_to_yolo_format(bucket, train_data, val_data, class_names, \n",
        "                           dataset_name, progress_callback=None):\n",
        "    \"\"\"Transform classification images to YOLO format and upload to S3\"\"\"\n",
        "    \n",
        "    # Create class ID mapping\n",
        "    class_to_id = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "    \n",
        "    # Define S3 prefixes\n",
        "    base_prefix = f\"datasets/{dataset_name}/\"\n",
        "    train_images_prefix = f\"{base_prefix}train/images/\"\n",
        "    train_labels_prefix = f\"{base_prefix}train/labels/\"\n",
        "    val_images_prefix = f\"{base_prefix}val/images/\"\n",
        "    val_labels_prefix = f\"{base_prefix}val/labels/\"\n",
        "    \n",
        "    def process_split(data, images_prefix, labels_prefix, split_name):\n",
        "        \"\"\"Process a data split (train or val)\"\"\"\n",
        "        processed_count = 0\n",
        "        total_count = sum(len(images) for images in data.values())\n",
        "        \n",
        "        print(f\"\\nProcessing {split_name} split ({total_count} images)...\")\n",
        "        \n",
        "        for class_name, images in data.items():\n",
        "            class_id = class_to_id[class_name]\n",
        "            \n",
        "            for img_obj in images:\n",
        "                try:\n",
        "                    # Get original image key and filename\n",
        "                    original_key = img_obj['Key']\n",
        "                    filename = os.path.basename(original_key)\n",
        "                    name_without_ext = os.path.splitext(filename)[0]\n",
        "                    \n",
        "                    # Download image to get dimensions\n",
        "                    response = s3_client.get_object(Bucket=bucket, Key=original_key)\n",
        "                    img_data = response['Body'].read()\n",
        "                    img = Image.open(io.BytesIO(img_data))\n",
        "                    width, height = img.size\n",
        "                    \n",
        "                    # Copy image to new location\n",
        "                    new_image_key = f\"{images_prefix}{filename}\"\n",
        "                    s3_client.copy_object(\n",
        "                        Bucket=bucket,\n",
        "                        CopySource={'Bucket': bucket, 'Key': original_key},\n",
        "                        Key=new_image_key\n",
        "                    )\n",
        "                    \n",
        "                    # Create YOLO label\n",
        "                    label_content = create_yolo_label(class_id, width, height)\n",
        "                    \n",
        "                    # Upload label file\n",
        "                    label_key = f\"{labels_prefix}{name_without_ext}.txt\"\n",
        "                    s3_client.put_object(\n",
        "                        Bucket=bucket,\n",
        "                        Key=label_key,\n",
        "                        Body=label_content.encode('utf-8')\n",
        "                    )\n",
        "                    \n",
        "                    processed_count += 1\n",
        "                    \n",
        "                    # Progress callback\n",
        "                    if progress_callback and processed_count % 10 == 0:\n",
        "                        progress_callback(processed_count, total_count, split_name)\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {original_key}: {str(e)}\")\n",
        "        \n",
        "        print(f\"Completed {split_name} split: {processed_count}/{total_count} images processed\")\n",
        "        return processed_count\n",
        "    \n",
        "    # Progress callback function\n",
        "    def show_progress(current, total, split_name):\n",
        "        percentage = (current / total) * 100\n",
        "        print(f\"  {split_name}: {current}/{total} ({percentage:.1f}%) processed\")\n",
        "    \n",
        "    # Process train and validation splits\n",
        "    train_processed = process_split(train_data, train_images_prefix, train_labels_prefix, \"Train\")\n",
        "    val_processed = process_split(val_data, val_images_prefix, val_labels_prefix, \"Validation\")\n",
        "    \n",
        "    return {\n",
        "        'train_processed': train_processed,\n",
        "        'val_processed': val_processed,\n",
        "        'class_to_id': class_to_id,\n",
        "        'base_prefix': base_prefix\n",
        "    }\n",
        "\n",
        "# Transform images to YOLO format\n",
        "if 'train_data' in locals() and 'val_data' in locals() and class_names:\n",
        "    print(\"Starting transformation to YOLO format...\")\n",
        "    \n",
        "    # Create a new dataset with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    dataset_name = f\"yolov11_dataset_{timestamp}\"\n",
        "    \n",
        "    # Log dataset creation to MLFlow\n",
        "    mlflow.log_param(\"dataset_name\", dataset_name)\n",
        "    mlflow.log_param(\"transformation_timestamp\", timestamp)\n",
        "    mlflow.log_param(\"bbox_strategy\", \"80% centered\")\n",
        "    \n",
        "    transformation_result = transform_to_yolo_format(\n",
        "        BUCKET_NAME, train_data, val_data, class_names, dataset_name\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nTransformation completed!\")\n",
        "    print(f\"Dataset created at: s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
        "    print(f\"Train images processed: {transformation_result['train_processed']}\")\n",
        "    print(f\"Validation images processed: {transformation_result['val_processed']}\")\n",
        "    \n",
        "    # Log transformation results to MLFlow\n",
        "    mlflow.log_param(\"dataset_s3_path\", f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
        "    mlflow.log_metric(\"train_processed\", transformation_result['train_processed'])\n",
        "    mlflow.log_metric(\"val_processed\", transformation_result['val_processed'])\n",
        "    mlflow.log_metric(\"total_processed\", transformation_result['train_processed'] + transformation_result['val_processed'])\n",
        "    \n",
        "    # Log class mapping\n",
        "    for class_name, class_id in transformation_result['class_to_id'].items():\n",
        "        mlflow.log_param(f\"class_id_{class_name}\", class_id)\n",
        "    \n",
        "else:\n",
        "    print(\"No data available for transformation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Continue with data.yaml creation and verification\n",
        "# Continue with the current active MLFlow run\n",
        "# with mlflow.start_run(run_name=f\"dataset-finalization-{datetime.now().strftime('%Y%m%d-%H%M%S')}\") - REMOVED\n",
        "\n",
        "# Function to create data.yaml configuration file\n",
        "def create_data_yaml(bucket, dataset_name, class_names, class_to_id):\n",
        "    \"\"\"Create YOLO data.yaml configuration file\"\"\"\n",
        "    \n",
        "    # Create the YAML configuration\n",
        "    data_config = {\n",
        "        'path': f's3://{bucket}/datasets/{dataset_name}',\n",
        "        'train': 'train/images',\n",
        "        'val': 'val/images',\n",
        "        'nc': len(class_names),\n",
        "        'names': {class_to_id[name]: name for name in class_names}\n",
        "    }\n",
        "    \n",
        "    # Convert to YAML string\n",
        "    yaml_content = yaml.dump(data_config, default_flow_style=False, sort_keys=False)\n",
        "    \n",
        "    # Upload to S3\n",
        "    yaml_key = f\"datasets/{dataset_name}/data.yaml\"\n",
        "    s3_client.put_object(\n",
        "        Bucket=bucket,\n",
        "        Key=yaml_key,\n",
        "        Body=yaml_content.encode('utf-8')\n",
        "    )\n",
        "    \n",
        "    print(f\"Created data.yaml at: s3://{bucket}/{yaml_key}\")\n",
        "    print(\"\\nYAML content:\")\n",
        "    print(yaml_content)\n",
        "    \n",
        "    return yaml_key\n",
        "\n",
        "# Create data.yaml file\n",
        "if 'transformation_result' in locals():\n",
        "    yaml_key = create_data_yaml(\n",
        "        BUCKET_NAME, \n",
        "        dataset_name, \n",
        "        class_names, \n",
        "        transformation_result['class_to_id']\n",
        "    )\n",
        "    \n",
        "    # Log YAML creation to MLFlow\n",
        "    mlflow.log_param(\"data_yaml_path\", f\"s3://{BUCKET_NAME}/{yaml_key}\")\n",
        "    mlflow.log_artifact_from_s3(f\"s3://{BUCKET_NAME}/{yaml_key}\", \"data.yaml\")\n",
        "    \n",
        "else:\n",
        "    print(\"No transformation result available to create data.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final dataset summary with MLFlow logging\n",
        "# Continue with the current active MLFlow run\n",
        "# with mlflow.start_run(run_name=f\"dataset-summary-{datetime.now().strftime('%Y%m%d-%H%M%S')}\") - REMOVED\n",
        "\n",
        "# Display dataset summary\n",
        "if 'dataset_name' in locals() and 'transformation_result' in locals():\n",
        "    print(\"🎉 Dataset Transformation Complete!\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Dataset Name: {dataset_name}\")\n",
        "    print(f\"S3 Location: s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
        "    print(f\"Classes: {len(class_names)} ({', '.join(class_names)})\")\n",
        "    print(f\"Train Images: {transformation_result['train_processed']}\")\n",
        "    print(f\"Validation Images: {transformation_result['val_processed']}\")\n",
        "    print(f\"Total Images: {transformation_result['train_processed'] + transformation_result['val_processed']}\")\n",
        "    \n",
        "    print(\"\\n📁 Dataset Structure:\")\n",
        "    print(f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
        "    print(\"├── train/\")\n",
        "    print(\"│   ├── images/     # Training images\")\n",
        "    print(\"│   └── labels/     # Training labels (.txt files)\")\n",
        "    print(\"├── val/\")\n",
        "    print(\"│   ├── images/     # Validation images\")\n",
        "    print(\"│   └── labels/     # Validation labels (.txt files)\")\n",
        "    print(\"└── data.yaml       # Dataset configuration\")\n",
        "    \n",
        "    print(\"\\n🏷️  Class Mapping:\")\n",
        "    for class_name, class_id in transformation_result['class_to_id'].items():\n",
        "        print(f\"  {class_id}: {class_name}\")\n",
        "    \n",
        "    print(\"\\n📝 Label Format:\")\n",
        "    print(\"Each .txt file contains one line per object:\")\n",
        "    print(\"<class_id> <x_center> <y_center> <width> <height>\")\n",
        "    print(\"All coordinates are normalized (0.0 to 1.0)\")\n",
        "    \n",
        "    print(\"\\n🚀 Next Steps:\")\n",
        "    print(\"1. Use this dataset in the ML Engineer notebook for training\")\n",
        "    print(\"2. The dataset is ready for YOLOv11 training\")\n",
        "    print(f\"3. Reference the dataset using: s3://{BUCKET_NAME}/datasets/{dataset_name}/data.yaml\")\n",
        "    \n",
        "    # Save dataset info for later use\n",
        "    dataset_info = {\n",
        "        'dataset_name': dataset_name,\n",
        "        'bucket': BUCKET_NAME,\n",
        "        'base_path': f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/\",\n",
        "        'config_path': f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/data.yaml\",\n",
        "        'classes': class_names,\n",
        "        'class_to_id': transformation_result['class_to_id'],\n",
        "        'train_count': transformation_result['train_processed'],\n",
        "        'val_count': transformation_result['val_processed'],\n",
        "        'created_at': datetime.now().isoformat(),\n",
        "        'transformation_method': 'classification_to_yolo',\n",
        "        'bbox_strategy': '80% centered',\n",
        "        'mlflow_experiment': experiment_name\n",
        "    }\n",
        "    \n",
        "    # Save dataset info to S3 for reference\n",
        "    info_key = f\"datasets/{dataset_name}/dataset_info.json\"\n",
        "    s3_client.put_object(\n",
        "        Bucket=BUCKET_NAME,\n",
        "        Key=info_key,\n",
        "        Body=json.dumps(dataset_info, indent=2).encode('utf-8')\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n💾 Dataset info saved to: s3://{BUCKET_NAME}/{info_key}\")\n",
        "    \n",
        "    # Log final summary to MLFlow\n",
        "    mlflow.log_param(\"final_dataset_name\", dataset_name)\n",
        "    mlflow.log_param(\"final_dataset_path\", f\"s3://{BUCKET_NAME}/datasets/{dataset_name}/\")\n",
        "    mlflow.log_param(\"dataset_info_path\", f\"s3://{BUCKET_NAME}/{info_key}\")\n",
        "    mlflow.log_metric(\"final_train_count\", transformation_result['train_processed'])\n",
        "    mlflow.log_metric(\"final_val_count\", transformation_result['val_processed'])\n",
        "    mlflow.log_metric(\"final_total_count\", transformation_result['train_processed'] + transformation_result['val_processed'])\n",
        "    \n",
        "    # Log dataset info as artifact\n",
        "    with open('dataset_info.json', 'w') as f:\n",
        "        json.dump(dataset_info, f, indent=2)\n",
        "    mlflow.log_artifact('dataset_info.json')\n",
        "    \n",
        "    # Set tags for easy filtering\n",
        "    mlflow.set_tag(\"stage\", \"dataset_creation\")\n",
        "    mlflow.set_tag(\"dataset_ready\", \"true\")\n",
        "    mlflow.set_tag(\"dataset_name\", dataset_name)\n",
        "    mlflow.set_tag(\"transformation_type\", \"classification_to_yolo\")\n",
        "    \n",
        "else:\n",
        "    print(\"No dataset transformation was completed.\")\n",
        "    print(\"Please run the transformation cells above first.\")\n",
        "    \n",
        "    # Log failure\n",
        "    mlflow.log_param(\"transformation_status\", \"failed\")\n",
        "    mlflow.set_tag(\"dataset_ready\", \"false\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Updated Summary and Next Steps\n",
        "\n",
        "In this enhanced notebook, we've explored the drone imagery dataset, transformed it for YOLOv11 training, created Ground Truth labeling jobs, and tracked everything with MLFlow. Here's a comprehensive summary of what we've accomplished:\n",
        "\n",
        "### Completed Tasks:\n",
        "\n",
        "1. **Data Exploration with MLFlow**:\n",
        "   - Listed and displayed sample images from the S3 bucket\n",
        "   - Analyzed image characteristics (dimensions, aspect ratios, file sizes)\n",
        "   - Visualized image statistics\n",
        "   - Tracked all metrics and artifacts in MLFlow\n",
        "\n",
        "2. **Data Preparation**:\n",
        "   - Checked for existing labeled data\n",
        "   - Created YOLO dataset structure in S3\n",
        "   - Logged dataset information to MLFlow\n",
        "\n",
        "3. **Image Transformation** (NEW):\n",
        "   - Discovered classes from raw-images/ directory structure\n",
        "   - Split images into train/validation sets (80/20 split)\n",
        "   - Created YOLO format labels for each image\n",
        "   - Organized data in proper YOLOv11 structure\n",
        "   - Generated data.yaml configuration file\n",
        "   - Tracked entire transformation process in MLFlow\n",
        "\n",
        "4. **Ground Truth Labeling**:\n",
        "   - Configured labeling job parameters\n",
        "   - Created input manifest for Ground Truth\n",
        "   - Set up Ground Truth labeling job for object detection\n",
        "   - Implemented job monitoring functionality\n",
        "\n",
        "5. **Comprehensive MLFlow Tracking**:\n",
        "   - Used MLFlow to track all data exploration and labeling activities\n",
        "   - Saved visualizations as artifacts\n",
        "   - Logged parameters and metrics for reproducibility\n",
        "   - Tracked transformation process with detailed metrics\n",
        "   - Created experiment lineage from exploration to dataset creation\n",
        "\n",
        "### Key Features of the Enhanced Transformation:\n",
        "\n",
        "- **Complete MLFlow Integration**: Every step is tracked with parameters, metrics, and artifacts\n",
        "- **Automatic Class Discovery**: Discovers classes from S3 directory structure\n",
        "- **Smart Label Generation**: Creates bounding boxes covering 80% of each image (centered)\n",
        "- **Train/Val Split Tracking**: Logs split ratios and per-class distributions\n",
        "- **YOLO Format Compliance**: Generates proper YOLO format labels with normalized coordinates\n",
        "- **Dataset Verification**: Validates the created dataset structure\n",
        "- **Configuration Management**: Creates data.yaml and dataset_info.json with full metadata\n",
        "- **Artifact Management**: Saves all configuration files and summaries as MLFlow artifacts\n",
        "- **Experiment Tagging**: Uses tags for easy filtering and organization\n",
        "\n",
        "### MLFlow Experiment Organization:\n",
        "\n",
        "The notebook creates multiple MLFlow runs for different stages:\n",
        "- **Data Exploration**: Image analysis and statistics\n",
        "- **Image Analysis**: Detailed image characteristics\n",
        "- **Data Preparation**: Dataset structure preparation\n",
        "- **Labeling Job Creation**: Ground Truth job configuration\n",
        "- **Image Transformation**: Classification to YOLO conversion\n",
        "- **Dataset Creation**: YOLO dataset assembly\n",
        "- **Dataset Finalization**: Configuration file creation\n",
        "- **Dataset Summary**: Final validation and metadata\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Use the transformed dataset** in the ML Engineer notebook for YOLOv11 training\n",
        "2. **Monitor Ground Truth labeling jobs** if created\n",
        "3. **Review MLFlow experiments** in the SageMaker Studio MLFlow UI\n",
        "4. **Fine-tune the bounding boxes** if needed (currently uses 80% centered boxes)\n",
        "5. **Adjust train/val split ratio** if different proportions are needed\n",
        "6. **Proceed to model training** using the generated dataset\n",
        "\n",
        "### Dataset Ready for Training!\n",
        "\n",
        "Your dataset is now in the proper YOLOv11 format and ready for training. The ML Engineer can use the dataset path provided in the transformation summary to train YOLOv11 models.\n",
        "\n",
        "### Accessing Your Work\n",
        "\n",
        "- **MLFlow UI**: Go to \"Experiments and trials\" > \"MLflow\" in SageMaker Studio\n",
        "- **Ground Truth Console**: Monitor labeling jobs in the SageMaker console\n",
        "- **S3 Data**: All data and artifacts are stored in your S3 bucket\n",
        "- **Dataset Info**: Complete metadata available in dataset_info.json\n",
        "\n",
        "### Benefits Summary\n",
        "\n",
        "**Ground Truth Integration**:\n",
        "- Quality control with professional annotation\n",
        "- Scalable handling of large datasets\n",
        "- Cost management with budget controls\n",
        "- Flexible workforce options (private/public)\n",
        "- Seamless SageMaker integration\n",
        "\n",
        "**MLFlow Integration**:\n",
        "- Complete activity tracking and lineage\n",
        "- Full reproducibility with logged parameters\n",
        "- Team collaboration through shared experiments\n",
        "- Comprehensive artifact management\n",
        "- Organized experiment structure with tagging\n",
        "- Easy filtering and searching capabilities\n",
        "\n",
        "**Image Transformation**:\n",
        "- Automatic conversion from classification to object detection format\n",
        "- Proper YOLO format compliance\n",
        "- Intelligent bounding box generation\n",
        "- Complete dataset validation\n",
        "- Ready-to-use training configuration\n",
        "\n",
        "For more detailed functionality, refer to the comprehensive notebooks in the `notebooks/data-labeling/` directory."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
