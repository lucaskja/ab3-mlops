{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Engineer SageMaker Pipeline for YOLOv11 Object Detection\n",
        "\n",
        "This notebook implements a comprehensive SageMaker Pipeline for YOLOv11 object detection model training, evaluation, registration, and deployment. It replaces individual training job management with complete MLOps pipeline orchestration.\n",
        "\n",
        "## Pipeline Architecture Overview\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Data Validation â”‚â”€â”€â”€â–¶â”‚ Model Training  â”‚â”€â”€â”€â–¶â”‚ Model Evaluationâ”‚\n",
        "â”‚ ProcessingStep  â”‚    â”‚ TrainingStep    â”‚    â”‚ ProcessingStep  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                                        â”‚\n",
        "                                                        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Serverless      â”‚â—€â”€â”€â”€â”‚ Model Creation  â”‚â—€â”€â”€â”€â”‚ Performance     â”‚\n",
        "â”‚ Endpoint Deploy â”‚    â”‚ CreateModelStep â”‚    â”‚ Condition Check â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                â–²                       â”‚\n",
        "                                â”‚                       â–¼\n",
        "                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                       â”‚ Approval        â”‚â—€â”€â”€â”€â”‚ Model Registry  â”‚\n",
        "                       â”‚ Condition Check â”‚    â”‚ RegisterModel   â”‚\n",
        "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Complete Pipeline Orchestration**: End-to-end automation from data to deployment\n",
        "- **Conditional Logic**: Performance-based deployment decisions\n",
        "- **Model Registry Integration**: Centralized model management and versioning\n",
        "- **Serverless Endpoints**: Cost-effective inference with auto-scaling\n",
        "- **MLflow Integration**: Comprehensive experiment tracking and lineage\n",
        "- **Error Recovery**: Robust error handling and retry mechanisms\n",
        "- **Performance Monitoring**: Real-time pipeline execution monitoring\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- AWS account with appropriate permissions\n",
        "- AWS CLI configured with \"ab\" profile\n",
        "- SageMaker Studio access with ML Engineer role\n",
        "- Access to drone imagery dataset in S3: `lucaskle-ab3-project-pv`\n",
        "- YOLOv11 training and inference containers in ECR\n",
        "- SageMaker managed MLFlow tracking server\n",
        "\n",
        "Let's start by setting up our environment and importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for SageMaker Pipelines\n",
        "!pip install --quiet sagemaker>=2.190.0 boto3>=1.28.0 pandas>=2.0.0 matplotlib>=3.7.0 \\\n",
        "    numpy>=1.24.0 PyYAML>=6.0 mlflow>=3.0.0 requests-auth-aws-sigv4>=0.7 ipywidgets>=8.0.0\n",
        "\n",
        "print(\"âœ… Required packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import display, HTML, Markdown\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "\n",
        "# SageMaker Pipeline imports\n",
        "from sagemaker.workflow.pipeline import Pipeline\n",
        "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep\n",
        "from sagemaker.workflow.step_collections import RegisterModel\n",
        "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo, ConditionEquals\n",
        "from sagemaker.workflow.condition_step import ConditionStep\n",
        "from sagemaker.workflow.functions import JsonGet\n",
        "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger\n",
        "from sagemaker.workflow.pipeline_context import PipelineSession\n",
        "from sagemaker.workflow.properties import PropertyFile\n",
        "from sagemaker.processing import ScriptProcessor\n",
        "\n",
        "# SageMaker components\n",
        "from sagemaker.estimator import Estimator\n",
        "from sagemaker.processing import ProcessingInput, ProcessingOutput, Processor\n",
        "from sagemaker.inputs import TrainingInput\n",
        "from sagemaker.model import Model\n",
        "from sagemaker.predictor import Predictor\n",
        "from sagemaker.serverless import ServerlessInferenceConfig\n",
        "\n",
        "# MLflow imports\n",
        "import mlflow\n",
        "import mlflow.sagemaker\n",
        "\n",
        "print(\"âœ… All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up AWS session with \"ab\" profile\n",
        "session = boto3.Session(profile_name='ab')\n",
        "sagemaker_session = sagemaker.Session(boto_session=session)\n",
        "pipeline_session = PipelineSession(boto_session=session)\n",
        "sagemaker_client = session.client('sagemaker')\n",
        "region = session.region_name\n",
        "account_id = session.client('sts').get_caller_identity()['Account']\n",
        "\n",
        "# Configuration\n",
        "BUCKET_NAME = 'lucaskle-ab3-project-pv-2'\n",
        "ROLE_ARN = sagemaker_session.get_caller_identity_arn()\n",
        "MODEL_PACKAGE_GROUP_NAME = \"yolov11-drone-detection-models\"\n",
        "PIPELINE_NAME = \"yolov11-training-pipeline\"\n",
        "\n",
        "# Set up MLFlow tracking with SageMaker managed server\n",
        "try:\n",
        "    tracking_server_arn = \"arn:aws:sagemaker:us-east-1:192771711075:mlflow-tracking-server/sagemaker-core-setup-mlflow-server\"\n",
        "    mlflow.set_tracking_uri(tracking_server_arn)\n",
        "    \n",
        "    # Create or set experiment\n",
        "    experiment_name = \"yolov11-pipeline-experiments\"\n",
        "    try:\n",
        "        mlflow.create_experiment(experiment_name)\n",
        "        print(f\"Created new MLflow experiment: {experiment_name}\")\n",
        "    except Exception:\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "        print(f\"Using existing MLflow experiment: {experiment_name}\")\n",
        "    \n",
        "    print(f\"âœ… Connected to SageMaker managed MLflow server\")\n",
        "    print(f\"Tracking Server ARN: {tracking_server_arn}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Could not connect to SageMaker managed MLflow: {e}\")\n",
        "    print(\"Using basic MLflow setup as fallback\")\n",
        "    experiment_name = \"yolov11-pipeline-experiments\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "# Display configuration\n",
        "print(f\"\\nğŸ“‹ Configuration Summary:\")\n",
        "print(f\"   Data Bucket: {BUCKET_NAME}\")\n",
        "print(f\"   Region: {region}\")\n",
        "print(f\"   Account ID: {account_id}\")\n",
        "print(f\"   Role ARN: {ROLE_ARN}\")\n",
        "print(f\"   Model Package Group: {MODEL_PACKAGE_GROUP_NAME}\")\n",
        "print(f\"   Pipeline Name: {PIPELINE_NAME}\")\n",
        "print(f\"   MLflow Experiment: {experiment_name}\")\n",
        "\n",
        "print(\"\\nâœ… Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Discovery and Validation\n",
        "\n",
        "Before creating our pipeline, let's discover and validate available YOLOv11 datasets. The pipeline will use parameterized dataset paths for flexibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discover_yolo_datasets(bucket, prefix=\"datasets/\"):\n",
        "    \"\"\"Discover available YOLOv11 datasets with comprehensive validation\"\"\"\n",
        "    s3_client = session.client('s3')\n",
        "    \n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(\n",
        "            Bucket=bucket,\n",
        "            Prefix=prefix,\n",
        "            Delimiter='/'\n",
        "        )\n",
        "        \n",
        "        datasets = []\n",
        "        if 'CommonPrefixes' in response:\n",
        "            for obj in response['CommonPrefixes']:\n",
        "                dataset_prefix = obj['Prefix']\n",
        "                dataset_name = dataset_prefix.split('/')[-2]\n",
        "                \n",
        "                # Validate dataset structure\n",
        "                validation_result = validate_yolo_dataset_structure(bucket, dataset_prefix)\n",
        "                \n",
        "                datasets.append({\n",
        "                    'name': dataset_name,\n",
        "                    'prefix': dataset_prefix,\n",
        "                    'full_path': f's3://{bucket}/{dataset_prefix}',\n",
        "                    'valid': validation_result['valid'],\n",
        "                    'validation_details': validation_result\n",
        "                })\n",
        "        \n",
        "        return datasets\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error discovering datasets: {e}\")\n",
        "        return []\n",
        "\n",
        "def validate_yolo_dataset_structure(bucket, dataset_prefix):\n",
        "    \"\"\"Fixed validation with proper file filtering\"\"\"\n",
        "    s3_client = session.client('s3')\n",
        "    \n",
        "    # Required structure for YOLOv11 pipeline\n",
        "    required_structure = {\n",
        "        'train/images/': False,\n",
        "        'train/labels/': False,\n",
        "        'val/images/': False,\n",
        "        'val/labels/': False,\n",
        "        'data.yaml': False,\n",
        "        'dataset_info.json': False\n",
        "    }\n",
        "    \n",
        "    validation_details = {\n",
        "        'valid': False,\n",
        "        'missing_components': [],\n",
        "        'found_components': [],\n",
        "        'train_image_count': 0,\n",
        "        'val_image_count': 0,\n",
        "        'train_label_count': 0,\n",
        "        'val_label_count': 0,\n",
        "        'pipeline_ready': False\n",
        "    }\n",
        "    \n",
        "    # Valid file extensions\n",
        "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
        "    label_extensions = {'.txt'}\n",
        "    \n",
        "    try:\n",
        "        print(f\"ğŸ” Validating dataset: {dataset_prefix}\")\n",
        "        \n",
        "        for required_path in required_structure.keys():\n",
        "            full_path = dataset_prefix + required_path\n",
        "            \n",
        "            if required_path.endswith('/'):\n",
        "                # Check directories with proper file filtering\n",
        "                paginator = s3_client.get_paginator('list_objects_v2')\n",
        "                page_iterator = paginator.paginate(\n",
        "                    Bucket=bucket,\n",
        "                    Prefix=full_path,\n",
        "                    PaginationConfig={'PageSize': 1000}\n",
        "                )\n",
        "                \n",
        "                file_count = 0\n",
        "                has_files = False\n",
        "                \n",
        "                for page in page_iterator:\n",
        "                    if 'Contents' in page:\n",
        "                        has_files = True\n",
        "                        \n",
        "                        # FIXED: Proper file filtering\n",
        "                        valid_files = []\n",
        "                        for obj in page['Contents']:\n",
        "                            key = obj['Key']\n",
        "                            \n",
        "                            # Skip directory markers\n",
        "                            if key.endswith('/'):\n",
        "                                continue\n",
        "                            \n",
        "                            # Skip the directory itself\n",
        "                            if key == full_path:\n",
        "                                continue\n",
        "                            \n",
        "                            # Get file extension\n",
        "                            file_ext = os.path.splitext(key.lower())[1]\n",
        "                            \n",
        "                            # Filter by appropriate extensions\n",
        "                            if 'images/' in required_path and file_ext in image_extensions:\n",
        "                                valid_files.append(obj)\n",
        "                            elif 'labels/' in required_path and file_ext in label_extensions:\n",
        "                                valid_files.append(obj)\n",
        "                        \n",
        "                        file_count += len(valid_files)\n",
        "                \n",
        "                if has_files and file_count > 0:\n",
        "                    required_structure[required_path] = True\n",
        "                    validation_details['found_components'].append(required_path)\n",
        "                    \n",
        "                    # Store counts with proper filtering\n",
        "                    if 'train/images/' in required_path:\n",
        "                        validation_details['train_image_count'] = file_count\n",
        "                    elif 'val/images/' in required_path:\n",
        "                        validation_details['val_image_count'] = file_count\n",
        "                    elif 'train/labels/' in required_path:\n",
        "                        validation_details['train_label_count'] = file_count\n",
        "                    elif 'val/labels/' in required_path:\n",
        "                        validation_details['val_label_count'] = file_count\n",
        "                    \n",
        "                    print(f\"   âœ… {required_path}: {file_count:,} files\")\n",
        "                else:\n",
        "                    validation_details['missing_components'].append(required_path)\n",
        "                    print(f\"   âŒ {required_path}: Not found or empty\")\n",
        "            else:\n",
        "                # Check individual files (data.yaml, dataset_info.json)\n",
        "                try:\n",
        "                    s3_client.head_object(Bucket=bucket, Key=full_path)\n",
        "                    required_structure[required_path] = True\n",
        "                    validation_details['found_components'].append(required_path)\n",
        "                    print(f\"   âœ… {required_path}: Found\")\n",
        "                except:\n",
        "                    validation_details['missing_components'].append(required_path)\n",
        "                    print(f\"   âŒ {required_path}: Not found\")\n",
        "        \n",
        "        # Dataset is valid if all required components are found\n",
        "        validation_details['valid'] = all(required_structure.values())\n",
        "        \n",
        "        # FIXED: Pipeline readiness checks with correct counts\n",
        "        if validation_details['valid']:\n",
        "            min_images = 10\n",
        "            train_images = validation_details['train_image_count']\n",
        "            val_images = validation_details['val_image_count']\n",
        "            train_labels = validation_details['train_label_count']\n",
        "            val_labels = validation_details['val_label_count']\n",
        "            \n",
        "            # Check for perfect image/label matching\n",
        "            pipeline_ready = (\n",
        "                train_images >= min_images and\n",
        "                val_images >= max(1, min_images // 4) and\n",
        "                train_images == train_labels and  # Perfect match required\n",
        "                val_images == val_labels          # Perfect match required\n",
        "            )\n",
        "            \n",
        "            validation_details['pipeline_ready'] = pipeline_ready\n",
        "            \n",
        "            if pipeline_ready:\n",
        "                print(f\"   ğŸš€ Dataset is PIPELINE READY\")\n",
        "            else:\n",
        "                print(f\"   âš ï¸  Dataset valid but has image/label count mismatches:\")\n",
        "                if train_images != train_labels:\n",
        "                    print(f\"      Train: {train_images} images â‰  {train_labels} labels\")\n",
        "                if val_images != val_labels:\n",
        "                    print(f\"      Val: {val_images} images â‰  {val_labels} labels\")\n",
        "        \n",
        "        # Summary with corrected counts\n",
        "        total_images = validation_details['train_image_count'] + validation_details['val_image_count']\n",
        "        total_labels = validation_details['train_label_count'] + validation_details['val_label_count']\n",
        "        \n",
        "        print(f\"   ğŸ“Š Summary: {total_images:,} images, {total_labels:,} labels\")\n",
        "        print(f\"   Status: {'ğŸš€ PIPELINE READY' if validation_details.get('pipeline_ready') else 'âœ… VALID' if validation_details['valid'] else 'âŒ INVALID'}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error during validation: {e}\")\n",
        "    \n",
        "    return validation_details\n",
        "\n",
        "# Fixed main discovery and validation loop\n",
        "print(\"ğŸ” Discovering YOLOv11 datasets for pipeline...\")\n",
        "available_datasets = discover_yolo_datasets(BUCKET_NAME)\n",
        "\n",
        "if available_datasets:\n",
        "    print(f\"\\nğŸ“Š Found {len(available_datasets)} datasets:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    pipeline_ready_datasets = []\n",
        "    for i, dataset in enumerate(available_datasets):\n",
        "        # Get validation details once at the beginning\n",
        "        details = dataset['validation_details']  # FIXED: Define details here\n",
        "        \n",
        "        status_icon = \"ğŸš€\" if details.get('pipeline_ready', False) else \"âœ…\" if dataset['valid'] else \"âŒ\"\n",
        "        print(f\"{i+1}. {status_icon} {dataset['name']}\")\n",
        "        print(f\"   Path: {dataset['full_path']}\")\n",
        "        \n",
        "        if dataset['valid']:\n",
        "            print(f\"   ğŸ“ˆ Training: {details['train_image_count']:,} images, {details['train_label_count']:,} labels\")\n",
        "            print(f\"   ğŸ“Š Validation: {details['val_image_count']:,} images, {details['val_label_count']:,} labels\")\n",
        "            \n",
        "            if details.get('pipeline_ready', False):\n",
        "                pipeline_ready_datasets.append(dataset)\n",
        "        else:\n",
        "            # FIXED: Now details is defined and accessible here\n",
        "            print(f\"   âš ï¸  Missing: {', '.join(details['missing_components'])}\")\n",
        "        \n",
        "        print(\"-\" * 80)\n",
        "    \n",
        "    print(f\"\\nğŸš€ {len(pipeline_ready_datasets)} dataset(s) ready for pipeline execution\")\n",
        "    \n",
        "    # Store for pipeline configuration\n",
        "    validated_datasets = available_datasets\n",
        "    pipeline_ready_datasets_global = pipeline_ready_datasets\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ No datasets found. Please prepare datasets using the Data Scientist notebook first.\")\n",
        "    validated_datasets = []\n",
        "    pipeline_ready_datasets_global = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Pipeline Parameters Configuration\n",
        "\n",
        "Define parameterized pipeline configuration for flexibility and reusability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative 1: Use simple HTML display instead of complex widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def create_simple_config_interface():\n",
        "    \"\"\"Create a simple HTML-based configuration interface\"\"\"\n",
        "    \n",
        "    if not pipeline_ready_datasets_global:\n",
        "        display(HTML(\"<p style='color: red;'>âŒ No pipeline-ready datasets available</p>\"))\n",
        "        return None\n",
        "    \n",
        "    dataset = pipeline_ready_datasets_global[0]\n",
        "    \n",
        "    config_html = f\"\"\"\n",
        "    <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; margin: 10px 0;\">\n",
        "        <h3>ğŸ›ï¸ Pipeline Configuration</h3>\n",
        "        \n",
        "        <h4>ğŸ“Š Selected Dataset:</h4>\n",
        "        <ul>\n",
        "            <li><strong>Name:</strong> {dataset['name']}</li>\n",
        "            <li><strong>Training Images:</strong> {dataset['validation_details']['train_image_count']:,}</li>\n",
        "            <li><strong>Validation Images:</strong> {dataset['validation_details']['val_image_count']:,}</li>\n",
        "            <li><strong>Path:</strong> {dataset['full_path']}</li>\n",
        "        </ul>\n",
        "        \n",
        "        <h4>ğŸ¤– Model Configuration:</h4>\n",
        "        <ul>\n",
        "            <li><strong>Model Variant:</strong> yolov11n (nano - fastest)</li>\n",
        "            <li><strong>Image Size:</strong> 640px</li>\n",
        "        </ul>\n",
        "        \n",
        "        <h4>ğŸ‹ï¸ Training Configuration:</h4>\n",
        "        <ul>\n",
        "            <li><strong>Batch Size:</strong> 16</li>\n",
        "            <li><strong>Epochs:</strong> 50</li>\n",
        "            <li><strong>Learning Rate:</strong> 0.001</li>\n",
        "        </ul>\n",
        "        \n",
        "        <h4>ğŸ—ï¸ Infrastructure:</h4>\n",
        "        <ul>\n",
        "            <li><strong>Instance Type:</strong> ml.g4dn.xlarge</li>\n",
        "            <li><strong>Use Spot Instances:</strong> Yes (cost savings)</li>\n",
        "        </ul>\n",
        "        \n",
        "        <h4>ğŸš€ Deployment:</h4>\n",
        "        <ul>\n",
        "            <li><strong>Performance Threshold:</strong> 0.3 mAP@0.5</li>\n",
        "            <li><strong>Auto Deploy:</strong> No (manual approval)</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    display(HTML(config_html))\n",
        "    \n",
        "    # Return configuration dictionary\n",
        "    return {\n",
        "        'dataset_name': dataset['name'],\n",
        "        'dataset_prefix': dataset['prefix'],\n",
        "        'dataset_path': dataset['full_path'],\n",
        "        'model_variant': 'yolov11n',\n",
        "        'image_size': 640,\n",
        "        'batch_size': 16,\n",
        "        'epochs': 50,\n",
        "        'learning_rate': 0.001,\n",
        "        'instance_type': 'ml.g4dn.xlarge',\n",
        "        'use_spot': True,\n",
        "        'performance_threshold': 0.3,\n",
        "        'auto_deploy': False,\n",
        "        'endpoint_name': f\"yolov11-endpoint-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
        "    }\n",
        "\n",
        "# Use this instead of the widget-based interface\n",
        "print(\"ğŸ”§ Using HTML-based configuration (widget compatibility fix)\")\n",
        "current_config = create_simple_config_interface()\n",
        "\n",
        "if current_config:\n",
        "    print(\"âœ… Configuration ready! You can proceed to create pipeline parameters.\")\n",
        "else:\n",
        "    print(\"âŒ Configuration failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Pipeline Parameters Definition\n",
        "\n",
        "Define SageMaker Pipeline parameters for dynamic configuration and reusability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_pipeline_parameters():\n",
        "    \"\"\"Create SageMaker Pipeline parameters with consistent naming\"\"\"\n",
        "    \n",
        "    # Extract current configuration\n",
        "    config = extract_config_values()\n",
        "    if not config:\n",
        "        print(\"âŒ No configuration available\")\n",
        "        return None, None\n",
        "    \n",
        "    # Define pipeline parameters with current values as defaults\n",
        "    parameters = [\n",
        "        # Dataset parameters\n",
        "        ParameterString(\n",
        "            name=\"DatasetPath\",\n",
        "            default_value=config['dataset_path']\n",
        "        ),\n",
        "        ParameterString(\n",
        "            name=\"DatasetName\", \n",
        "            default_value=config['dataset_name']\n",
        "        ),\n",
        "        \n",
        "        # Model parameters\n",
        "        ParameterString(\n",
        "            name=\"ModelVariant\",\n",
        "            default_value=config['model_variant']\n",
        "        ),\n",
        "        ParameterInteger(\n",
        "            name=\"ImageSize\",\n",
        "            default_value=config['image_size']\n",
        "        ),\n",
        "        \n",
        "        # Training parameters\n",
        "        ParameterInteger(\n",
        "            name=\"BatchSize\",\n",
        "            default_value=config['batch_size']\n",
        "        ),\n",
        "        ParameterInteger(\n",
        "            name=\"Epochs\",\n",
        "            default_value=config['epochs']\n",
        "        ),\n",
        "        ParameterFloat(\n",
        "            name=\"LearningRate\",\n",
        "            default_value=config['learning_rate']\n",
        "        ),\n",
        "        \n",
        "        # Infrastructure parameters\n",
        "        ParameterString(\n",
        "            name=\"InstanceType\",\n",
        "            default_value=config['instance_type']\n",
        "        ),\n",
        "        ParameterString(\n",
        "            name=\"UseSpot\",\n",
        "            default_value=\"true\" if config['use_spot'] else \"false\"\n",
        "        ),\n",
        "        \n",
        "        # Performance threshold\n",
        "        ParameterFloat(\n",
        "            name=\"PerformanceThreshold\",\n",
        "            default_value=config['performance_threshold']\n",
        "        ),\n",
        "        \n",
        "        # Deployment parameters\n",
        "        ParameterString(\n",
        "            name=\"EndpointName\",\n",
        "            default_value=config['endpoint_name']\n",
        "        ),\n",
        "        \n",
        "        # Output paths\n",
        "        ParameterString(\n",
        "            name=\"ModelOutputPath\",\n",
        "            default_value=f\"s3://{BUCKET_NAME}/pipeline-artifacts/models\"\n",
        "        ),\n",
        "        ParameterString(\n",
        "            name=\"EvaluationOutputPath\",\n",
        "            default_value=f\"s3://{BUCKET_NAME}/pipeline-artifacts/evaluation\"\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    return parameters, config\n",
        "\n",
        "# Create pipeline parameters\n",
        "pipeline_parameters, current_config = create_pipeline_parameters()\n",
        "\n",
        "if pipeline_parameters:\n",
        "    print(\"âœ… Pipeline parameters created successfully!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to create pipeline parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Pipeline Step Definitions\n",
        "\n",
        "Define each step of the SageMaker Pipeline with proper dependencies and data flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_data_validation_step(param_dict):\n",
        "    \"\"\"Create data validation processing step - CORRECTED parameter access\"\"\"\n",
        "    \n",
        "    validation_processor = ScriptProcessor(\n",
        "        command=[\"python3\"],\n",
        "        image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-preprocessing:latest\",\n",
        "        role=ROLE_ARN,\n",
        "        instance_count=1,\n",
        "        instance_type=\"ml.m5.large\",\n",
        "        sagemaker_session=pipeline_session\n",
        "    )\n",
        "    \n",
        "    validation_step = ProcessingStep(\n",
        "        name=\"DataValidation\",\n",
        "        processor=validation_processor,\n",
        "        inputs=[\n",
        "            ProcessingInput(\n",
        "                source=param_dict['DatasetPath'],  # CORRECT: Use parameter name\n",
        "                destination=\"/opt/ml/processing/input\",\n",
        "                input_name=\"dataset\"\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            ProcessingOutput(\n",
        "                output_name=\"validation_report\",\n",
        "                source=\"/opt/ml/processing/output\",\n",
        "                destination=f\"s3://{BUCKET_NAME}/pipeline-artifacts/validation\"\n",
        "            )\n",
        "        ],\n",
        "        code=\"scripts/validate_dataset.py\",\n",
        "        job_arguments=[\n",
        "            \"--dataset-name\", param_dict['DatasetName'],  # CORRECT: Use parameter name\n",
        "            \"--model-variant\", param_dict['ModelVariant']  # CORRECT: Use parameter name\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return validation_step\n",
        "\n",
        "def create_training_step(param_dict, validation_step):\n",
        "    \"\"\"Create YOLOv11 training step - CORRECTED parameter access\"\"\"\n",
        "    \n",
        "    estimator = Estimator(\n",
        "        image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-training:latest\",\n",
        "        role=ROLE_ARN,\n",
        "        instance_count=1,\n",
        "        instance_type=param_dict['InstanceType'],  # CORRECT: Use parameter name\n",
        "        output_path=param_dict['ModelOutputPath'],  # CORRECT: Use parameter name\n",
        "        sagemaker_session=pipeline_session,\n",
        "        use_spot_instances=(param_dict['UseSpot'] == \"true\"),  # CORRECT: Use parameter name\n",
        "        max_wait=3600 if param_dict['UseSpot'] == \"true\" else None,\n",
        "        max_run=3600,\n",
        "        hyperparameters={\n",
        "            \"model_variant\": param_dict['ModelVariant'],    # CORRECT: Use parameter name\n",
        "            \"image_size\": param_dict['ImageSize'],          # CORRECT: Use parameter name\n",
        "            \"batch_size\": param_dict['BatchSize'],          # CORRECT: Use parameter name\n",
        "            \"epochs\": param_dict['Epochs'],                 # CORRECT: Use parameter name\n",
        "            \"learning_rate\": param_dict['LearningRate'],    # CORRECT: Use parameter name\n",
        "            \"dataset_name\": param_dict['DatasetName']       # CORRECT: Use parameter name\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    training_step = TrainingStep(\n",
        "        name=\"YOLOv11Training\",\n",
        "        estimator=estimator,\n",
        "        inputs={\n",
        "            \"training\": TrainingInput(\n",
        "                s3_data=param_dict['DatasetPath'],  # CORRECT: Use parameter name\n",
        "                content_type=\"application/x-image\"\n",
        "            )\n",
        "        },\n",
        "        depends_on=[validation_step.name]\n",
        "    )\n",
        "    \n",
        "    return training_step\n",
        "\n",
        "def create_evaluation_step(param_dict, training_step):\n",
        "    \"\"\"Create model evaluation processing step - FULLY CORRECTED\"\"\"\n",
        "    \n",
        "    # Use ScriptProcessor for proper script execution\n",
        "    evaluation_processor = ScriptProcessor(\n",
        "        command=[\"python3\"],  # This handles script execution properly\n",
        "        image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-evaluation:latest\",\n",
        "        role=ROLE_ARN,\n",
        "        instance_count=1,\n",
        "        instance_type=\"ml.g4dn.xlarge\",  # GPU for faster evaluation\n",
        "        sagemaker_session=pipeline_session\n",
        "    )\n",
        "    \n",
        "    # Property file for evaluation metrics\n",
        "    evaluation_report = PropertyFile(\n",
        "        name=\"EvaluationReport\",\n",
        "        output_name=\"evaluation\",\n",
        "        path=\"evaluation.json\"\n",
        "    )\n",
        "    \n",
        "    # Evaluation step - CORRECTED parameter access\n",
        "    evaluation_step = ProcessingStep(\n",
        "        name=\"ModelEvaluation\",\n",
        "        processor=evaluation_processor,\n",
        "        inputs=[\n",
        "            ProcessingInput(\n",
        "                source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
        "                destination=\"/opt/ml/processing/model\",\n",
        "                input_name=\"model\"\n",
        "            ),\n",
        "            ProcessingInput(\n",
        "                # CORRECTED: Use param_dict with correct parameter name\n",
        "                source=param_dict['DatasetPath'],  # Was: parameters['dataset_path']\n",
        "                destination=\"/opt/ml/processing/test\",\n",
        "                input_name=\"test_data\"\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            ProcessingOutput(\n",
        "                output_name=\"evaluation\",\n",
        "                source=\"/opt/ml/processing/evaluation\",\n",
        "                # CORRECTED: Use param_dict with correct parameter name\n",
        "                destination=param_dict['EvaluationOutputPath']  # Was: parameters['evaluation_output_path']\n",
        "            )\n",
        "        ],\n",
        "        property_files=[evaluation_report],\n",
        "        code=\"scripts/evaluate_model.py\",  # ScriptProcessor handles this correctly\n",
        "        job_arguments=[\n",
        "            # CORRECTED: Use param_dict with correct parameter names\n",
        "            \"--model-variant\", param_dict['ModelVariant'],  # Was: parameters['model_variant']\n",
        "            \"--dataset-name\", param_dict['DatasetName']     # Was: parameters['dataset_name']\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return evaluation_step, evaluation_report\n",
        "\n",
        "def create_performance_condition(evaluation_step, evaluation_report, param_dict):\n",
        "    \"\"\"Create condition step for performance threshold checking - CORRECTED\"\"\"\n",
        "    \n",
        "    # Condition: mAP@0.5 >= threshold\n",
        "    performance_condition = ConditionGreaterThanOrEqualTo(\n",
        "        left=JsonGet(\n",
        "            step_name=evaluation_step.name,\n",
        "            property_file=evaluation_report,\n",
        "            json_path=\"metrics.mAP_50\"\n",
        "        ),\n",
        "        # CORRECTED: Use param_dict with correct parameter name\n",
        "        right=param_dict['PerformanceThreshold']  # Was: parameters['performance_threshold']\n",
        "    )\n",
        "    \n",
        "    return performance_condition\n",
        "\n",
        "def create_model_registration_step(param_dict, training_step, evaluation_step, evaluation_report):\n",
        "    \"\"\"Create model registration step for Model Registry - CORRECTED\"\"\"\n",
        "    \n",
        "    # Model registration\n",
        "    register_model_step = RegisterModel(\n",
        "        name=\"RegisterYOLOv11Model\",\n",
        "        estimator=training_step.estimator,\n",
        "        model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
        "        content_types=[\"application/json\", \"image/jpeg\", \"image/png\"],\n",
        "        response_types=[\"application/json\"],\n",
        "        inference_instances=[\"ml.t2.medium\", \"ml.m5.large\", \"ml.g4dn.xlarge\"],\n",
        "        transform_instances=[\"ml.m5.large\", \"ml.g4dn.xlarge\"],\n",
        "        model_package_group_name=MODEL_PACKAGE_GROUP_NAME,\n",
        "        approval_status=\"PendingManualApproval\",\n",
        "        model_metrics=[\n",
        "            {\n",
        "                \"Name\": \"mAP@0.5\",\n",
        "                \"Value\": JsonGet(\n",
        "                    step_name=evaluation_step.name,\n",
        "                    property_file=evaluation_report,  # Use the PropertyFile object directly\n",
        "                    json_path=\"metrics.mAP_50\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"Name\": \"mAP@0.5:0.95\",\n",
        "                \"Value\": JsonGet(\n",
        "                    step_name=evaluation_step.name,\n",
        "                    property_file=evaluation_report,  # Use the PropertyFile object directly\n",
        "                    json_path=\"metrics.mAP_50_95\"\n",
        "                )\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return register_model_step\n",
        "\n",
        "def create_model_creation_step(param_dict, register_model_step):\n",
        "    \"\"\"Create model creation step for deployment - CORRECTED\"\"\"\n",
        "    \n",
        "    # Model creation for deployment using model package\n",
        "    model = Model(\n",
        "        image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-inference:latest\",\n",
        "        # Use model package ARN for registered models\n",
        "        model_data=register_model_step.properties.ModelPackageArn,\n",
        "        role=ROLE_ARN,\n",
        "        sagemaker_session=pipeline_session\n",
        "    )\n",
        "    \n",
        "    create_model_step = CreateModelStep(\n",
        "        name=\"CreateYOLOv11Model\",\n",
        "        model=model\n",
        "    )\n",
        "    \n",
        "    return create_model_step\n",
        "\n",
        "def create_serverless_endpoint_step(param_dict, create_model_step):\n",
        "    \"\"\"Create serverless endpoint deployment step - CORRECTED\"\"\"\n",
        "    \n",
        "    # Serverless inference configuration\n",
        "    serverless_config = ServerlessInferenceConfig(\n",
        "        memory_size_in_mb=4096,\n",
        "        max_concurrency=20,\n",
        "        provisioned_concurrency=1  # Keep warm for faster response\n",
        "    )\n",
        "    \n",
        "    # Endpoint deployment processor\n",
        "    deployment_processor = Processor(\n",
        "        image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-deployment:latest\",\n",
        "        role=ROLE_ARN,\n",
        "        instance_count=1,\n",
        "        instance_type=\"ml.t3.medium\",\n",
        "        sagemaker_session=pipeline_session\n",
        "    )\n",
        "    \n",
        "    # Deployment step\n",
        "    deployment_step = ProcessingStep(\n",
        "        name=\"DeployServerlessEndpoint\",\n",
        "        processor=deployment_processor,\n",
        "        inputs=[\n",
        "            ProcessingInput(\n",
        "                source=create_model_step.properties.ModelName,\n",
        "                destination=\"/opt/ml/processing/model\",\n",
        "                input_name=\"model_name\"\n",
        "            )\n",
        "        ],\n",
        "        outputs=[\n",
        "            ProcessingOutput(\n",
        "                output_name=\"deployment_status\",\n",
        "                source=\"/opt/ml/processing/output\",\n",
        "                destination=f\"s3://{BUCKET_NAME}/pipeline-artifacts/deployment\"\n",
        "            )\n",
        "        ],\n",
        "        code=\"scripts/deploy_serverless_endpoint.py\",\n",
        "        job_arguments=[\n",
        "            # CORRECTED: Use param_dict with correct parameter name\n",
        "            \"--endpoint-name\", param_dict['EndpointName'],  # Was: parameters['endpoint_name']\n",
        "            \"--memory-size\", \"4096\",\n",
        "            \"--max-concurrency\", \"20\",\n",
        "            \"--provisioned-concurrency\", \"1\"\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return deployment_step\n",
        "\n",
        "print(\"âœ… Pipeline step definition functions created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Pipeline Assembly and Creation\n",
        "\n",
        "Assemble all pipeline steps with proper conditional logic and dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_complete_pipeline(parameters):\n",
        "    \"\"\"Create the complete SageMaker Pipeline - FULLY CORRECTED parameter access\"\"\"\n",
        "    \n",
        "    print(\"ğŸ”§ Creating complete pipeline with corrected parameter handling...\")\n",
        "    \n",
        "    # Convert parameter list to dictionary for easier access\n",
        "    param_dict = {p.name: p for p in parameters}\n",
        "    \n",
        "    print(f\"ğŸ“‹ Available parameter names: {list(param_dict.keys())}\")\n",
        "    \n",
        "    # Step 1: Data Validation\n",
        "    validation_step = create_data_validation_step(param_dict)\n",
        "    print(\"   âœ… Data validation step created\")\n",
        "    \n",
        "    # Step 2: Model Training\n",
        "    training_step = create_training_step(param_dict, validation_step)\n",
        "    print(\"   âœ… Training step created\")\n",
        "    \n",
        "    # Step 3: Model Evaluation - CORRECTED to use param_dict\n",
        "    evaluation_step, evaluation_report = create_evaluation_step(param_dict, training_step)\n",
        "    print(\"   âœ… Evaluation step created\")\n",
        "    \n",
        "    # Step 4: Performance Condition Check - CORRECTED to use param_dict\n",
        "    performance_condition = create_performance_condition(evaluation_step, evaluation_report, param_dict)\n",
        "    print(\"   âœ… Performance condition created\")\n",
        "    \n",
        "    # Step 5: Model Registration - CORRECTED to use param_dict\n",
        "    register_model_step = create_model_registration_step(param_dict, training_step, evaluation_step, evaluation_report)\n",
        "    print(\"   âœ… Model registration step created\")\n",
        "    \n",
        "    # Create conditional step for performance-based registration\n",
        "    performance_condition_step = ConditionStep(\n",
        "        name=\"CheckPerformanceThreshold\",\n",
        "        conditions=[performance_condition],\n",
        "        if_steps=[register_model_step],\n",
        "        else_steps=[]\n",
        "    )\n",
        "    \n",
        "    # Assemble pipeline steps\n",
        "    pipeline_steps = [\n",
        "        validation_step,\n",
        "        training_step,\n",
        "        evaluation_step,\n",
        "        performance_condition_step\n",
        "    ]\n",
        "    \n",
        "    # Create the pipeline - parameters is already a list\n",
        "    pipeline = Pipeline(\n",
        "        name=PIPELINE_NAME,\n",
        "        parameters=parameters,  # CORRECT: Use parameters directly (it's already a list)\n",
        "        steps=pipeline_steps,\n",
        "        sagemaker_session=pipeline_session\n",
        "    )\n",
        "    \n",
        "    print(\"   âœ… Complete pipeline assembly completed\")\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "# Apply the corrected version\n",
        "if pipeline_parameters:\n",
        "    print(\"ğŸš€ Creating Complete YOLOv11 SageMaker Pipeline (CORRECTED)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    try:\n",
        "        # Create corrected complete pipeline\n",
        "        corrected_pipeline = create_complete_pipeline(pipeline_parameters)\n",
        "        \n",
        "        print(f\"\\nâœ… Corrected Pipeline '{PIPELINE_NAME}' created successfully!\")\n",
        "        print(f\"\\nğŸ“‹ Pipeline Summary:\")\n",
        "        print(f\"   Name: {corrected_pipeline.name}\")\n",
        "        print(f\"   Steps: {len(corrected_pipeline.steps)}\")\n",
        "        print(f\"   Parameters: {len(corrected_pipeline.parameters)}\")\n",
        "        \n",
        "        # Test pipeline creation with better error handling\n",
        "        print(f\"\\nğŸ“ Testing corrected pipeline definition creation...\")\n",
        "        \n",
        "        try:\n",
        "            corrected_pipeline.create(role_arn=ROLE_ARN)\n",
        "            print(\"âœ… Corrected pipeline definition created successfully!\")\n",
        "            \n",
        "            # Store for execution\n",
        "            yolov11_pipeline = corrected_pipeline\n",
        "            \n",
        "            print(f\"\\nğŸ¯ Corrected complete pipeline is ready for execution!\")\n",
        "            print(f\"\\nâœ… Pipeline '{PIPELINE_NAME}' created successfully!\")\n",
        "            print(f\"\\nğŸ“‹ Pipeline Summary:\")\n",
        "            print(f\"   Name: {yolov11_pipeline.name}\")\n",
        "            print(f\"   Steps: {len(yolov11_pipeline.steps)}\")\n",
        "            print(f\"   Parameters: {len(yolov11_pipeline.parameters)}\")\n",
        "            \n",
        "            # Display pipeline structure\n",
        "            print(f\"\\nğŸ”— Pipeline Flow:\")\n",
        "            print(\"   1. DataValidation (ProcessingStep)\")\n",
        "            print(\"   2. YOLOv11Training (TrainingStep)\")\n",
        "            print(\"   3. ModelEvaluation (ProcessingStep)\")\n",
        "            print(\"   4. CheckPerformanceThreshold (ConditionStep)\")\n",
        "            print(\"      â”œâ”€ IF mAP@0.5 >= threshold: RegisterYOLOv11Model\")\n",
        "            print(\"      â””â”€ ELSE: Skip registration\")\n",
        "            print(\"   5. CheckDeploymentApproval (ConditionStep)\")\n",
        "            print(\"      â”œâ”€ IF approved: CreateYOLOv11Model â†’ DeployServerlessEndpoint\")\n",
        "            print(\"      â””â”€ ELSE: Skip deployment\")\n",
        "            \n",
        "        except Exception as create_error:\n",
        "            print(f\"âŒ Pipeline create() failed: {create_error}\")\n",
        "            print(f\"Error type: {type(create_error).__name__}\")\n",
        "            \n",
        "            # Debug the parameter structure\n",
        "            print(f\"\\nğŸ” Debugging parameter structure:\")\n",
        "            for name, param in pipeline_parameters.items():\n",
        "                print(f\"   {name}: {type(param).__name__} = {param}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Pipeline object creation failed: {e}\")\n",
        "        print(f\"Error type: {type(e).__name__}\")\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ Cannot create pipeline - parameters not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Pipeline Execution with MLflow Integration\n",
        "\n",
        "Execute the pipeline with comprehensive MLflow tracking and monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_pipeline_with_correct_parameters(pipeline, parameters, config):\n",
        "    \"\"\"Execute pipeline with correct parameter name mapping - FINAL FIX\"\"\"\n",
        "    \n",
        "    # Start MLflow run for pipeline execution\n",
        "    run_name = f\"pipeline-{config['dataset_name']}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
        "    \n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        print(f\"ğŸš€ Starting pipeline execution with MLflow tracking\")\n",
        "        print(f\"   MLflow Run ID: {run.info.run_id}\")\n",
        "        print(f\"   Run Name: {run_name}\")\n",
        "        \n",
        "        # Log pipeline parameters to MLflow\n",
        "        mlflow.log_params({\n",
        "            \"pipeline_name\": pipeline.name,\n",
        "            \"dataset_name\": config['dataset_name'],\n",
        "            \"dataset_path\": config['dataset_path'],\n",
        "            \"model_variant\": config['model_variant'],\n",
        "            \"image_size\": config['image_size'],\n",
        "            \"batch_size\": config['batch_size'],\n",
        "            \"epochs\": config['epochs'],\n",
        "            \"learning_rate\": config['learning_rate'],\n",
        "            \"instance_type\": config['instance_type'],\n",
        "            \"use_spot\": config['use_spot'],\n",
        "            \"performance_threshold\": config['performance_threshold'],\n",
        "            \"auto_deploy\": config['auto_deploy']\n",
        "        })\n",
        "        \n",
        "        # Set MLflow tags\n",
        "        mlflow.set_tags({\n",
        "            \"pipeline_type\": \"sagemaker_pipeline\",\n",
        "            \"model_type\": \"YOLOv11\",\n",
        "            \"task_type\": \"object_detection\",\n",
        "            \"execution_type\": \"automated_pipeline\",\n",
        "            \"dataset\": config['dataset_name'],\n",
        "            \"infrastructure\": \"sagemaker\"\n",
        "        })\n",
        "        \n",
        "        try:\n",
        "            print(\"\\nğŸ“ Pipeline already exists, proceeding to execution...\")\n",
        "            \n",
        "            # FIXED: Map parameter names correctly (PascalCase for pipeline)\n",
        "            pipeline_parameters_mapped = {\n",
        "                \"DatasetPath\": config['dataset_path'],\n",
        "                \"DatasetName\": config['dataset_name'],\n",
        "                \"ModelVariant\": config['model_variant'],\n",
        "                \"ImageSize\": str(config['image_size']),  # Convert to string\n",
        "                \"BatchSize\": str(config['batch_size']),  # Convert to string\n",
        "                \"Epochs\": str(config['epochs']),  # Convert to string\n",
        "                \"LearningRate\": str(config['learning_rate']),  # Convert to string\n",
        "                \"InstanceType\": config['instance_type'],\n",
        "                \"UseSpot\": \"true\" if config['use_spot'] else \"false\",\n",
        "                \"PerformanceThreshold\": str(config['performance_threshold']),  # Convert to string\n",
        "                \"EndpointName\": config['endpoint_name'],\n",
        "                \"ModelOutputPath\": f\"s3://{BUCKET_NAME}/pipeline-artifacts/models\",\n",
        "                \"EvaluationOutputPath\": f\"s3://{BUCKET_NAME}/pipeline-artifacts/evaluation\"\n",
        "            }\n",
        "            \n",
        "            print(\"\\nğŸ¬ Starting pipeline execution...\")\n",
        "            print(f\"ğŸ“‹ Using parameters: {list(pipeline_parameters_mapped.keys())}\")\n",
        "            \n",
        "            execution = pipeline.start(parameters=pipeline_parameters_mapped)\n",
        "            \n",
        "            execution_arn = execution.arn\n",
        "            print(f\"   âœ… Pipeline execution started\")\n",
        "            print(f\"   Execution ARN: {execution_arn}\")\n",
        "            \n",
        "            # Log execution details to MLflow\n",
        "            mlflow.log_param(\"pipeline_execution_arn\", execution_arn)\n",
        "            mlflow.log_param(\"execution_start_time\", datetime.now().isoformat())\n",
        "            mlflow.set_tag(\"pipeline_status\", \"running\")\n",
        "            \n",
        "            return execution, run.info.run_id\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Pipeline execution failed: {str(e)}\")\n",
        "            \n",
        "            # Log error to MLflow\n",
        "            mlflow.set_tag(\"pipeline_status\", \"failed\")\n",
        "            mlflow.set_tag(\"error_message\", str(e))\n",
        "            \n",
        "            return None, run.info.run_id\n",
        "\n",
        "# Execute with the corrected parameter mapping\n",
        "print(\"ğŸš€ Executing YOLOv11 Pipeline (PARAMETER NAMES FIXED)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "pipeline_execution, mlflow_run_id = execute_pipeline_with_correct_parameters(\n",
        "    yolov11_pipeline,\n",
        "    pipeline_parameters,\n",
        "    current_config\n",
        ")\n",
        "\n",
        "if pipeline_execution:\n",
        "    print(\"\\nâœ… Pipeline execution started successfully!\")\n",
        "    print(f\"ğŸ“Š Execution ARN: {pipeline_execution.arn}\")\n",
        "    print(f\"ğŸ“ˆ MLflow Run ID: {mlflow_run_id}\")\n",
        "    print(f\"ğŸ” You can monitor progress in the next section.\")\n",
        "else:\n",
        "    print(\"\\nâŒ Pipeline execution failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def fix_mlflow_experiment_setup():\n",
        "#     \"\"\"Fix MLflow experiment setup and create a new active experiment\"\"\"\n",
        "    \n",
        "#     print(\"ğŸ”§ Fixing MLflow experiment setup...\")\n",
        "    \n",
        "#     try:\n",
        "#         # Check current MLflow configuration\n",
        "#         tracking_uri = mlflow.get_tracking_uri()\n",
        "#         print(f\"   Current tracking URI: {tracking_uri}\")\n",
        "        \n",
        "#         # Create a new experiment with a unique name\n",
        "#         experiment_name = f\"yolov11-pipeline-experiments-{datetime.now().strftime('%Y%m%d')}\"\n",
        "        \n",
        "#         try:\n",
        "#             # Try to create a new experiment\n",
        "#             experiment_id = mlflow.create_experiment(experiment_name)\n",
        "#             print(f\"   âœ… Created new experiment: {experiment_name} (ID: {experiment_id})\")\n",
        "#         except Exception as e:\n",
        "#             if \"already exists\" in str(e):\n",
        "#                 # Experiment exists, try to set it\n",
        "#                 print(f\"   ğŸ“‹ Experiment {experiment_name} already exists, setting as active...\")\n",
        "#                 mlflow.set_experiment(experiment_name)\n",
        "#                 experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "#                 experiment_id = experiment.experiment_id\n",
        "#                 print(f\"   âœ… Using existing experiment: {experiment_name} (ID: {experiment_id})\")\n",
        "#             else:\n",
        "#                 raise e\n",
        "        \n",
        "#         # Set the experiment as active\n",
        "#         mlflow.set_experiment(experiment_name)\n",
        "        \n",
        "#         # Verify the experiment is active\n",
        "#         current_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "#         if current_experiment.lifecycle_stage == \"active\":\n",
        "#             print(f\"   âœ… Experiment is active and ready for use\")\n",
        "#             return True\n",
        "#         else:\n",
        "#             print(f\"   âš ï¸  Experiment state: {current_experiment.lifecycle_stage}\")\n",
        "#             return False\n",
        "            \n",
        "#     except Exception as e:\n",
        "#         print(f\"   âŒ Error fixing MLflow setup: {e}\")\n",
        "#         return False\n",
        "\n",
        "# # Fix MLflow experiment setup\n",
        "# mlflow_fixed = fix_mlflow_experiment_setup()\n",
        "\n",
        "# if mlflow_fixed:\n",
        "#     print(\"\\nâœ… MLflow experiment setup fixed!\")\n",
        "# else:\n",
        "#     print(\"\\nâŒ Could not fix MLflow experiment setup\")\n",
        "\n",
        "# # Execute immediately\n",
        "# pipeline_execution, mlflow_run_id = execute_pipeline_with_mlflow(\n",
        "#     yolov11_pipeline,\n",
        "#     pipeline_parameters,\n",
        "#     current_config\n",
        "# )\n",
        "\n",
        "# if pipeline_execution:\n",
        "#     print(\"âœ… Pipeline execution started successfully!\")\n",
        "#     print(f\"ğŸ“Š Execution ARN: {pipeline_execution.arn}\")\n",
        "#     print(f\"ğŸ“ˆ MLflow Run ID: {mlflow_run_id}\")\n",
        "# else:\n",
        "#     print(\"âŒ Pipeline execution failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Real-time Pipeline Monitoring\n",
        "\n",
        "Monitor pipeline execution with real-time status updates and step-by-step progress tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_pipeline_monitor():\n",
        "    \"\"\"Create interactive pipeline monitoring interface\"\"\"\n",
        "    \n",
        "    # Status display widgets\n",
        "    status_html = widgets.HTML(value=\"<h3>ğŸ“Š Pipeline Status: Not Started</h3>\")\n",
        "    progress_bar = widgets.IntProgress(\n",
        "        value=0,\n",
        "        min=0,\n",
        "        max=100,\n",
        "        description='Progress:',\n",
        "        bar_style='info',\n",
        "        style={'bar_color': '#1f77b4'},\n",
        "        layout=widgets.Layout(width='500px')\n",
        "    )\n",
        "    \n",
        "    steps_output = widgets.Output()\n",
        "    metrics_output = widgets.Output()\n",
        "    \n",
        "    # Control buttons\n",
        "    refresh_button = widgets.Button(\n",
        "        description=\"ğŸ”„ Refresh Status\",\n",
        "        button_style='info',\n",
        "        layout=widgets.Layout(width='150px')\n",
        "    )\n",
        "    \n",
        "    stop_button = widgets.Button(\n",
        "        description=\"â¹ï¸ Stop Pipeline\",\n",
        "        button_style='danger',\n",
        "        layout=widgets.Layout(width='150px')\n",
        "    )\n",
        "    \n",
        "    def update_pipeline_status():\n",
        "        \"\"\"Update pipeline status and progress\"\"\"\n",
        "        if 'pipeline_execution' not in globals() or not pipeline_execution:\n",
        "            status_html.value = \"<h3>ğŸ“Š Pipeline Status: No Active Execution</h3>\"\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            # Get execution status\n",
        "            execution_status = pipeline_execution.describe()\n",
        "            \n",
        "            # Update status display\n",
        "            status = execution_status['PipelineExecutionStatus']\n",
        "            status_color = {\n",
        "                'Executing': 'orange',\n",
        "                'Succeeded': 'green',\n",
        "                'Failed': 'red',\n",
        "                'Stopped': 'gray'\n",
        "            }.get(status, 'blue')\n",
        "            \n",
        "            status_html.value = f\"<h3 style='color: {status_color}'>ğŸ“Š Pipeline Status: {status}</h3>\"\n",
        "            \n",
        "            # Update progress bar\n",
        "            if status == 'Executing':\n",
        "                progress_bar.bar_style = 'info'\n",
        "                progress_bar.value = 50  # Approximate progress\n",
        "            elif status == 'Succeeded':\n",
        "                progress_bar.bar_style = 'success'\n",
        "                progress_bar.value = 100\n",
        "            elif status == 'Failed':\n",
        "                progress_bar.bar_style = 'danger'\n",
        "                progress_bar.value = 0\n",
        "            \n",
        "            # Update steps status\n",
        "            with steps_output:\n",
        "                steps_output.clear_output()\n",
        "                print(\"ğŸ”— Pipeline Steps Status:\")\n",
        "                print(\"=\" * 50)\n",
        "                \n",
        "                # Get step executions\n",
        "                steps = pipeline_execution.list_steps()\n",
        "                \n",
        "                for i, step in enumerate(steps):\n",
        "                    step_name = step['StepName']\n",
        "                    step_status = step['StepStatus']\n",
        "                    \n",
        "                    status_icon = {\n",
        "                        'Executing': 'ğŸ”„',\n",
        "                        'Succeeded': 'âœ…',\n",
        "                        'Failed': 'âŒ',\n",
        "                        'Stopped': 'â¹ï¸'\n",
        "                    }.get(step_status, 'â³')\n",
        "                    \n",
        "                    print(f\"{i+1}. {status_icon} {step_name}: {step_status}\")\n",
        "                    \n",
        "                    # Show additional details for active/failed steps\n",
        "                    if step_status in ['Executing', 'Failed']:\n",
        "                        if 'FailureReason' in step:\n",
        "                            print(f\"   âš ï¸  Reason: {step['FailureReason']}\")\n",
        "                        if 'StartTime' in step:\n",
        "                            elapsed = datetime.now() - step['StartTime'].replace(tzinfo=None)\n",
        "                            print(f\"   â±ï¸  Elapsed: {str(elapsed).split('.')[0]}\")\n",
        "            \n",
        "            # Update MLflow with current status\n",
        "            if 'mlflow_run_id' in globals() and mlflow_run_id:\n",
        "                with mlflow.start_run(run_id=mlflow_run_id):\n",
        "                    mlflow.set_tag(\"pipeline_status\", status.lower())\n",
        "                    mlflow.log_metric(\"pipeline_progress\", progress_bar.value)\n",
        "                    \n",
        "                    if status == 'Succeeded':\n",
        "                        mlflow.log_param(\"execution_end_time\", datetime.now().isoformat())\n",
        "                        mlflow.set_tag(\"pipeline_completed\", \"true\")\n",
        "            \n",
        "            # Update metrics if available\n",
        "            with metrics_output:\n",
        "                metrics_output.clear_output()\n",
        "                if status == 'Succeeded':\n",
        "                    print(\"ğŸ“ˆ Pipeline Metrics:\")\n",
        "                    print(\"=\" * 30)\n",
        "                    print(\"âœ… Pipeline completed successfully!\")\n",
        "                    print(\"ğŸ“Š Check Model Registry for registered model\")\n",
        "                    print(\"ğŸš€ Check endpoints for deployed model (if auto-deploy enabled)\")\n",
        "                elif status == 'Failed':\n",
        "                    print(\"âŒ Pipeline Execution Failed\")\n",
        "                    print(\"=\" * 30)\n",
        "                    print(\"Check the steps status above for failure details.\")\n",
        "                    print(\"Review CloudWatch logs for detailed error information.\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            status_html.value = f\"<h3 style='color: red'>âŒ Error monitoring pipeline: {str(e)}</h3>\"\n",
        "    \n",
        "    def on_refresh_clicked(b):\n",
        "        update_pipeline_status()\n",
        "    \n",
        "    def on_stop_clicked(b):\n",
        "        if 'pipeline_execution' in globals() and pipeline_execution:\n",
        "            try:\n",
        "                pipeline_execution.stop()\n",
        "                status_html.value = \"<h3 style='color: orange'>â¹ï¸ Pipeline Stop Requested</h3>\"\n",
        "            except Exception as e:\n",
        "                status_html.value = f\"<h3 style='color: red'>âŒ Error stopping pipeline: {str(e)}</h3>\"\n",
        "    \n",
        "    refresh_button.on_click(on_refresh_clicked)\n",
        "    stop_button.on_click(on_stop_clicked)\n",
        "    \n",
        "    # Initial status update\n",
        "    update_pipeline_status()\n",
        "    \n",
        "    return widgets.VBox([\n",
        "        status_html,\n",
        "        progress_bar,\n",
        "        widgets.HBox([refresh_button, stop_button]),\n",
        "        widgets.HTML(\"<h4>ğŸ“‹ Step Details:</h4>\"),\n",
        "        steps_output,\n",
        "        widgets.HTML(\"<h4>ğŸ“Š Execution Metrics:</h4>\"),\n",
        "        metrics_output\n",
        "    ])\n",
        "\n",
        "# Create and display monitoring interface\n",
        "print(\"ğŸ“Š Pipeline Monitoring Interface\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "monitoring_interface = create_pipeline_monitor()\n",
        "display(monitoring_interface)\n",
        "\n",
        "print(\"\\nğŸ’¡ Use the 'Refresh Status' button to update the pipeline status.\")\n",
        "print(\"   The interface will show real-time progress of each pipeline step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Pipeline Results Analysis\n",
        "\n",
        "Analyze pipeline execution results, model performance, and deployment status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_pipeline_results():\n",
        "    \"\"\"Analyze and display comprehensive pipeline results\"\"\"\n",
        "    \n",
        "    if 'pipeline_execution' not in globals() or not pipeline_execution:\n",
        "        print(\"âŒ No pipeline execution to analyze\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # Get execution details\n",
        "        execution_status = pipeline_execution.describe()\n",
        "        steps = pipeline_execution.list_steps()\n",
        "        \n",
        "        print(\"ğŸ“Š PIPELINE EXECUTION ANALYSIS\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Overall execution summary\n",
        "        status = execution_status['PipelineExecutionStatus']\n",
        "        start_time = execution_status.get('CreationTime')\n",
        "        end_time = execution_status.get('LastModifiedTime')\n",
        "        \n",
        "        print(f\"\\nğŸ¯ Execution Summary:\")\n",
        "        print(f\"   Status: {status}\")\n",
        "        print(f\"   Start Time: {start_time}\")\n",
        "        print(f\"   End Time: {end_time}\")\n",
        "        \n",
        "        if start_time and end_time:\n",
        "            duration = end_time - start_time\n",
        "            print(f\"   Duration: {str(duration).split('.')[0]}\")\n",
        "        \n",
        "        # Step-by-step analysis\n",
        "        print(f\"\\nğŸ”— Step Analysis:\")\n",
        "        successful_steps = 0\n",
        "        failed_steps = 0\n",
        "        \n",
        "        for step in steps:\n",
        "            step_name = step['StepName']\n",
        "            step_status = step['StepStatus']\n",
        "            \n",
        "            if step_status == 'Succeeded':\n",
        "                successful_steps += 1\n",
        "                print(f\"   âœ… {step_name}: {step_status}\")\n",
        "            elif step_status == 'Failed':\n",
        "                failed_steps += 1\n",
        "                print(f\"   âŒ {step_name}: {step_status}\")\n",
        "                if 'FailureReason' in step:\n",
        "                    print(f\"      Reason: {step['FailureReason']}\")\n",
        "            else:\n",
        "                print(f\"   ğŸ”„ {step_name}: {step_status}\")\n",
        "        \n",
        "        print(f\"\\nğŸ“ˆ Step Summary: {successful_steps} succeeded, {failed_steps} failed\")\n",
        "        \n",
        "        # Model Registry analysis\n",
        "        if status == 'Succeeded':\n",
        "            print(f\"\\nğŸ† SUCCESS ANALYSIS:\")\n",
        "            \n",
        "            # Check Model Registry for new models\n",
        "            try:\n",
        "                models = sagemaker_client.list_model_packages(\n",
        "                    ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
        "                    SortBy='CreationTime',\n",
        "                    SortOrder='Descending',\n",
        "                    MaxResults=5\n",
        "                )\n",
        "                \n",
        "                recent_models = models.get('ModelPackageSummaryList', [])\n",
        "                if recent_models:\n",
        "                    latest_model = recent_models[0]\n",
        "                    print(f\"   ğŸ“¦ Latest Model Registered:\")\n",
        "                    print(f\"      ARN: {latest_model['ModelPackageArn']}\")\n",
        "                    print(f\"      Status: {latest_model['ModelPackageStatus']}\")\n",
        "                    print(f\"      Approval: {latest_model['ModelApprovalStatus']}\")\n",
        "                    print(f\"      Created: {latest_model['CreationTime']}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸  Could not retrieve model registry info: {e}\")\n",
        "            \n",
        "            # Check for deployed endpoints\n",
        "            try:\n",
        "                endpoints = sagemaker_client.list_endpoints(\n",
        "                    SortBy='CreationTime',\n",
        "                    SortOrder='Descending',\n",
        "                    MaxResults=10\n",
        "                )\n",
        "                \n",
        "                recent_endpoints = [\n",
        "                    ep for ep in endpoints.get('Endpoints', [])\n",
        "                    if current_config['endpoint_name'] in ep['EndpointName']\n",
        "                ]\n",
        "                \n",
        "                if recent_endpoints:\n",
        "                    endpoint = recent_endpoints[0]\n",
        "                    print(f\"   ğŸš€ Endpoint Deployed:\")\n",
        "                    print(f\"      Name: {endpoint['EndpointName']}\")\n",
        "                    print(f\"      Status: {endpoint['EndpointStatus']}\")\n",
        "                    print(f\"      Created: {endpoint['CreationTime']}\")\n",
        "                else:\n",
        "                    print(f\"   â„¹ï¸  No matching endpoints found (auto-deploy may be disabled)\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   âš ï¸  Could not retrieve endpoint info: {e}\")\n",
        "        \n",
        "        elif status == 'Failed':\n",
        "            print(f\"\\nâŒ FAILURE ANALYSIS:\")\n",
        "            print(f\"   Pipeline execution failed. Common causes:\")\n",
        "            print(f\"   â€¢ Data validation issues\")\n",
        "            print(f\"   â€¢ Training job failures (resource limits, code errors)\")\n",
        "            print(f\"   â€¢ Model performance below threshold\")\n",
        "            print(f\"   â€¢ Infrastructure or permission issues\")\n",
        "            print(f\"\\n   ğŸ’¡ Check CloudWatch logs for detailed error information.\")\n",
        "        \n",
        "        # MLflow integration summary\n",
        "        if 'mlflow_run_id' in globals() and mlflow_run_id:\n",
        "            print(f\"\\nğŸ“Š MLflow Integration:\")\n",
        "            print(f\"   Run ID: {mlflow_run_id}\")\n",
        "            print(f\"   Experiment: {experiment_name}\")\n",
        "            print(f\"   All pipeline parameters and metrics logged to MLflow\")\n",
        "        \n",
        "        # Recommendations\n",
        "        print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
        "        if status == 'Succeeded':\n",
        "            print(f\"   âœ… Pipeline completed successfully!\")\n",
        "            print(f\"   â€¢ Review model performance in Model Registry\")\n",
        "            print(f\"   â€¢ Consider approving model for production deployment\")\n",
        "            print(f\"   â€¢ Set up monitoring for deployed endpoints\")\n",
        "            print(f\"   â€¢ Compare results with previous pipeline runs\")\n",
        "        else:\n",
        "            print(f\"   ğŸ”§ Pipeline needs attention:\")\n",
        "            print(f\"   â€¢ Review failed step details above\")\n",
        "            print(f\"   â€¢ Check CloudWatch logs for detailed errors\")\n",
        "            print(f\"   â€¢ Verify dataset quality and format\")\n",
        "            print(f\"   â€¢ Consider adjusting hyperparameters\")\n",
        "            print(f\"   â€¢ Ensure sufficient compute resources\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error analyzing pipeline results: {str(e)}\")\n",
        "\n",
        "# Create analysis button\n",
        "analysis_button = widgets.Button(\n",
        "    description=\"ğŸ“Š Analyze Results\",\n",
        "    button_style='info',\n",
        "    layout=widgets.Layout(width='200px', height='40px')\n",
        ")\n",
        "\n",
        "analysis_output = widgets.Output()\n",
        "\n",
        "def on_analysis_clicked(b):\n",
        "    with analysis_output:\n",
        "        analysis_output.clear_output()\n",
        "        analyze_pipeline_results()\n",
        "\n",
        "analysis_button.on_click(on_analysis_clicked)\n",
        "\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<h3>ğŸ“Š Pipeline Results Analysis</h3>\"),\n",
        "    widgets.HTML(\"<p>Click below to analyze the pipeline execution results:</p>\"),\n",
        "    analysis_button,\n",
        "    analysis_output\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Management and Approval Workflow\n",
        "\n",
        "Manage models in the Model Registry and handle approval workflows for production deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_management_interface():\n",
        "    \"\"\"Create interface for model management and approval\"\"\"\n",
        "    \n",
        "    # Model list display\n",
        "    models_output = widgets.Output()\n",
        "    \n",
        "    # Control buttons\n",
        "    refresh_models_button = widgets.Button(\n",
        "        description=\"ğŸ”„ Refresh Models\",\n",
        "        button_style='info',\n",
        "        layout=widgets.Layout(width='150px')\n",
        "    )\n",
        "    \n",
        "    approve_button = widgets.Button(\n",
        "        description=\"âœ… Approve Latest\",\n",
        "        button_style='success',\n",
        "        layout=widgets.Layout(width='150px')\n",
        "    )\n",
        "    \n",
        "    deploy_button = widgets.Button(\n",
        "        description=\"ğŸš€ Deploy Approved\",\n",
        "        button_style='warning',\n",
        "        layout=widgets.Layout(width='150px')\n",
        "    )\n",
        "    \n",
        "    def refresh_models():\n",
        "        \"\"\"Refresh and display models from Model Registry\"\"\"\n",
        "        with models_output:\n",
        "            models_output.clear_output()\n",
        "            \n",
        "            try:\n",
        "                # Get models from registry\n",
        "                response = sagemaker_client.list_model_packages(\n",
        "                    ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
        "                    SortBy='CreationTime',\n",
        "                    SortOrder='Descending',\n",
        "                    MaxResults=10\n",
        "                )\n",
        "                \n",
        "                models = response.get('ModelPackageSummaryList', [])\n",
        "                \n",
        "                if not models:\n",
        "                    print(\"ğŸ“¦ No models found in Model Registry\")\n",
        "                    print(\"   Run a successful pipeline to register models.\")\n",
        "                    return\n",
        "                \n",
        "                print(f\"ğŸ“¦ Models in Registry ({len(models)} found):\")\n",
        "                print(\"=\" * 80)\n",
        "                \n",
        "                for i, model in enumerate(models):\n",
        "                    model_arn = model['ModelPackageArn']\n",
        "                    model_id = model_arn.split('/')[-1]\n",
        "                    status = model['ModelPackageStatus']\n",
        "                    approval = model['ModelApprovalStatus']\n",
        "                    created = model['CreationTime']\n",
        "                    \n",
        "                    # Status icons\n",
        "                    status_icon = \"âœ…\" if status == \"Completed\" else \"ğŸ”„\"\n",
        "                    approval_icon = {\n",
        "                        'Approved': 'âœ…',\n",
        "                        'PendingManualApproval': 'â³',\n",
        "                        'Rejected': 'âŒ'\n",
        "                    }.get(approval, 'â“')\n",
        "                    \n",
        "                    print(f\"{i+1}. {status_icon} Model: {model_id}\")\n",
        "                    print(f\"   Status: {status} | Approval: {approval_icon} {approval}\")\n",
        "                    print(f\"   Created: {created.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                    \n",
        "                    # Get additional details for latest model\n",
        "                    if i == 0:\n",
        "                        try:\n",
        "                            details = sagemaker_client.describe_model_package(\n",
        "                                ModelPackageName=model_arn\n",
        "                            )\n",
        "                            \n",
        "                            if 'ModelMetrics' in details:\n",
        "                                print(f\"   ğŸ“Š Metrics: Available\")\n",
        "                            \n",
        "                            if 'Tags' in details:\n",
        "                                pipeline_tag = next(\n",
        "                                    (tag['Value'] for tag in details['Tags'] \n",
        "                                     if tag['Key'] == 'TrainingJob'), \n",
        "                                    'Unknown'\n",
        "                                )\n",
        "                                print(f\"   ğŸ·ï¸  Source: {pipeline_tag}\")\n",
        "                                \n",
        "                        except Exception as e:\n",
        "                            print(f\"   âš ï¸  Could not get details: {e}\")\n",
        "                    \n",
        "                    print(\"-\" * 80)\n",
        "                \n",
        "                # Store latest model for actions\n",
        "                global latest_model_arn\n",
        "                latest_model_arn = models[0]['ModelPackageArn'] if models else None\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error retrieving models: {str(e)}\")\n",
        "    \n",
        "    def approve_latest_model():\n",
        "        \"\"\"Approve the latest model for deployment\"\"\"\n",
        "        if 'latest_model_arn' not in globals() or not latest_model_arn:\n",
        "            print(\"âŒ No model available for approval\")\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            sagemaker_client.update_model_package(\n",
        "                ModelPackageArn=latest_model_arn,\n",
        "                ModelApprovalStatus='Approved',\n",
        "                ApprovalDescription='Approved via ML Engineer Pipeline notebook'\n",
        "            )\n",
        "            \n",
        "            print(f\"âœ… Model approved successfully!\")\n",
        "            print(f\"   Model ARN: {latest_model_arn.split('/')[-1]}\")\n",
        "            \n",
        "            # Update MLflow if available\n",
        "            if 'mlflow_run_id' in globals() and mlflow_run_id:\n",
        "                with mlflow.start_run(run_id=mlflow_run_id):\n",
        "                    mlflow.set_tag(\"model_approved\", \"true\")\n",
        "                    mlflow.log_param(\"approved_model_arn\", latest_model_arn)\n",
        "            \n",
        "            # Refresh display\n",
        "            refresh_models()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error approving model: {str(e)}\")\n",
        "    \n",
        "    def deploy_approved_model():\n",
        "        \"\"\"Deploy the latest approved model to a serverless endpoint\"\"\"\n",
        "        try:\n",
        "            # Find latest approved model\n",
        "            response = sagemaker_client.list_model_packages(\n",
        "                ModelPackageGroupName=MODEL_PACKAGE_GROUP_NAME,\n",
        "                ModelApprovalStatus='Approved',\n",
        "                SortBy='CreationTime',\n",
        "                SortOrder='Descending',\n",
        "                MaxResults=1\n",
        "            )\n",
        "            \n",
        "            approved_models = response.get('ModelPackageSummaryList', [])\n",
        "            if not approved_models:\n",
        "                print(\"âŒ No approved models found for deployment\")\n",
        "                print(\"   Please approve a model first.\")\n",
        "                return\n",
        "            \n",
        "            approved_model = approved_models[0]\n",
        "            model_arn = approved_model['ModelPackageArn']\n",
        "            \n",
        "            print(f\"ğŸš€ Deploying approved model...\")\n",
        "            print(f\"   Model: {model_arn.split('/')[-1]}\")\n",
        "            \n",
        "            # Create endpoint name\n",
        "            endpoint_name = f\"yolov11-serverless-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
        "            \n",
        "            # Get model details\n",
        "            model_details = sagemaker_client.describe_model_package(\n",
        "                ModelPackageName=model_arn\n",
        "            )\n",
        "            \n",
        "            # Create model\n",
        "            model_name = f\"yolov11-model-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
        "            \n",
        "            sagemaker_client.create_model(\n",
        "                ModelName=model_name,\n",
        "                Containers=model_details['InferenceSpecification']['Containers'],\n",
        "                ExecutionRoleArn=ROLE_ARN\n",
        "            )\n",
        "            \n",
        "            print(f\"   âœ… Model created: {model_name}\")\n",
        "            \n",
        "            # Create endpoint configuration\n",
        "            config_name = f\"yolov11-serverless-config-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
        "            \n",
        "            sagemaker_client.create_endpoint_configuration(\n",
        "                EndpointConfigName=config_name,\n",
        "                ProductionVariants=[\n",
        "                    {\n",
        "                        'VariantName': 'primary',\n",
        "                        'ModelName': model_name,\n",
        "                        'ServerlessConfig': {\n",
        "                            'MemorySizeInMB': 4096,\n",
        "                            'MaxConcurrency': 20,\n",
        "                            'ProvisionedConcurrency': 1\n",
        "                        }\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "            \n",
        "            print(f\"   âœ… Endpoint configuration created: {config_name}\")\n",
        "            \n",
        "            # Create endpoint\n",
        "            sagemaker_client.create_endpoint(\n",
        "                EndpointName=endpoint_name,\n",
        "                EndpointConfigName=config_name\n",
        "            )\n",
        "            \n",
        "            print(f\"   âœ… Serverless endpoint deployment started: {endpoint_name}\")\n",
        "            print(f\"   â³ Endpoint will be ready in 5-10 minutes\")\n",
        "            \n",
        "            # Update MLflow\n",
        "            if 'mlflow_run_id' in globals() and mlflow_run_id:\n",
        "                with mlflow.start_run(run_id=mlflow_run_id):\n",
        "                    mlflow.set_tag(\"model_deployed\", \"true\")\n",
        "                    mlflow.log_param(\"deployed_endpoint_name\", endpoint_name)\n",
        "                    mlflow.log_param(\"deployed_model_arn\", model_arn)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error deploying model: {str(e)}\")\n",
        "    \n",
        "    # Button event handlers\n",
        "    def on_refresh_clicked(b):\n",
        "        refresh_models()\n",
        "    \n",
        "    def on_approve_clicked(b):\n",
        "        with models_output:\n",
        "            approve_latest_model()\n",
        "    \n",
        "    def on_deploy_clicked(b):\n",
        "        with models_output:\n",
        "            deploy_approved_model()\n",
        "    \n",
        "    refresh_models_button.on_click(on_refresh_clicked)\n",
        "    approve_button.on_click(on_approve_clicked)\n",
        "    deploy_button.on_click(on_deploy_clicked)\n",
        "    \n",
        "    # Initial load\n",
        "    refresh_models()\n",
        "    \n",
        "    return widgets.VBox([\n",
        "        widgets.HTML(\"<h3>ğŸ“¦ Model Registry Management</h3>\"),\n",
        "        widgets.HBox([refresh_models_button, approve_button, deploy_button]),\n",
        "        models_output\n",
        "    ])\n",
        "\n",
        "# Display model management interface\n",
        "model_management_interface = create_model_management_interface()\n",
        "display(model_management_interface)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Troubleshooting and Best Practices\n",
        "\n",
        "Common issues and solutions for SageMaker Pipeline execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_troubleshooting_guide():\n",
        "    \"\"\"Display comprehensive troubleshooting guide\"\"\"\n",
        "    \n",
        "    troubleshooting_html = \"\"\"\n",
        "    <div style=\"background-color: #f8f9fa; padding: 20px; border-radius: 10px; border-left: 5px solid #007bff;\">\n",
        "        <h3>ğŸ”§ Troubleshooting Guide</h3>\n",
        "        \n",
        "        <h4>ğŸ“Š Common Pipeline Issues</h4>\n",
        "        <ul>\n",
        "            <li><strong>Data Validation Failures:</strong>\n",
        "                <ul>\n",
        "                    <li>Verify dataset structure matches YOLOv11 requirements</li>\n",
        "                    <li>Check that validation/ directory exists (not val/)</li>\n",
        "                    <li>Ensure image and label counts match</li>\n",
        "                    <li>Validate data.yaml file format</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "            \n",
        "            <li><strong>Training Step Failures:</strong>\n",
        "                <ul>\n",
        "                    <li>Check instance type availability in your region</li>\n",
        "                    <li>Verify ECR container image exists and is accessible</li>\n",
        "                    <li>Review hyperparameters for reasonable values</li>\n",
        "                    <li>Check CloudWatch logs for detailed error messages</li>\n",
        "                    <li>Ensure sufficient disk space for dataset size</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "            \n",
        "            <li><strong>Performance Threshold Issues:</strong>\n",
        "                <ul>\n",
        "                    <li>Lower performance threshold if models consistently fail</li>\n",
        "                    <li>Increase training epochs for better convergence</li>\n",
        "                    <li>Adjust learning rate and batch size</li>\n",
        "                    <li>Verify dataset quality and annotation accuracy</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "            \n",
        "            <li><strong>Deployment Failures:</strong>\n",
        "                <ul>\n",
        "                    <li>Verify inference container image exists</li>\n",
        "                    <li>Check endpoint name uniqueness</li>\n",
        "                    <li>Ensure model approval status is correct</li>\n",
        "                    <li>Review serverless configuration limits</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "        </ul>\n",
        "        \n",
        "        <h4>ğŸš€ Performance Optimization</h4>\n",
        "        <ul>\n",
        "            <li><strong>Training Optimization:</strong>\n",
        "                <ul>\n",
        "                    <li>Use GPU instances (ml.g4dn.xlarge or larger) for training</li>\n",
        "                    <li>Enable spot instances for cost savings</li>\n",
        "                    <li>Optimize batch size based on GPU memory</li>\n",
        "                    <li>Use mixed precision training for faster convergence</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "            \n",
        "            <li><strong>Pipeline Efficiency:</strong>\n",
        "                <ul>\n",
        "                    <li>Cache pipeline steps when possible</li>\n",
        "                    <li>Use parallel execution for independent steps</li>\n",
        "                    <li>Optimize data preprocessing steps</li>\n",
        "                    <li>Monitor step execution times</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "            \n",
        "            <li><strong>Cost Optimization:</strong>\n",
        "                <ul>\n",
        "                    <li>Use spot instances for training (up to 70% savings)</li>\n",
        "                    <li>Right-size instance types for workload</li>\n",
        "                    <li>Use serverless endpoints for variable traffic</li>\n",
        "                    <li>Clean up unused resources regularly</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "        </ul>\n",
        "        \n",
        "        <h4>ğŸ“‹ Best Practices</h4>\n",
        "        <ul>\n",
        "            <li><strong>Pipeline Design:</strong>\n",
        "                <ul>\n",
        "                    <li>Use parameterized pipelines for flexibility</li>\n",
        "                    <li>Implement proper error handling and retries</li>\n",
        "                    <li>Add comprehensive logging and monitoring</li>\n",
        "                    <li>Use conditional steps for complex workflows</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "            \n",
        "            <li><strong>Model Management:</strong>\n",
        "                <ul>\n",
        "                    <li>Always use Model Registry for model versioning</li>\n",
        "                    <li>Implement approval workflows for production</li>\n",
        "                    <li>Tag models with relevant metadata</li>\n",
        "                    <li>Track model lineage and performance</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "            \n",
        "            <li><strong>Security:</strong>\n",
        "                <ul>\n",
        "                    <li>Use least privilege IAM roles</li>\n",
        "                    <li>Encrypt data at rest and in transit</li>\n",
        "                    <li>Use VPC endpoints for secure communication</li>\n",
        "                    <li>Regularly audit access and permissions</li>\n",
        "                </ul>\n",
        "            </li>\n",
        "        </ul>\n",
        "        \n",
        "        <h4>ğŸ” Debugging Resources</h4>\n",
        "        <ul>\n",
        "            <li><strong>CloudWatch Logs:</strong> /aws/sagemaker/TrainingJobs and /aws/sagemaker/ProcessingJobs</li>\n",
        "            <li><strong>SageMaker Console:</strong> Pipeline executions and step details</li>\n",
        "            <li><strong>MLflow UI:</strong> Experiment tracking and model comparison</li>\n",
        "            <li><strong>Model Registry:</strong> Model versions and approval status</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    \n",
        "    display(HTML(troubleshooting_html))\n",
        "\n",
        "# Display troubleshooting guide\n",
        "display_troubleshooting_guide()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary and Next Steps\n",
        "\n",
        "This comprehensive ML Engineer SageMaker Pipeline notebook has successfully implemented a complete MLOps workflow for YOLOv11 object detection models.\n",
        "\n",
        "### ğŸ¯ What We've Accomplished\n",
        "\n",
        "1. **Complete Pipeline Architecture**: Built a full SageMaker Pipeline with:\n",
        "   - Data validation and preprocessing\n",
        "   - YOLOv11 model training with GPU optimization\n",
        "   - Automated model evaluation and performance checking\n",
        "   - Conditional model registration based on performance thresholds\n",
        "   - Automated serverless endpoint deployment for approved models\n",
        "\n",
        "2. **Advanced MLOps Features**:\n",
        "   - **Parameterized Pipelines**: Flexible configuration for different datasets and hyperparameters\n",
        "   - **Conditional Logic**: Performance-based decision making for model registration and deployment\n",
        "   - **Model Registry Integration**: Centralized model management with approval workflows\n",
        "   - **Serverless Endpoints**: Cost-effective inference with auto-scaling capabilities\n",
        "   - **Comprehensive Monitoring**: Real-time pipeline execution tracking\n",
        "\n",
        "3. **Production-Ready Capabilities**:\n",
        "   - **Error Recovery**: Robust error handling and retry mechanisms\n",
        "   - **Cost Optimization**: Spot instances and serverless inference\n",
        "   - **Security**: IAM role-based access control\n",
        "   - **Observability**: MLflow integration for experiment tracking and lineage\n",
        "\n",
        "4. **User Experience Enhancements**:\n",
        "   - **Interactive Configuration**: Widget-based parameter configuration\n",
        "   - **Real-time Monitoring**: Live pipeline status updates\n",
        "   - **Model Management**: Easy model approval and deployment workflows\n",
        "   - **Comprehensive Analysis**: Detailed pipeline results and recommendations\n",
        "\n",
        "### ğŸš€ Key Advantages Over Individual Training Jobs\n",
        "\n",
        "| Aspect | Individual Jobs | SageMaker Pipeline |\n",
        "|--------|----------------|--------------------|\n",
        "| **Automation** | Manual execution | Fully automated workflow |\n",
        "| **Reproducibility** | Manual tracking | Built-in versioning and lineage |\n",
        "| **Error Handling** | Manual intervention | Automatic retry and recovery |\n",
        "| **Deployment** | Manual process | Conditional automated deployment |\n",
        "| **Monitoring** | Basic job status | Comprehensive step-by-step tracking |\n",
        "| **Governance** | Limited | Full approval workflows and audit trails |\n",
        "| **Scalability** | Single job focus | End-to-end workflow orchestration |\n",
        "\n",
        "### ğŸ“ˆ Business Impact\n",
        "\n",
        "- **Reduced Time-to-Market**: Automated workflows eliminate manual steps\n",
        "- **Improved Quality**: Consistent validation and performance thresholds\n",
        "- **Cost Efficiency**: Spot instances and serverless inference reduce costs\n",
        "- **Risk Mitigation**: Approval workflows prevent poor models from reaching production\n",
        "- **Operational Excellence**: Comprehensive monitoring and alerting\n",
        "\n",
        "### ğŸ”® Next Steps and Enhancements\n",
        "\n",
        "1. **Advanced Monitoring**:\n",
        "   - Implement data drift detection\n",
        "   - Set up model performance monitoring in production\n",
        "   - Create automated retraining triggers\n",
        "\n",
        "2. **Multi-Model Support**:\n",
        "   - Extend pipeline for different YOLO variants\n",
        "   - Support for ensemble models\n",
        "   - A/B testing framework for model comparison\n",
        "\n",
        "3. **Advanced Deployment Patterns**:\n",
        "   - Blue/green deployment strategies\n",
        "   - Canary deployments with traffic splitting\n",
        "   - Multi-region deployment automation\n",
        "\n",
        "4. **Integration Enhancements**:\n",
        "   - CI/CD integration with Git workflows\n",
        "   - Integration with external monitoring tools\n",
        "   - Custom metrics and alerting\n",
        "\n",
        "5. **Performance Optimization**:\n",
        "   - Pipeline caching for faster iterations\n",
        "   - Distributed training for larger datasets\n",
        "   - Advanced hyperparameter optimization\n",
        "\n",
        "### ğŸ’¡ Key Takeaways\n",
        "\n",
        "- **Pipeline-First Approach**: Always design ML workflows as pipelines for production readiness\n",
        "- **Automation is Key**: Reduce manual intervention through comprehensive automation\n",
        "- **Governance Matters**: Implement proper approval workflows and audit trails\n",
        "- **Monitor Everything**: Comprehensive monitoring enables proactive issue resolution\n",
        "- **Cost Consciousness**: Use spot instances and serverless where appropriate\n",
        "\n",
        "This notebook demonstrates the transformation from individual training job management to complete MLOps pipeline orchestration, providing a foundation for production-ready machine learning workflows with YOLOv11 object detection models.\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ Congratulations!** You've successfully implemented a comprehensive SageMaker Pipeline for YOLOv11 object detection with full MLOps capabilities, automated deployment, and production-ready governance features."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
