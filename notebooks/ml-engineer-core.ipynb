{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Engineer Core Workflow\n",
    "\n",
    "This notebook provides essential functionality for ML Engineers to execute and manage YOLOv11 training pipelines. It focuses on core capabilities while maintaining simplicity.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Pipeline Configuration**: Set up YOLOv11 training pipeline parameters\n",
    "2. **Pipeline Execution**: Execute the training pipeline\n",
    "3. **Pipeline Monitoring**: Monitor training progress and results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AWS account with appropriate permissions\n",
    "- AWS CLI configured with \"ab\" profile\n",
    "- SageMaker Studio access with ML Engineer role\n",
    "- Access to the drone imagery dataset in S3 bucket: `lucaskle-ab3-project-pv`\n",
    "- Labeled data in YOLOv11 format\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set up AWS session with \"ab\" profile\n",
    "session = boto3.Session(profile_name='ab')\n",
    "sagemaker_session = sagemaker.Session(boto_session=session)\n",
    "sagemaker_client = session.client('sagemaker')\n",
    "region = session.region_name\n",
    "account_id = session.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "# Set up visualization\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# Define bucket name\n",
    "BUCKET_NAME = 'lucaskle-ab3-project-pv'\n",
    "ROLE_ARN = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "print(f\"Data Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Account ID: {account_id}\")\n",
    "print(f\"Role ARN: {ROLE_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Core SageMaker Pipeline (NEW)\n",
    "\n",
    "The core setup now includes a simplified SageMaker Pipeline for YOLOv11 training. This section shows how to use the pipeline for streamlined model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 List Available Pipelines\n",
    "\n",
    "First, let's check what pipelines are available in our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list SageMaker pipelines\n",
    "def list_core_pipelines():\n",
    "    \"\"\"List available core SageMaker pipelines\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.list_pipelines(\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending',\n",
    "            MaxResults=50\n",
    "        )\n",
    "        \n",
    "        pipelines = response.get('PipelineSummaries', [])\n",
    "        \n",
    "        # Filter for core setup pipelines\n",
    "        core_pipelines = [\n",
    "            p for p in pipelines \n",
    "            if 'sagemaker-core-setup' in p['PipelineName'] or 'yolov11' in p['PipelineName'].lower()\n",
    "        ]\n",
    "        \n",
    "        if core_pipelines:\n",
    "            print(f\"Found {len(core_pipelines)} core pipeline(s):\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            for i, pipeline in enumerate(core_pipelines, 1):\n",
    "                print(f\"{i}. {pipeline['PipelineName']}\")\n",
    "                print(f\"   Status: {pipeline['PipelineStatus']}\")\n",
    "                print(f\"   Created: {pipeline['CreationTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                if 'PipelineDescription' in pipeline:\n",
    "                    print(f\"   Description: {pipeline['PipelineDescription']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No core pipelines found.\")\n",
    "            print(\"\\nTo create a core pipeline, run:\")\n",
    "            print(\"!cd ../scripts/setup && ./setup_core_pipeline.sh --profile ab\")\n",
    "        \n",
    "        return core_pipelines\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing pipelines: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# List available pipelines\n",
    "available_pipelines = list_core_pipelines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Execute Core Pipeline\n",
    "\n",
    "Now let's execute a pipeline with our dataset. Make sure you have a dataset prepared using the Data Scientist notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a SageMaker pipeline\n",
    "def execute_core_pipeline(pipeline_name, parameters=None):\n",
    "    \"\"\"Execute a SageMaker pipeline with optional parameters\"\"\"\n",
    "    \n",
    "    # Prepare execution parameters\n",
    "    execution_params = []\n",
    "    if parameters:\n",
    "        for key, value in parameters.items():\n",
    "            execution_params.append({\n",
    "                'Name': key,\n",
    "                'Value': str(value)\n",
    "            })\n",
    "    \n",
    "    # Generate execution name\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    execution_name = f\"{pipeline_name}-execution-{timestamp}\"\n",
    "    \n",
    "    try:\n",
    "        # Start pipeline execution\n",
    "        response = sagemaker_client.start_pipeline_execution(\n",
    "            PipelineName=pipeline_name,\n",
    "            PipelineExecutionDisplayName=execution_name,\n",
    "            PipelineParameters=execution_params\n",
    "        )\n",
    "        \n",
    "        execution_arn = response['PipelineExecutionArn']\n",
    "        \n",
    "        print(f\"‚úÖ Pipeline execution started!\")\n",
    "        print(f\"Pipeline: {pipeline_name}\")\n",
    "        print(f\"Execution ARN: {execution_arn}\")\n",
    "        \n",
    "        if parameters:\n",
    "            print(f\"\\nParameters:\")\n",
    "            for key, value in parameters.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return execution_arn\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to execute pipeline: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Configure pipeline execution parameters\n",
    "pipeline_parameters = {\n",
    "    # Update this with your dataset path from the Data Scientist notebook\n",
    "    'InputData': f\"s3://{BUCKET_NAME}/datasets/\",  # Update with specific dataset path\n",
    "    'Epochs': 10,\n",
    "    'BatchSize': 16,\n",
    "    'LearningRate': 0.001,\n",
    "    'TrainingInstanceType': 'ml.g4dn.xlarge',\n",
    "    'ModelVariant': 'yolov11n'  # Options: yolov11n, yolov11s, yolov11m, yolov11l, yolov11x\n",
    "}\n",
    "\n",
    "print(\"Pipeline Parameters:\")\n",
    "for key, value in pipeline_parameters.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Execute pipeline if available\n",
    "if available_pipelines:\n",
    "    # Use the first available pipeline\n",
    "    selected_pipeline = available_pipelines[0]['PipelineName']\n",
    "    print(f\"\\nExecuting pipeline: {selected_pipeline}\")\n",
    "    \n",
    "    # Uncomment the line below to execute the pipeline\n",
    "    # execution_arn = execute_core_pipeline(selected_pipeline, pipeline_parameters)\n",
    "    print(\"\\n‚ö†Ô∏è  Pipeline execution is commented out for safety.\")\n",
    "    print(\"Uncomment the execution line above to run the pipeline.\")\n",
    "else:\n",
    "    print(\"\\nNo pipelines available for execution.\")\n",
    "    print(\"Please create a pipeline first using the setup script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Monitor Pipeline Execution\n",
    "\n",
    "Let's create functions to monitor pipeline execution progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor pipeline execution\n",
    "def monitor_pipeline_execution(execution_arn):\n",
    "    \"\"\"Monitor pipeline execution status\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.describe_pipeline_execution(\n",
    "            PipelineExecutionArn=execution_arn\n",
    "        )\n",
    "        \n",
    "        status = response['PipelineExecutionStatus']\n",
    "        creation_time = response['CreationTime']\n",
    "        pipeline_name = response['PipelineName']\n",
    "        \n",
    "        print(f\"Pipeline: {pipeline_name}\")\n",
    "        print(f\"Status: {status}\")\n",
    "        print(f\"Started: {creation_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        if 'LastModifiedTime' in response:\n",
    "            last_modified = response['LastModifiedTime']\n",
    "            duration = last_modified - creation_time\n",
    "            print(f\"Last Modified: {last_modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            print(f\"Duration: {duration}\")\n",
    "        \n",
    "        if 'FailureReason' in response:\n",
    "            print(f\"‚ùå Failure Reason: {response['FailureReason']}\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error monitoring execution: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to list recent executions\n",
    "def list_pipeline_executions(pipeline_name, max_results=10):\n",
    "    \"\"\"List recent pipeline executions\"\"\"\n",
    "    try:\n",
    "        response = sagemaker_client.list_pipeline_executions(\n",
    "            PipelineName=pipeline_name,\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending',\n",
    "            MaxResults=max_results\n",
    "        )\n",
    "        \n",
    "        executions = response.get('PipelineExecutionSummaries', [])\n",
    "        \n",
    "        if executions:\n",
    "            print(f\"Recent executions for {pipeline_name}:\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            for i, execution in enumerate(executions, 1):\n",
    "                print(f\"{i}. {execution['PipelineExecutionDisplayName']}\")\n",
    "                print(f\"   Status: {execution['PipelineExecutionStatus']}\")\n",
    "                print(f\"   Started: {execution['StartTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                if 'EndTime' in execution:\n",
    "                    print(f\"   Ended: {execution['EndTime'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "                print(f\"   ARN: {execution['PipelineExecutionArn']}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(f\"No executions found for pipeline: {pipeline_name}\")\n",
    "        \n",
    "        return executions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error listing executions: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Example usage (uncomment to use with actual execution ARN)\n",
    "# if 'execution_arn' in locals():\n",
    "#     monitor_pipeline_execution(execution_arn)\n",
    "\n",
    "# List recent executions for available pipelines\n",
    "if available_pipelines:\n",
    "    pipeline_name = available_pipelines[0]['PipelineName']\n",
    "    recent_executions = list_pipeline_executions(pipeline_name)\n",
    "else:\n",
    "    print(\"No pipelines available to check executions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Pipeline Management Commands\n",
    "\n",
    "Here are some useful commands for managing pipelines from the command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display useful pipeline management commands\n",
    "print(\"üîß Pipeline Management Commands\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\n1. Create a new pipeline:\")\n",
    "print(\"   !cd ../scripts/setup && ./setup_core_pipeline.sh --profile ab\")\n",
    "\n",
    "print(\"\\n2. List available pipelines:\")\n",
    "print(\"   !cd ../scripts/setup && ./execute_core_pipeline.py --list-pipelines --profile ab\")\n",
    "\n",
    "print(\"\\n3. Execute a pipeline:\")\n",
    "print(\"   !cd ../scripts/setup && ./execute_core_pipeline.py \\\\\")\n",
    "print(\"       --pipeline-name PIPELINE_NAME \\\\\")\n",
    "print(\"       --input-data s3://lucaskle-ab3-project-pv/datasets/your_dataset/ \\\\\")\n",
    "print(\"       --epochs 20 \\\\\")\n",
    "print(\"       --batch-size 32 \\\\\")\n",
    "print(\"       --profile ab\")\n",
    "\n",
    "print(\"\\n4. Monitor execution:\")\n",
    "print(\"   !cd ../scripts/setup && ./execute_core_pipeline.py --monitor EXECUTION_ARN --profile ab\")\n",
    "\n",
    "print(\"\\n5. List recent executions:\")\n",
    "print(\"   !cd ../scripts/setup && ./execute_core_pipeline.py --list-executions PIPELINE_NAME --profile ab\")\n",
    "\n",
    "print(\"\\nüìö Documentation:\")\n",
    "print(\"   - Pipeline setup guide: ../scripts/setup/CORE_PIPELINE_README.md\")\n",
    "print(f\"   - SageMaker Console: https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/pipelines\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"   - Use Data Scientist notebook to prepare datasets first\")\n",
    "print(\"   - Start with small datasets and short training for testing\")\n",
    "print(\"   - Monitor costs when using GPU instances\")\n",
    "print(\"   - Check CloudWatch logs for detailed training information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Configuration\n",
    "\n",
    "Let's configure our YOLOv11 training pipeline parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list available datasets\n",
    "def list_datasets(bucket, prefix=\"datasets/\"):\n",
    "    \"\"\"List available datasets in S3\"\"\"\n",
    "    s3_client = session.client('s3')\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix=prefix,\n",
    "        Delimiter='/'\n",
    "    )\n",
    "    \n",
    "    datasets = []\n",
    "    if 'CommonPrefixes' in response:\n",
    "        for obj in response['CommonPrefixes']:\n",
    "            dataset_prefix = obj['Prefix']\n",
    "            dataset_name = dataset_prefix.split('/')[-2]\n",
    "            datasets.append({\n",
    "                'name': dataset_name,\n",
    "                'prefix': dataset_prefix\n",
    "            })\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# List available datasets\n",
    "datasets = list_datasets(BUCKET_NAME)\n",
    "\n",
    "print(f\"Found {len(datasets)} datasets:\")\n",
    "for i, dataset in enumerate(datasets):\n",
    "    print(f\"  {i+1}. {dataset['name']} - s3://{BUCKET_NAME}/{dataset['prefix']}\")\n",
    "\n",
    "# If no datasets found, provide instructions\n",
    "if not datasets:\n",
    "    print(\"\\nNo datasets found. Please prepare a dataset using the Data Scientist notebook first.\")\n",
    "    print(\"The dataset should be organized in the following structure:\")\n",
    "    print(\"s3://lucaskle-ab3-project-pv/datasets/your_dataset_name/\")\n",
    "    print(\"‚îú‚îÄ‚îÄ train/\")\n",
    "    print(\"‚îÇ   ‚îú‚îÄ‚îÄ images/\")\n",
    "    print(\"‚îÇ   ‚îî‚îÄ‚îÄ labels/\")\n",
    "    print(\"‚îî‚îÄ‚îÄ val/\")\n",
    "    print(\"    ‚îú‚îÄ‚îÄ images/\")\n",
    "    print(\"    ‚îî‚îÄ‚îÄ labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "# You can modify these parameters based on your requirements\n",
    "training_params = {\n",
    "    # Dataset parameters\n",
    "    'dataset_name': datasets[0]['name'] if datasets else 'your_dataset_name',\n",
    "    'dataset_prefix': datasets[0]['prefix'] if datasets else 'datasets/your_dataset_name/',\n",
    "    \n",
    "    # Model parameters\n",
    "    'model_variant': 'yolov11n',  # Options: yolov11n, yolov11s, yolov11m, yolov11l, yolov11x\n",
    "    'image_size': 640,  # Input image size (px)\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 16,\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    \n",
    "    # Infrastructure parameters\n",
    "    'instance_type': 'ml.g4dn.xlarge',\n",
    "    'instance_count': 1,\n",
    "    'use_spot': True,\n",
    "    'max_wait': 36000,  # Max wait time for spot instances (seconds)\n",
    "    'max_run': 3600,    # Max run time (seconds)\n",
    "    \n",
    "    # Output parameters\n",
    "    'output_path': f\"s3://{BUCKET_NAME}/model-artifacts/\",\n",
    "    'job_name': f\"yolov11-training-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "}\n",
    "\n",
    "# Display training parameters\n",
    "print(\"YOLOv11 Training Parameters:\")\n",
    "for key, value in training_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Customize Training Parameters\n",
    "\n",
    "You can modify the training parameters above based on your requirements. Here's a guide to the parameters:\n",
    "\n",
    "- **Dataset Parameters**:\n",
    "  - `dataset_name`: Name of your dataset\n",
    "  - `dataset_prefix`: S3 prefix where your dataset is stored\n",
    "\n",
    "- **Model Parameters**:\n",
    "  - `model_variant`: YOLOv11 model variant (yolov11n, yolov11s, yolov11m, yolov11l, yolov11x)\n",
    "  - `image_size`: Input image size in pixels\n",
    "\n",
    "- **Training Parameters**:\n",
    "  - `batch_size`: Batch size for training\n",
    "  - `epochs`: Number of training epochs\n",
    "  - `learning_rate`: Learning rate for optimizer\n",
    "\n",
    "- **Infrastructure Parameters**:\n",
    "  - `instance_type`: SageMaker instance type for training\n",
    "  - `instance_count`: Number of instances to use\n",
    "  - `use_spot`: Whether to use spot instances (cheaper but can be interrupted)\n",
    "  - `max_wait`: Maximum wait time for spot instances (seconds)\n",
    "  - `max_run`: Maximum run time for training job (seconds)\n",
    "\n",
    "- **Output Parameters**:\n",
    "  - `output_path`: S3 path for model artifacts\n",
    "  - `job_name`: Name for the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline Execution\n",
    "\n",
    "Now let's execute the YOLOv11 training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and execute training job\n",
    "def execute_training_job(params):\n",
    "    \"\"\"Create and execute SageMaker training job for YOLOv11\"\"\"\n",
    "    # Define hyperparameters\n",
    "    hyperparameters = {\n",
    "        \"model_variant\": params['model_variant'],\n",
    "        \"image_size\": str(params['image_size']),\n",
    "        \"batch_size\": str(params['batch_size']),\n",
    "        \"epochs\": str(params['epochs']),\n",
    "        \"learning_rate\": str(params['learning_rate'])\n",
    "    }\n",
    "    \n",
    "    # Define input data channels\n",
    "    input_data = {\n",
    "        'training': f\"s3://{BUCKET_NAME}/{params['dataset_prefix']}\"\n",
    "    }\n",
    "    \n",
    "    # Create SageMaker estimator\n",
    "    estimator = sagemaker.estimator.Estimator(\n",
    "        image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/yolov11-training:latest\",\n",
    "        role=ROLE_ARN,\n",
    "        instance_count=params['instance_count'],\n",
    "        instance_type=params['instance_type'],\n",
    "        hyperparameters=hyperparameters,\n",
    "        output_path=params['output_path'],\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        use_spot_instances=params['use_spot'],\n",
    "        max_wait=params['max_wait'] if params['use_spot'] else None,\n",
    "        max_run=params['max_run']\n",
    "    )\n",
    "    \n",
    "    # Start training job\n",
    "    print(f\"Starting training job: {params['job_name']}\")\n",
    "    estimator.fit(input_data, job_name=params['job_name'], wait=False)\n",
    "    \n",
    "    return params['job_name']\n",
    "\n",
    "# Execute training job\n",
    "try:\n",
    "    job_name = execute_training_job(training_params)\n",
    "    print(f\"\\nTraining job started: {job_name}\")\n",
    "    print(f\"You can monitor the job in the SageMaker console or using the cell below.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error starting training job: {str(e)}\")\n",
    "    print(\"\\nPossible causes:\")\n",
    "    print(\"1. The dataset doesn't exist or has incorrect structure\")\n",
    "    print(\"2. The YOLOv11 training container doesn't exist in ECR\")\n",
    "    print(\"3. Insufficient permissions to start training job\")\n",
    "    print(\"\\nPlease check the error message and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline Monitoring\n",
    "\n",
    "Let's monitor the progress of our training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor training job\n",
    "def monitor_training_job(job_name):\n",
    "    \"\"\"Monitor SageMaker training job status\"\"\"\n",
    "    # Get job description\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Extract job status\n",
    "    status = response['TrainingJobStatus']\n",
    "    creation_time = response['CreationTime']\n",
    "    last_modified_time = response.get('LastModifiedTime', creation_time)\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = last_modified_time - creation_time\n",
    "    duration_minutes = duration.total_seconds() / 60\n",
    "    \n",
    "    # Display job information\n",
    "    print(f\"Job Name: {job_name}\")\n",
    "    print(f\"Status: {status}\")\n",
    "    print(f\"Creation Time: {creation_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Last Modified: {last_modified_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Duration: {duration_minutes:.2f} minutes\")\n",
    "    \n",
    "    # Display additional information based on status\n",
    "    if status == 'InProgress':\n",
    "        print(\"\\nJob is still running. Check back later for results.\")\n",
    "    elif status == 'Completed':\n",
    "        print(\"\\nJob completed successfully!\")\n",
    "        print(f\"Model artifacts: {response['ModelArtifacts']['S3ModelArtifacts']}\")\n",
    "    elif status == 'Failed':\n",
    "        print(\"\\nJob failed!\")\n",
    "        print(f\"Failure reason: {response.get('FailureReason', 'Unknown')}\")\n",
    "    elif status == 'Stopped':\n",
    "        print(\"\\nJob was stopped.\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Monitor the training job\n",
    "try:\n",
    "    if 'job_name' in locals():\n",
    "        job_response = monitor_training_job(job_name)\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error monitoring training job: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Refresh Job Status\n",
    "\n",
    "You can run the cell below to refresh the job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh job status\n",
    "try:\n",
    "    if 'job_name' in locals():\n",
    "        job_response = monitor_training_job(job_name)\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error monitoring training job: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 View Training Metrics\n",
    "\n",
    "Once the training job is complete, you can view the training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get training metrics\n",
    "def get_training_metrics(job_name):\n",
    "    \"\"\"Get training metrics from CloudWatch\"\"\"\n",
    "    # Get job description\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Check if job is complete\n",
    "    if response['TrainingJobStatus'] != 'Completed':\n",
    "        print(f\"Job is not yet complete. Current status: {response['TrainingJobStatus']}\")\n",
    "        return None\n",
    "    \n",
    "    # Get CloudWatch metrics\n",
    "    cloudwatch = session.client('cloudwatch')\n",
    "    \n",
    "    # Define metrics to retrieve\n",
    "    metrics = [\n",
    "        'train:loss',\n",
    "        'val:loss',\n",
    "        'val:mAP50',\n",
    "        'val:mAP50-95'\n",
    "    ]\n",
    "    \n",
    "    # Get metrics data\n",
    "    metrics_data = {}\n",
    "    for metric_name in metrics:\n",
    "        try:\n",
    "            response = cloudwatch.get_metric_statistics(\n",
    "                Namespace='SageMaker',\n",
    "                MetricName=metric_name,\n",
    "                Dimensions=[\n",
    "                    {\n",
    "                        'Name': 'TrainingJobName',\n",
    "                        'Value': job_name\n",
    "                    }\n",
    "                ],\n",
    "                StartTime=response['CreationTime'],\n",
    "                EndTime=response['LastModifiedTime'],\n",
    "                Period=60,  # 1-minute periods\n",
    "                Statistics=['Average']\n",
    "            )\n",
    "            \n",
    "            # Extract datapoints\n",
    "            datapoints = response.get('Datapoints', [])\n",
    "            if datapoints:\n",
    "                # Sort by timestamp\n",
    "                datapoints.sort(key=lambda x: x['Timestamp'])\n",
    "                \n",
    "                # Extract values\n",
    "                timestamps = [dp['Timestamp'] for dp in datapoints]\n",
    "                values = [dp['Average'] for dp in datapoints]\n",
    "                \n",
    "                metrics_data[metric_name] = {\n",
    "                    'timestamps': timestamps,\n",
    "                    'values': values\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving metric {metric_name}: {str(e)}\")\n",
    "    \n",
    "    return metrics_data\n",
    "\n",
    "# Get training metrics\n",
    "try:\n",
    "    if 'job_name' in locals():\n",
    "        metrics_data = get_training_metrics(job_name)\n",
    "        \n",
    "        if metrics_data:\n",
    "            # Plot metrics\n",
    "            fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "            \n",
    "            # Plot loss\n",
    "            if 'train:loss' in metrics_data:\n",
    "                axes[0].plot(\n",
    "                    metrics_data['train:loss']['timestamps'],\n",
    "                    metrics_data['train:loss']['values'],\n",
    "                    label='Train Loss'\n",
    "                )\n",
    "            \n",
    "            if 'val:loss' in metrics_data:\n",
    "                axes[0].plot(\n",
    "                    metrics_data['val:loss']['timestamps'],\n",
    "                    metrics_data['val:loss']['values'],\n",
    "                    label='Validation Loss'\n",
    "                )\n",
    "            \n",
    "            axes[0].set_title('Training and Validation Loss')\n",
    "            axes[0].set_xlabel('Time')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot mAP\n",
    "            if 'val:mAP50' in metrics_data:\n",
    "                axes[1].plot(\n",
    "                    metrics_data['val:mAP50']['timestamps'],\n",
    "                    metrics_data['val:mAP50']['values'],\n",
    "                    label='mAP@0.5'\n",
    "                )\n",
    "            \n",
    "            if 'val:mAP50-95' in metrics_data:\n",
    "                axes[1].plot(\n",
    "                    metrics_data['val:mAP50-95']['timestamps'],\n",
    "                    metrics_data['val:mAP50-95']['values'],\n",
    "                    label='mAP@0.5:0.95'\n",
    "                )\n",
    "            \n",
    "            axes[1].set_title('Validation mAP')\n",
    "            axes[1].set_xlabel('Time')\n",
    "            axes[1].set_ylabel('mAP')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display final metrics\n",
    "            print(\"Final Metrics:\")\n",
    "            for metric_name, data in metrics_data.items():\n",
    "                if data['values']:\n",
    "                    print(f\"  {metric_name}: {data['values'][-1]:.4f}\")\n",
    "        else:\n",
    "            print(\"No metrics available yet. Job may still be running or has failed.\")\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving training metrics: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Artifacts\n",
    "\n",
    "Once the training job is complete, you can access the model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get model artifacts\n",
    "def get_model_artifacts(job_name):\n",
    "    \"\"\"Get model artifacts from training job\"\"\"\n",
    "    # Get job description\n",
    "    response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=job_name\n",
    "    )\n",
    "    \n",
    "    # Check if job is complete\n",
    "    if response['TrainingJobStatus'] != 'Completed':\n",
    "        print(f\"Job is not yet complete. Current status: {response['TrainingJobStatus']}\")\n",
    "        return None\n",
    "    \n",
    "    # Get model artifacts\n",
    "    model_artifacts = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    print(f\"Model artifacts: {model_artifacts}\")\n",
    "    print(\"\\nThe model artifacts contain:\")\n",
    "    print(\"1. model.tar.gz - The compressed model files\")\n",
    "    print(\"2. Inside model.tar.gz:\")\n",
    "    print(\"   - best.pt - The best model weights\")\n",
    "    print(\"   - last.pt - The last model weights\")\n",
    "    print(\"   - results.csv - Training results\")\n",
    "    print(\"   - args.yaml - Training arguments\")\n",
    "    \n",
    "    return model_artifacts\n",
    "\n",
    "# Get model artifacts\n",
    "try:\n",
    "    if 'job_name' in locals():\n",
    "        model_artifacts = get_model_artifacts(job_name)\n",
    "    else:\n",
    "        print(\"No active training job to monitor.\")\n",
    "        print(\"Please execute a training job first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving model artifacts: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've executed and monitored YOLOv11 training pipelines using both the new core SageMaker Pipeline and traditional training jobs. Here's a summary of what we've accomplished:\n",
    "\n",
    "0. **Core SageMaker Pipeline** (NEW):\n",
    "   - Listed available simplified pipelines for YOLOv11 training\n",
    "   - Configured pipeline parameters for training execution\n",
    "   - Monitored pipeline execution progress and results\n",
    "   - Learned command-line tools for pipeline management\n",
    "\n",
    "1. **Pipeline Configuration**:\n",
    "   - Listed available datasets\n",
    "   - Configured training parameters\n",
    "\n",
    "2. **Pipeline Execution**:\n",
    "   - Created and executed SageMaker training jobs\n",
    "\n",
    "3. **Pipeline Monitoring**:\n",
    "   - Monitored training job status\n",
    "   - Viewed training metrics\n",
    "\n",
    "4. **Model Artifacts**:\n",
    "   - Accessed model artifacts from training jobs\n",
    "\n",
    "### Key Features of the Core Pipeline:\n",
    "\n",
    "- **Simplified Setup**: Easy-to-use SageMaker Pipeline for YOLOv11 training\n",
    "- **Configurable Parameters**: Customizable training parameters (epochs, batch size, learning rate, etc.)\n",
    "- **Multiple Model Variants**: Support for all YOLOv11 variants (n, s, m, l, x)\n",
    "- **GPU Training**: Optimized for GPU instances with cost-effective options\n",
    "- **Automated Evaluation**: Built-in model evaluation step\n",
    "- **Command-Line Tools**: Scripts for pipeline creation, execution, and monitoring\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Create Core Pipeline**: Use the setup script to create a simplified pipeline\n",
    "2. **Prepare Dataset**: Use Data Scientist notebook to create YOLOv11-formatted datasets\n",
    "3. **Execute Pipeline**: Run training with your prepared dataset\n",
    "4. **Monitor Progress**: Track training progress and results\n",
    "5. **Model Evaluation**: Review evaluation metrics and model performance\n",
    "6. **Model Deployment**: Deploy trained models to endpoints (advanced)\n",
    "7. **Iterative Improvement**: Refine models based on evaluation results\n",
    "\n",
    "### Pipeline Management Workflow:\n",
    "\n",
    "1. **Setup**: `./setup_core_pipeline.sh --profile ab`\n",
    "2. **Execute**: `./execute_core_pipeline.py --pipeline-name NAME --input-data PATH --profile ab`\n",
    "3. **Monitor**: `./execute_core_pipeline.py --monitor ARN --profile ab`\n",
    "4. **Review**: Check results in SageMaker Console and S3\n",
    "\n",
    "### Cost Optimization Tips:\n",
    "\n",
    "- Start with `yolov11n` (nano) for initial testing\n",
    "- Use `ml.g4dn.xlarge` for cost-effective GPU training\n",
    "- Set appropriate `max_run` times to prevent runaway costs\n",
    "- Use small datasets for development and testing\n",
    "- Monitor training progress and stop early if needed\n",
    "\n",
    "For more detailed functionality, refer to the comprehensive notebooks in the `notebooks/` directory and the pipeline documentation in `scripts/setup/CORE_PIPELINE_README.md`.",
    "3. **Pipeline Monitoring**:\n",
    "   - Monitored training job status\n",
    "   - Viewed training metrics\n",
    "\n",
    "4. **Model Artifacts**:\n",
    "   - Accessed model artifacts from the training job\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Model Evaluation**: Evaluate the trained model on test data\n",
    "2. **Model Deployment**: Deploy the model to a SageMaker endpoint\n",
    "3. **Model Monitoring**: Set up monitoring for the deployed model\n",
    "4. **Iterative Improvement**: Refine the model based on evaluation results\n",
    "\n",
    "For more detailed functionality, refer to the comprehensive notebooks in the `notebooks/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
